{
  "hash": "6e3190570a219c48839bbd54b8ba1ac8",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Generalized linear models\n\n\n\n\n\n## Why Generalized Linear Models (GLMs)\n- GLMs extend linear model framework to outcome variables that do not follow normal distribution.\n- They are most frequently used to model binary, categorical or count data.\n- For instance, fitting a regression line to binary data yields predicted values that could take any value, including $<0$, \n- not to mention that it is hard to argue that the values of 0 and 1s are normally distributed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_diabetes %>%\n  mutate(obese = as.numeric(obese) - 1) %>%\n  ggplot(aes(y=obese, x=waist)) +\n  geom_jitter(width=0, height = 0) +\n  geom_smooth(method=\"lm\", se=FALSE, color=col.blue.dark) + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![Example of fitting linear model to binary data, to model obesity status coded as 1 (Yes) and 0 (No) with waist variable. Linear model does not fit the data well in this case and the predicted values can take any value, not only 0 and 1.](lm-glm_files/figure-html/fig-log-example-1.png){#fig-log-example width=480}\n:::\n:::\n\n\n\n## Logistic regression\n\nLet's look again at the binary `obesity` status data and try to fit logistic regression model using `waist` as explanatory variable instead of fitting inappropriate here simple linear model.\n\n\n::: {.cell .fig-cap-location-margin}\n\n```{.r .cell-code}\np1 <- data_diabetes %>%\n  mutate(obese = as.numeric(obese) - 1) %>%\n  ggplot(aes(y=obese, x=waist)) +\n  geom_jitter(width=0, height = 0.05, alpha = 0.7) +\n  my.ggtheme\n\np2 <- data_diabetes %>%\n  ggplot(aes(x = obese, y = waist, fill = obese)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \"Set2\") + \n  my.ggtheme\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![Obesity status data: jittered plot (left) and box plot of waist stratified by obesity status for the 130 study participants.](lm-glm_files/figure-html/fig-obesity-1.png){#fig-obesity width=672}\n:::\n:::\n\n\n- Since the response variable takes only two values (Yes/No) we use GLM model \n- to fit **logistic regression** model for the **probability of suffering from obesity (Yes).**\n- We let $p_i=P(Y_i=1)$ denote the probability of suffering from obesity (success)\n- and we assume that the response follows binomial distribution: $Y_i \\sim Bi(1, p_i)$ distribution.\n- We can then write the regression model now as: \n$$log(\\frac{p_i}{1-p_i})=\\beta_0 + \\beta_1x_i$$\nand given the properties of logarithms this is also equivalent to:\n$$p_i = \\frac{exp(\\beta_0 + \\beta_1x_i)}{1 + exp(\\beta_0 + \\beta_1x_i)}$$\n\n<br>\n\n- In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a **link function**.\n- Here, the **link function** $log(\\frac{p_i}{1-p_i})$ provides the link between the binomial distribution of $Y_i$ (suffering from obesity) and the linear predictor (waist) \n- Thus the **GLM model** can be written as $$g(\\mu_i)=\\mathbf{X}\\boldsymbol\\beta$$ where `g()` is the link function.\n\n<br>\n\nIn R we can use `glm()` function to fit GLM models:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# re-code obese status from Yes/No to 1/0\ndata_diabetes <- \n  data_diabetes %>%\n  mutate(obese = as.numeric(obese) - 1)\n\n# fit logistic regression model\nlogmodel_1 <- glm(obese ~ waist, family = binomial(link=\"logit\"), data = data_diabetes)\n\n# print model summary\nprint(summary(logmodel_1))\n## \n## Call:\n## glm(formula = obese ~ waist, family = binomial(link = \"logit\"), \n##     data = data_diabetes)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.8408  -0.5918  -0.1867   0.6340   2.2816  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -17.357      2.973  -5.837 5.30e-09 ***\n## waist         17.174      2.974   5.775 7.71e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 102.79  on 128  degrees of freedom\n## AIC: 106.79\n## \n## Number of Fisher Scoring iterations: 5\n\n# plot\nggPredict(logmodel_1) + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![Fitted logistic model to diabetes data given the 130 study participants and using waist as explantatory variable to model obesity status.](lm-glm_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# to get predictions use predict() functions\n# if no new observations is specified predictions are returned for the values of exploratory variables used\n# we specify response to return prediction on the probability scale\nobese_predicted <- predict(logmodel_1, type=\"response\")\nprint(head(obese_predicted))\n##          3          7          9         11         16         21 \n## 0.98231495 0.93752915 0.07405337 0.41460606 0.22839680 0.72385967\n```\n:::\n\n\n- The regression equation for the fitted model is:\n$$log(\\frac{\\hat{p_i}}{1-\\hat{p_i}})=-17.357  +  17.174\\cdot x_i$$\n- We see from the output that $\\hat{\\beta_0} = -17.357$ and $\\hat{\\beta_1} = 17.174$.\n- These estimates are arrived at via **maximum likelihood estimation**, something that is out of scope here.\n\n### Hypothesis testing {-}\n\n- Similarly to linear models, we can determine whether each variable is related to the outcome of interest by testing the null hypothesis that the relevant logistic regression coefficient is zero.\n- This can be performed by **Wald test** which is equals to estimated logistic regression coefficient divided by its standard error and follows the Standard Normal distribution:\n$$W = \\frac{\\hat\\beta-\\beta}{e.s.e.(\\hat\\beta)} \\sim N(0,1)$$.\n- Alternatively, its square approximates the Chi-squared distribution with 1 degree of freedom:\n$$W^2 = \\frac{(\\hat\\beta-\\beta)^2}{\\hat {var}(\\hat\\beta)} \\sim \\chi_1^2$$.\n\n\nIn our example, we can check whether `waist` is associated with `obesity status` by testing null hypothesis: $H_0:\\beta_1=0$. We calculate Wald statistics as $W^2 = \\frac{(\\hat\\beta-0)}{\\hat {e.s.e}(\\hat\\beta)} = \\frac{17.174}{2.974} = 5.774714$ and we can find the corresponding p-value using standard normal distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n2*pnorm(5.774714, lower.tail = F)\n## [1] 7.70839e-09\n```\n:::\n\n\nwhich confirms the summary output shown previously above and shows that there is enough evidence to reject the null hypothesis at 5% significance level $p-value << 0.05$ and conclude that there is a significant association between `waist` and `obesity status`.\n\n### Deviance {-}\n\n- Deviance is the number that measures the goodness of fit of a logistic regression model.\n- We use saturated and residual deviance to assess model, instead of $R^2$ or $R^2(adj)$.\n- We can also use deviance to check the association between explanatory variable and the outcome, an alternative and slightly more powerful test than Wald test (although these two test give similar results when sample size is large).\n- In the **likelihood ratio test** the test statistics is the **deviance** for the full model minus the deviance for the full model excluding the relevant explanatory variable. This test statistics follow a Chi-squared distribution with 1 degree of freedom.\n\n<br>\n\nFor instance, given our model we have null deviance of 274.4 and residual deviance of 268.7. The difference 5.7 is larger than than 95th percentile of $\\chi^2(129-128)$ = 3.841459, where 129 is degrees of freedom for null model and 128 is degrees of freedom for null model excluding the `waist` explanatory variable. \n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nqchisq(df=1, p=0.95)\n## [1] 3.841459\n```\n:::\n\n- Again $5.7 > 3.84$ and we can conclude that `waist` is a significant term in the model.\n\n### Odds ratios {-}\n\n- In logistic regression we often interpret the model coefficients by taking $e^{\\hat{\\beta}}$\n- and we talk about **odd ratios**.\n- For instance we can say, given our above model, $e^{17.174} = 28745736$ that for each unit increase in `waist` the odds of suffering from obesity get multiplied by 28745736. \n\n\nThese odds ratios are very high as here we are modeling obesity status with waist measurements, and increasing one unit in waist would mean adding up additional 1m to waist measurements. Typically:\n\n- Odd ratios of 1.0 (or close to 1.0) indicates that exposure is not associated with the outcome.\n- Odds ratios $> 1.0$ indicates that the odds of exposure among cases are greater than the odds of\nexposure among controls. \n- Odds raitos $< 1.0$ indicates that the odds of exposure among cases are lower than the odds of\nexposure among controls. \n- The magnitude of the odds ratio is called the **strength of the association**. The further away an odds ratio is from 1.0, the more likely it is that the relationship between the exposure and the disease is causal. For example, an odds ratio of 1.2 is above 1.0, but is not a strong association. An odds ratio of 10 suggests a stronger association.\n\n### Other covariates {-}\n\n- We can use the same logic as in multiple regression to expand by models by additional variables, numerical, binary or categorical. \n- For instance, we can test whether there is a gender effect when suffering from obesity:\n\n\n::: {.cell .fig-cap-location-margin}\n\n```{.r .cell-code  code-fold=\"false\"}\n# fit logistic regression including age and gender\nlogmodel_2 <- glm(obese ~ waist + gender, family = binomial(link=\"logit\"), data = data_diabetes)\n\n# print model summary\nprint(summary(logmodel_2))\n## \n## Call:\n## glm(formula = obese ~ waist + gender, family = binomial(link = \"logit\"), \n##     data = data_diabetes)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.0572  -0.5316  -0.1813   0.5024   2.0543  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -18.2756     3.1077  -5.881 4.08e-09 ***\n## waist         17.4401     3.0523   5.714 1.11e-08 ***\n## genderfemale   1.2335     0.5228   2.359   0.0183 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.708  on 129  degrees of freedom\n## Residual deviance:  96.877  on 127  degrees of freedom\n## AIC: 102.88\n## \n## Number of Fisher Scoring iterations: 6\n\n# plot model\nggPredict(logmodel_2) + \n  my.ggtheme + \n  scale_color_brewer(palette = \"Set2\")\n```\n\n::: {.cell-output-display}\n![Obesity status model with logistic regression given waist and gender exploratory variables. There is some seperation between the fitted lines for men and women and the model summary shows that there is enough evidence to reject a null hypothesis o no association between gender and obesity status.](lm-glm_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## Poisson regression\n\n- GLMs can be also applied to count data\n- For instance to model hospital admissions due to respiratory disease or number of bird nests in a certain habitat. \n- Here, we commonly assume that data follow the Poisson distribution $Y_i \\sim Pois(\\mu_i)$\n- and the corresponding model is:\n$$E(Y_i)=\\mu_i = \\eta_ie^{\\mathbf{x_i}^T\\boldsymbol\\beta}$$ with a log link $\\ln\\mu_i = \\ln \\eta_i + \\mathbf{x_i}^T\\boldsymbol\\beta$\n- Hypothesis testing and assessing model fit follows the same logic as in logistic regression.\n\n::: {#exm-poisson}\n\n## Number of cancer cases\n\nSuppose we wish to model $Y_i$ the number of cancer cases in the i-th intermediate geographical location (IG) in Glasgow. We have collected data for 271 small regions with between 2500 and 6000 people living in them. Together with cancer occurrence with have the following data:\n\n- Y\\_all: number of cases of all types of cancer in the IG in 2013\n- E\\_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013\n- pm10: air pollution\n- smoke: percentage of people in an area that smoke\n- ethnic: percentage of people who are non-white\n- log.price: natural log of average house price\n- easting and northing: co-ordinates of the central point of the IG divided by 10000\n\nWe can model the **rate of occurrence of cancer** using the very same `glm` function:Â¨\n- now we use **poisson family distribution** to model counts\n- and we will include an **offset term** to the model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions.\n\n\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Read in and preview data\ncancer <- read.csv(\"data/lm/cancer.csv\")\nhead(cancer)\n##          IG Y_all     E_all pm10 smoke ethnic log.price  easting northing\n## 1 S02000260   133 106.17907 17.8  21.9   5.58  11.59910 26.16245 66.96574\n## 2 S02000261    38  62.43131 18.6  21.8   7.91  11.84940 26.29271 67.00278\n## 3 S02000262    97 120.00694 18.6  20.8   9.58  11.74106 26.21429 67.04280\n## 4 S02000263    80 109.10245 17.0  14.0  10.39  12.30138 25.45705 67.05938\n## 5 S02000264   181 149.77821 18.6  15.2   5.67  11.88449 26.12484 67.09280\n## 6 S02000265    77  82.31156 17.0  14.6   5.61  11.82004 25.37644 67.09826\n\n# fit Poisson regression\nepid1 <- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), \n             family = poisson, \n             data = cancer)\n\nprint(summary(epid1))\n## \n## Call:\n## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + \n##     northing + offset(log(E_all)), family = poisson, data = cancer)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -4.2011  -0.9338  -0.1763   0.8959   3.8416  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -0.8592657  0.8029040  -1.070 0.284531    \n## pm10         0.0500269  0.0066724   7.498 6.50e-14 ***\n## smoke        0.0033516  0.0009463   3.542 0.000397 ***\n## ethnic      -0.0049388  0.0006354  -7.773 7.66e-15 ***\n## log.price   -0.1034461  0.0169943  -6.087 1.15e-09 ***\n## easting     -0.0331305  0.0103698  -3.195 0.001399 ** \n## northing     0.0300213  0.0111013   2.704 0.006845 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 972.94  on 270  degrees of freedom\n## Residual deviance: 565.18  on 264  degrees of freedom\n## AIC: 2356.2\n## \n## Number of Fisher Scoring iterations: 4\n```\n:::\n\n\n\n**Rate ratio**\n\n- Similarly to logistic regression, it is common to look at the $e^\\beta$.\n- For instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient.\n- The ppm10 coefficient is positive, 0.0500269, indicating that cancer incidence rate increases with increased air pollution.\n- The rate ratio allows us to quantify by how much, here by a factor of $e^{0.0500269} = 1.05$.\n\n",
    "supporting": [
      "lm-glm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}