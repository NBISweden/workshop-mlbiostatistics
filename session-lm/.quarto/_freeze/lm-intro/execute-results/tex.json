{
  "hash": "cbbde2e3c2948729d7d24d521bc473bf",
  "result": {
    "markdown": "\n# Introduction to linear models\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Why linear models? \n\nWith linear models we can answer questions such as: \n\n  - is there a relationship between exposure and outcome, e.g. body weight and plasma volume?\n  - how strong is the relationship between the two variables?\n  - what will be a predicted value of the outcome given a new set of exposure values?\n  - how accurately can we predict outcome?\n  - which variables are associated with the response, e.g. is it body weight and height that can explain the plasma volume or is it just the body weight?\n\n## Statistical vs. deterministic relationship\n\nRelationships in probability and statistics can generally be one of three things: deterministic, random, or statistical:\n\n- a **deterministic** relationship involves **an exact relationship** between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation $Fahrenheit=\\frac{9}{5}\\cdot Celcius+32$\n- there is **no relationship** between variables in the **random relationship**, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year\n- **a statistical relationship** is a **mixture of deterministic and random relationship**, e.g. the savings that Olga has left in the bank account depend on Olga's monthly salary income (deterministic part) and the money spent on buying succulents (random part)\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between $x$ and $y$ is not perfect (increasing relationship), c)  statistical relationship between $x$ and $y$ is not perfect (decreasing relationship), d) random signal](lm-intro_files/figure-pdf/fig-relationship-1.pdf){#fig-relationship}\n:::\n:::\n\n\n\n## What linear models are and are not\n\n- A linear model is one in which the parameters appear linearly in the deterministic part of the model\n- e.g. **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \\beta x + \\epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled\n- linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared\n\nMore examples where model parameters appear linearly:\n\n\n\n::: {.cell .column-margin fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of a linear model: $y_i = x_i^2 + e_i$ showing that linear models can capture more than a straight line relationship](lm-intro_files/figure-pdf/fig-linear-adv-1.pdf){#fig-linear-adv}\n:::\n:::\n\n\n\n\n- $Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i$\n- $Y_i = \\alpha + \\beta x_i^2 \\epsilon$\n- $Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon$\n- $Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon$\n\nand an example on a non-linear model where parameter $\\beta$ appears in the exponent of $x_i$\n\n- $Y_i = \\alpha + x_i^\\beta +  \\epsilon$\n\n\n## Terminology\nThere are many terms and notations used interchangeably:\n\n- $y$ is being called:\n  - response\n  - outcome\n  - dependent variable\n\n- $x$ is being called:\n  - exposure\n  - explanatory variable\n  - dependent variable\n  - predictor\n  - covariate\n\n\n\n## Simple linear regression\n- It is used to check the association between **the numerical outcome and one numerical explanatory variable**\n- In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n\n\n\n\n\n:::{#exm-simple-lm}\n## Weight and plasma volume\n\nLet's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\n\nExample data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n```\n:::\n\n\n:::\n\n\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*.](lm-intro_files/figure-pdf/fig-lm-intro-example-1.pdf){#fig-lm-intro-example}\n:::\n:::\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable](lm-intro_files/figure-pdf/fig-lm-example-reg-1.pdf){#fig-lm-example-reg}\n:::\n:::\n\n\n\nThe equation for the red line is:\n\n\n$$Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8$$\n\n\nand in general:\n\n\n$$Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n$$\n\n\n\n- In other words, by finding the best-fitting straight line we are **building a statistical model** to represent the relationship between plasma volume ($Y$) and explanatory body weight variable ($x$)\n- If we were to use our model $Y_i=0.086 + 0.044 \\cdot x_i$ to find plasma volume given a weight of 58 kg (our first observation, $i=1$), we would notice that we would get $Y=0.086 +  0.044 \\cdot 58 = 2.638$, not exactly $2.75$ as we have for our first man in our dataset that we started with, i.e. $2.75 - 2.638 = 0.112 \\neq 0$.\n- We thus add to the above equation an **error term** to account for this and now we can write our **simple regression model** more formally as:\n\n\n\n$$Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$$ {#eq-lm}\nwhere:\n\n- $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n- $y$: is called: response, outcome, dependent variable\n- $\\alpha$ and $\\beta$ are **model coefficients**\n- and $\\epsilon_i$ is an **error terms**\n\n\n## Least squares\n- in the above **\"body weight - plasma volume\"** example, the values of $\\alpha$ and $\\beta$ have just appeared\n- in practice, $\\alpha$ and $\\beta$ values are unknown and we use data to **estimate these coefficients**, noting the estimates with a **hat**, $\\hat{\\alpha}$ and $\\hat{\\beta}$\n- **least squares** is one of the methods of parameters estimation, i.e. finding $\\hat{\\alpha}$ and $\\hat{\\beta}$\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line](lm-intro_files/figure-pdf/fig-reg-errors-1.pdf){#fig-reg-errors}\n:::\n:::\n\n\n<br>\n\nLet $\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:\n\n- Then $\\epsilon_i = y_i - \\hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model\n- RSS, the **residual sum of squares** is defined as: $$RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2$$ or\nequivalently as: $$RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2$$\n- the least squares approach chooses $\\hat{\\alpha}$ and $\\hat{\\beta}$ **to minimize the RSS**. With some calculus, a good video explanation for the interested ones is [here](https://www.youtube.com/watch?v=ewnc1cXJmGA), we get @thm-lss\n\n::: {#thm-lss}\n## Least squares estimates for a simple linear regression\n\n\n\n$$\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}$$\n\n$$\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}$$\n\n\n\nwhere:\n\n- $\\bar{x}$: mean value of $x$\n- $\\bar{y}$: mean value of $y$\n- $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2$\n- $S_{yy}$: sum of squares of $Y$ defined as  $S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2$\n- $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$\n\n:::\n\n\nWe can further re-write the above sum of squares to obtain\n\n- sum of squares of $X$, $$S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})$$\n- sum of products of $X$ and $Y$\n\n\n\n$$S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}$$\n\n\n\n\n::: {#exm-lss}\n\n## Least squares\n\nLet's try least squares method to find coefficient estimates in the **\"body weight and plasma volume example\"**\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# initial data\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# rename variables for convenience\nx <- weight\ny <- plasma\n\n# mean values of x and y\nx.bar <- mean(x)\ny.bar <- mean(y)\n\n# Sum of squares\nSxx <-  sum((x - x.bar)^2)\nSxy <- sum((x-x.bar)*(y-y.bar))\n\n# Coefficient estimates\nbeta.hat <- Sxy / Sxx\nalpha.hat <- y.bar - Sxy/Sxx*x.bar\n\n# Print estimated coefficients alpha and beta\nprint(alpha.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08572428\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nprint(beta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04361534\n```\n:::\n:::\n\n:::\n\n\nIn R we can use `lm()`, the built-in function, to fit a linear regression model and we can replace the above code with one line\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlm(plasma ~ weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nCoefficients:\n(Intercept)       weight  \n    0.08572      0.04362  \n```\n:::\n:::\n\n## Intercept and Slope\n- Linear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n- $\\alpha$ is known as the **intercept**\n- $\\beta$ is known as the **slope**\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)](lm-intro_files/figure-pdf/fig-lm-parameters-1.pdf){#fig-lm-parameters}\n:::\n:::\n\n## Hypothesis testing\n\n**Is there a relationship between the response and the predictor?**\n\n- the calculated $\\hat{\\alpha}$ and $\\hat{\\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**\n- their precision is measured by their **estimated standard errors**, `e.s.e`($\\hat{\\alpha}$) and `e.s.e`($\\hat{\\beta}$)\n- these estimated standard errors are used in **hypothesis testing** and in constructing **confidence and prediction intervals**\n\n**The most common hypothesis test** involves testing the ``null hypothesis`` of:\n\n- $H_0:$ There is no relationship between $X$ and $Y$\n- versus the ``alternative hypothesis`` $H_a:$ there is some relationship between $X$ and $Y$\n\n**Mathematically**, this corresponds to testing:\n\n- $H_0: \\beta=0$\n- versus $H_a: \\beta\\neq0$\n- since if $\\beta=0$ then the model $Y_i=\\alpha+\\beta x_i + \\epsilon_i$ reduces to $Y=\\alpha + \\epsilon_i$\n\n**Under the null hypothesis** $H_0: \\beta = 0$\n<!-- we have: $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)$$  -->\n![](figures/linear-models/lm-tstatistics.png)\n\n- $n$ is number of observations\n- $p$ is number of model parameters\n- $\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}$ is the ratio of the departure of the estimated value of a parameter, $\\hat\\beta$, from its hypothesized value, $\\beta$, to its standard error, called `t-statistics`\n- the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom\n\n::: {#exm-hypothesis-testing}\n\n## Hypothesis testing\n\nLet's look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the `R summary` of the model\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n```\n:::\n:::\n\n:::\n\n- Under `Estimate` we see estimates of our model coefficients, $\\hat{\\alpha}$ (intercept) and $\\hat{\\beta}$ (slope, here weight), followed by their estimated standard errors, `Std. Errors`\n- If we were to test if there is an **association between weight and plasma volume** we would write under the null hypothesis $H_0: \\beta = 0$ $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582$$\n- and we would **compare** `t-statistics` to `Student's t distribution` with $n-p = 8 - 2 = 6$ degrees of freedom (as we have 8 observations and two model parameters, $\\alpha$ and $\\beta$)\n- we can use **Student's t distribution table** or **R code** to obtain the associated *P*-value\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n2*pt(2.856582, df=6, lower=F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02893095\n```\n:::\n:::\n\n- here the observed t-statistics is large and therefore yields a small *P*-value, meaning that **there is sufficient evidence to reject null hypothesis in favor of the alternative** and conclude that there is a significant association between weight and plasma volume\n\n\n## Vector-matrix notations\n\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of **multiple regression**, with more than one explanatory variable in the model, it is **more efficient to use vectors and matrices to define the regression model**.\n\nLet's **rewrite** our simple linear regression model $Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n$ **into vector-matrix notation** in **6 steps**.\n\n1. First we rename our $\\alpha$ to $\\beta_0$ and $\\beta$ to $\\beta_1$ as it is easier to keep tracking the number of model parameters this way\n\n2. Then we notice that we actually have $n$ equations such as:\n\n\n$$y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1$$\n\n$$y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2$$\n\n$$y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3$$\n\n$$\\dots$$\n\n$$y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n$$\n\n\n\n3. We group all $Y_i$ and $\\epsilon_i$ into column vectors:\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$ and\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n\n4. We stack two parameters $\\beta_0$ and $\\beta_1$ into another column vector:$$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$$\n\n5. We append a vector of ones with the single predictor for each $i$ and create a matrix with two columns called **design matrix** $$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}$$\n\n6. We write our linear model in a vector-matrix notations as:\n\n\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n\n\n::: {#def-vector-matrix-lm}\n\n## vector matrix form of the linear model\n\nThe vector-matrix representation of a linear model with $p-1$ predictors can be written as\n\n\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n\n\nwhere:\n\n- $\\mathbf{Y}$ is $n \\times1$ vector of observations\n- $\\mathbf{X}$ is $n \\times p$ **design matrix**\n- $\\boldsymbol\\beta$ is $p \\times1$ vector of parameters\n- $\\boldsymbol\\epsilon$ is $n \\times1$ vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, $\\sigma^2$)\n\nIn full, the above vectors and matrix have the form:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1    \\\\\n  \\vdots \\\\\n  \\beta_{p}\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_{1,1} & \\dots & x_{1,p-1} \\\\\n  1 & x_{2,1} & \\dots & x_{2,p-1} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & x_{n,1} & \\dots & x_{n,p-1}\n\\end{bmatrix}$\n\n:::\n\n\n::: {#thm-lss-vector-matrix}\n\n## Least squares in vector-matrix notation\n\nThe least squares estimates for a linear regression of the form:\n\n\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n\n\nis given by:\n\n\n$$\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n\n\n\n:::\n\n\n::: {#exm-vector-matrix-notation}\n\n## vector-matrix-notation\n\nFollowing the above definition we can write the **\"weight - plasma volume model\"** as:\n\n\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n\nwhere:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n 2.75  \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12\n\\end{bmatrix}$\n\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{8}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & 58.0 \\\\\n  1 & 70.0 \\\\\n  1 & 74.0 \\\\\n  1 & 63.5 \\\\\n  1 & 62.0 \\\\\n  1 & 70.5 \\\\\n  1 & 71.0 \\\\\n  1 & 66.0 \\\\\n\\end{bmatrix}$\n\nand we can estimate model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n\nWe can do it by hand or in `R` as follows:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,] 2.75\n[2,] 2.86\n[3,] 3.37\n[4,] 2.76\n[5,] 2.62\n[6,] 3.49\n[7,] 3.05\n[8,] 3.12\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       weight\n[1,] 1   58.0\n[2,] 1   70.0\n[3,] 1   74.0\n[4,] 1   63.5\n[5,] 1   62.0\n[6,] 1   70.5\n[7,] 1   71.0\n[8,] 1   66.0\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n       0.08572428\nweight 0.04361534\n```\n:::\n:::\n\n:::\n\n## Confidence intervals and prediction intervals\n\n- when we estimate coefficients we can also find their **confidence intervals**, typically 95\\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter\n- we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval\n\n::: {exm-prediction-and-intervals}\n## prediction and intervals\n\nLet's:\n\n- find confidence intervals for our coefficient estimates\n- predict plasma volume for a men weighting 60 kg\n- find prediction interval\n- plot original data, fitted regression model, predicted observation and prediction interval\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# fit regression model\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# find confidence intervals for the model coefficients\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   2.5 %     97.5 %\n(Intercept) -2.419908594 2.59135716\nweight       0.006255005 0.08097567\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# predict plasma volume for a new observation of 60 kg\n# we have to create data frame with a variable name matching the one used to build the model\nnew.obs <- data.frame(weight = 60)\npredict(model, newdata = new.obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n2.702645 \n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# find prediction intervals\nprediction.interval <- predict(model, newdata = new.obs,  interval = \"prediction\")\nprint(prediction.interval)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 2.702645 2.079373 3.325916\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# plot the original data, fitted regression and predicted value\nplot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\", ylim=c(2,4))\nlines(weight, model$fitted.values, col=\"red\") # fitted model in red\npoints(new.obs, predict(model, newdata = new.obs), pch=19, col=\"blue\") # predicted value at 60kg\nsegments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval\n```\n\n::: {.cell-output-display}\n![](lm-intro_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-pos='H'}\n:::\n:::\n\n",
    "supporting": [
      "lm-intro_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}