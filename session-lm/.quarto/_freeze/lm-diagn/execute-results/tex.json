{
  "hash": "748e3472c33297d84f2217cad4af6f98",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Model diagnostics\n\n**Aims**\n\n- to introduce concepts of linear models summary and assumptions\n\n**Learning outcomes**\n\n- to able to interpret $R^2$ and $R^2(adj)$ values\n- state the assumptions of a linear model and assess them using residual plots\n\n## Assessing model fit\n- earlier we learned how to estimate parameters in a liner model using least squares\n- now we will consider how to assess the goodness of fit of a model\n- we do that by calculating the amount of variability in the response that is explained by the model\n\n## $R^2$: summary of the fitted model\n- considering a simple linear regression, the simplest model, **Model 0**, we could consider fitting is $$Y_i = \\beta_0+ \\epsilon_i$$ that corresponds to a line that run through the data but lies parallel to the horizontal axis\n- in our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lm-diagn_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-align='center'}\n:::\n:::\n\n\n\n- TSS, denoted **Total corrected sum-of-squares** is the residual sum-of-squares for Model 0\n\n\n$$S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}$$ corresponding the to the sum of squared distances to the purple line\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lm-diagn_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-align='center'}\n:::\n:::\n\n- Fitting **Model 1** of the form $$Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$$ we have earlier defined\n- **RSS**, the residual sum-of-squares as:\n\n\n$$RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n\n\n- that corresponds to the squared distances between the observed values $y_i, \\dots,y_n$ to fitted values $\\hat{y_1}, \\dots \\hat{y_n}$, i.e. distances to the red fitted line\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lm-diagn_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-align='center'}\n:::\n:::\n\n::: {#def-r2}\n\n## $R^2$\n\nA simple but useful measure of model fit is given by $$R^2 = 1 - \\frac{RSS}{TSS}$$ where:\n\n- RSS is the residual sum-of-squares for Model 1, the fitted model of interest\n- TSS is the sum of squares of the **null model**\n\n:::\n\n- $R^2$ quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model\n- $R^2$ is also referred as **coefficient of determination**\n- It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data\n- Values of $R^2$ approaching 1 indicate the model to be a good fit\n- Values of $R^2$ less than 0.5 suggest that the model gives rather a poor fit to the data\n\n## $R^2$ and correlation coefficient\n\n::: {#thm-r2}\n## $R^2$\n\nIn the case of simple linear regression:\n\nModel 1: $Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$\n\n\n$$R^2 = r^2$$\n\n\nwhere:\n\n- $R^2$ is the coefficient of determination\n- $r^2$ is the sample correlation coefficient\n\n:::\n\n\n## $R^2(adj)$\n- in the case of multiple linear regression, where there is more than one explanatory variable in the model\n- we are using the adjusted version of $R^2$ to assess the model fit\n- as the number of explanatory variables increase, $R^2$ also increases\n- $R^2(adj)$ takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model\n\n\n::: {#thm-r2adj}\n\n## $R^2(adj)$ \n\nFor any multiple linear regression\n$$Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i$$ $R^2(adj)$ is defined as\n\n$$R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}$$ where\n\n- $p$ is the number of independent predictors, i.e. the number of variables in the model, excluding the constant\n\n$R^2(adj)$ can also be calculated from $R^2$:\n\n\n$$R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}$$\n\n\n\n:::\n\nWe can calculate the values in R and compare the results to the output of linear regression\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhtwtgen <- read.csv(\"data/lm/heights_weights_genders.csv\")\nhead(htwtgen)\n##   Gender   Height   Weight\n## 1   Male 73.84702 241.8936\n## 2   Male 68.78190 162.3105\n## 3   Male 74.11011 212.7409\n## 4   Male 71.73098 220.0425\n## 5   Male 69.88180 206.3498\n## 6   Male 67.25302 152.2122\nattach(htwtgen)\n\n## Simple linear regression\nmodel.simple <- lm(Height ~ Weight, data=htwtgen)\n\n# TSS\nTSS <- sum((Height - mean(Height))^2)\n\n# RSS\n# residuals are returned in the model type names(model.simple)\nRSS <- sum((model.simple$residuals)^2)\nR2 <- 1 - (RSS/TSS)\n\nprint(R2)\n## [1] 0.8551742\nprint(summary(model.simple))\n## \n## Call:\n## lm(formula = Height ~ Weight, data = htwtgen)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8142 -0.9907  0.0263  0.9918  5.5950 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 4.848e+01  7.507e-02   645.8   <2e-16 ***\n## Weight      1.108e-01  4.561e-04   243.0   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.464 on 9998 degrees of freedom\n## Multiple R-squared:  0.8552,\tAdjusted R-squared:  0.8552 \n## F-statistic: 5.904e+04 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n## Multiple regression\nmodel.multiple <- lm(Height ~ Weight + Gender, data=htwtgen)\nn <- length(Weight)\np <- 1\n\nRSS <- sum((model.multiple$residuals)^2)\nR2_adj <- 1 - (RSS/(n-p-1))/(TSS/(n-1))\n\nprint(R2_adj)\n## [1] 0.8608793\nprint(summary(model.multiple))\n## \n## Call:\n## lm(formula = Height ~ Weight + Gender, data = htwtgen)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.4956 -0.9583  0.0126  0.9867  5.8358 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 47.0306678  0.1025161  458.76   <2e-16 ***\n## Weight       0.1227594  0.0007396  165.97   <2e-16 ***\n## GenderMale  -0.9628643  0.0474947  -20.27   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.435 on 9997 degrees of freedom\n## Multiple R-squared:  0.8609,\tAdjusted R-squared:  0.8609 \n## F-statistic: 3.093e+04 on 2 and 9997 DF,  p-value: < 2.2e-16\n```\n:::\n\n## The assumptions of a linear model\n- up until now we were fitting models and discussed how to assess the model fit\n- before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data\n- informally we have been using box plots and scatter plots to look at the data\n- there are however formal definitions of the assumptions\n\n**Assumption A: The deterministic part of the model captures all the non-random structure in the data**\n\n- this implies that the **mean of the errors $\\epsilon_i$** is zero\n- it applies only over the range of explanatory variables\n\n**Assumption B: the scale of variability of the errors is constant at all values of the explanatory variables**\n\n- practically we are looking at whether the observations are equally spread on both side of the regression line\n\n**Assumption C: the errors are independent**\n\n- broadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another\n\n**Assumptions D: the errors are normally distributed**\n\n- this will allow us to describe the variation in the model's parameters estimates and therefore make inferences about the population from which our sample was taken\n\n**Assumption E: the values of the explanatory variables are recorded without error**\n\n- this one is not possible to check via examining the data, instead we have to consider the nature of the experiment\n\n\n\n## Checking assumptions\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**. We use plots such as:\n\n1. Histograms or normal probability plots of $\\hat{\\epsilon_i}$\n- useful to check the assumption of normality\n\n2. Plots of $\\hat{\\epsilon_i}$ versus the fitted values $\\hat{y_i}$\n- used to detect changes in error variance\n- used to check if the mean of the errors is zero\n\n3. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{ij}$\n- this helps to check that the variable $x_j$ has a linear relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{kj}$ that is **not** in the model\n- this helps to check whether the additional variable $x_k$ might have a relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ in the order of the observations were collected\n- this is useful to check whether errors might be correlated over time\n\nLet's look at the \"good\" example going back to our data of protein levels during pregnancy\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# read in data\ndata.protein <- read.csv(\"data/lm/protein.csv\")\n\nprotein <- data.protein$Protein # our Y\ngestation <- data.protein$Gestation # our X\n\nmodel <- lm(protein ~ gestation)\n\n# plot diagnostic plots of the linear model\n# by default plot(model) calls four diagnostics plots\n# par() divides plot window in 2 x 2 grid\npar(mfrow=c(2,2))\nplot(model)\n```\n\n::: {.cell-output-display}\n![](lm-diagn_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n- the residual plots provides examples of a situation where the assumptions appear to be met\n- the linear regression appears to describe data quite well\n- there is no obvious trend of any kind in the residuals vs. fitted values (the shape is scattered)\n- points lie reasonably well along the line in the normal probability plot, hence normality appears to be met\n\n**Examples of assumptions not being met**\n\n\n::: {.cell layout-align=\"center\" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case](figures/linear-models/lm-assumptions-01.png){fig-align='center' width=8.62in}\n:::\n:::\n\n::: {.cell layout-align=\"center\" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of non-constant variance](figures/linear-models/lm-assumptions-02.png){fig-align='center' width=10.76in}\n:::\n:::\n\n::: {.cell layout-align=\"center\" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected](figures/linear-models/lm-assumptions-03.png){fig-align='center' width=8.34in}\n:::\n:::\n\n\n## Influential observations\n- Sometimes individual observations can exert a great deal of influence on the fitted model\n- One routine way of checking for this is to fit the model $n$ times, missing out each observation in turn\n- If we removed i-th observation and compared the fitted value from the full model, say $\\hat{y_j}$ to those obtained by removing this point, denoted $\\hat{y_{j(i)}}$ then\n- observations with a high Cook's distance (measuring the effect of deleting a given observation) could be influential\n\nLet's remove some observation with higher Cook's distance from protein data set, re-fit our model and compare the diagnostics plots\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# observations to be removed (based on Residuals vs. Leverage plot)\nobs <- c(18,7)\n\n# fit models removing observations\nmodel.2 <- lm(protein[-obs] ~ gestation[-obs])\n\n# plot diagnostics plot\npar(mfrow=c(2,2))\nplot(model.2)\n```\n\n::: {.cell-output-display}\n![](lm-diagn_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Selecting best model\n- We have learned what linear models are, how to find estimates and interpret model coefficients and how to check for the overall relationship between response and predictors. We also know how to assess model fit, check model assumptions and find potential outliers. Given a set of predictors, e.g. many genes, how do we arrive at the best model?\n- As a rule of thumb, we want a model that **fits the data best and is as simple as possible**, meaning it contains only relevant predictors.\n- In practice, this means, that for smaller data sets, e.g. with up to 10 predictors, one works with **manually** trying different models, including different subsets of predictors, interactions terms and/or their transformations. \n- When the number of predictors is large, one can try **automated approaches of feature selection** like forward selection or stepwise regression, the last one demonstrated in the exercises below. \n- Finally, as we will learn later in the course, we can use **regularization techniques** that allow including all parameters in the model but constrain (regularizes) coefficient estimates towards zero for the less relevant predictors, preventing building complex models and thus overfitting. \n\n\n",
    "supporting": [
      "lm-diagn_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}