{"title":"Introduction to linear models","markdown":{"headingText":"Introduction to linear models","containsRefs":false,"markdown":"\n\n```{r}\n#| message: false\n#| warning: false\n#| echo: false\n\n# load libraries\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(ggplot2)\n\nfont.size <- 12\ncol.blue.light <- \"#a6cee3\"\ncol.blue.dark <- \"#1f78b4\"\nmy.ggtheme <- theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \"top\") \n```\n\n## Why linear models? \n\nWith linear models we can answer questions such as: \n\n  - is there a relationship between exposure and outcome, e.g. body weight and plasma volume?\n  - how strong is the relationship between the two variables?\n  - what will be a predicted value of the outcome given a new set of exposure values?\n  - how accurately can we predict outcome?\n  - which variables are associated with the response, e.g. is it body weight and height that can explain the plasma volume or is it just the body weight?\n\n## Statistical vs. deterministic relationship\n\nRelationships in probability and statistics can generally be one of three things: deterministic, random, or statistical:\n\n- a **deterministic** relationship involves **an exact relationship** between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation $Fahrenheit=\\frac{9}{5}\\cdot Celcius+32$\n- there is **no relationship** between variables in the **random relationship**, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year\n- **a statistical relationship** is a **mixture of deterministic and random relationship**, e.g. the savings that Olga has left in the bank account depend on Olga's monthly salary income (deterministic part) and the money spent on buying succulents (random part)\n\n\n```{r}\n#| label: fig-relationship\n#| fig-cap: \"Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between $x$ and $y$ is not perfect (increasing relationship), c)  statistical relationship between $x$ and $y$ is not perfect (decreasing relationship), d) random signal\"\n#| echo: false\n#| fig-height: 8\n\n# prepare plot space\npar(mfrow=c(2,2))\n\n# Deterministic relationship example\nx_celcius <- seq(from=0, to=50, by=5)\ny_fahr <- 9/5*x_celcius+32\nplot(x_celcius, y_fahr, type=\"b\", pch=19, xlab=\"Celcius\", ylab=\"Fahrenheit\", main=\"a) Deterministic\", cex.main=0.8, las=2)\n\n# Statistical relationship (increasing)\nx <- seq(from=0, to=100, by=5)\ny_increasing <- 2*x + rnorm(length(x), mean=100, sd=25)\nplot(x, y_increasing, pch=19, xlab=\"x\", ylab=\"y\", main=\"b) Statistical\", cex.main=0.8, las=2)\n\n# Statistical relationship (decreasing)\ny_decreasing <- -2*x + rnorm(length(x), mean=100, sd=25)\nplot(x, y_decreasing, pch=19, xlab=\"x\", ylab=\"y\", main=\"c) Statistical\", cex.main=0.8, las=2)\n\n# Statistical relationship (random)\ny_random <- - rnorm(length(x), mean=100, sd=25)\nplot(x, y_random, pch=19, xlab=\"x\", ylab=\"y\", main=\"d) Random\", cex.main=0.8, las=2)\n\n```\n\n## What linear models are and are not\n\n- A linear model is one in which the parameters appear linearly in the deterministic part of the model\n- e.g. **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \\beta x + \\epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled\n- linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared\n\nMore examples where model parameters appear linearly:\n\n```{r}\n#| label: fig-linear-adv\n#| fig-cap: \"Example of a linear model: $y_i = x_i^2 + e_i$ showing that linear models can capture more than a straight line relationship\"\n#| fig-cap-location: margin\n#| warning: false\n#| column: margin\n#| echo: false\n\n# advance 1\nx <- seq(-10, 10, 1)\ny <- x^2 + rnorm(length(x), mean(x), 10)\ndata.xy <- data.frame(x=x, y = y, ymodel = x^2)\np.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel), color = col.blue.dark) + \n  theme_classic() +\n  my.ggtheme\n\nplot(p.adv1)\n\n```\n\n\n- $Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i$\n- $Y_i = \\alpha + \\beta x_i^2 \\epsilon$\n- $Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon$\n- $Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon$\n\nand an example on a non-linear model where parameter $\\beta$ appears in the exponent of $x_i$\n\n- $Y_i = \\alpha + x_i^\\beta +  \\epsilon$\n\n\n## Terminology\nThere are many terms and notations used interchangeably:\n\n- $y$ is being called:\n  - response\n  - outcome\n  - dependent variable\n\n- $x$ is being called:\n  - exposure\n  - explanatory variable\n  - dependent variable\n  - predictor\n  - covariate\n\n\n\n## Simple linear regression\n- It is used to check the association between **the numerical outcome and one numerical explanatory variable**\n- In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n\n\n\n\n\n:::{#exm-simple-lm}\n## Weight and plasma volume\n\nLet's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\n\nExample data:\n```{r}\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n```\n:::\n\n```{r}\n#| label: fig-lm-intro-example\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*.\"\n#| fig-cap-location: margin\n#| echo: false\n#| fig-width: 4\n#| fig-heigth: 4\n\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\n\n```\n\n```{r}\n#| label: fig-lm-example-reg\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable\"\n#| fig-cap-location: margin\n#| echo: false\n#| fig-width: 4\n#| fig-heigth: 4\n\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\", panel.first = grid())\n\nreg1 <- lm(plasma ~ weight)\na <- reg1$coefficients[1]\nb <- reg1$coefficients[2]\n\nabline(a=a+0.1 , b + 0.001, col=\"gray\")\nabline(a=a+0.1 , b + 0.0001, col=\"gray\")\n#abline(a=a , b + 0.00015, col=\"gray\")\nabline(a=a+0.1 , b + 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.001, col=\"gray\")\nabline(a=a, b - 0.001, col=\"gray\")\nabline(a=a+0.5 , b , col=\"gray\")\nabline(a=a-0.5 , b , col=\"gray\")\n\nabline(lm(plasma~weight), col=\"red\") # regression line\npoints(weight, plasma, pch=19)\n\n\n```\n\nThe equation for the red line is:\n$$Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8$$\nand in general:\n$$Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n$$\n\n- In other words, by finding the best-fitting straight line we are **building a statistical model** to represent the relationship between plasma volume ($Y$) and explanatory body weight variable ($x$)\n- If we were to use our model $Y_i=0.086 + 0.044 \\cdot x_i$ to find plasma volume given a weight of 58 kg (our first observation, $i=1$), we would notice that we would get $Y=0.086 +  0.044 \\cdot 58 = 2.638$, not exactly $2.75$ as we have for our first man in our dataset that we started with, i.e. $2.75 - 2.638 = 0.112 \\neq 0$.\n- We thus add to the above equation an **error term** to account for this and now we can write our **simple regression model** more formally as:\n\n$$Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$$ {#eq-lm}\nwhere:\n\n- $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n- $y$: is called: response, outcome, dependent variable\n- $\\alpha$ and $\\beta$ are **model coefficients**\n- and $\\epsilon_i$ is an **error terms**\n\n\n## Least squares\n- in the above **\"body weight - plasma volume\"** example, the values of $\\alpha$ and $\\beta$ have just appeared\n- in practice, $\\alpha$ and $\\beta$ values are unknown and we use data to **estimate these coefficients**, noting the estimates with a **hat**, $\\hat{\\alpha}$ and $\\hat{\\beta}$\n- **least squares** is one of the methods of parameters estimation, i.e. finding $\\hat{\\alpha}$ and $\\hat{\\beta}$\n\n```{r}\n#| label: fig-reg-errors\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line\"\n#| fig-cap-location: margin\n#| echo: false\n#| warning: false\n#| message: false\n#| fig-width: 4\n#| fig-height: 3\n\ndata.reg <- data.frame(plasma=plasma, weight=weight)\nfit.reg <- lm(plasma~weight, data=data.reg)\ndata.reg$predicted <- predict(fit.reg)\ndata.reg$residuals <- residuals((fit.reg))\n\nggplot(data.reg, aes(x=weight, plasma)) + geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  geom_segment(aes(xend = weight, yend = predicted), color=\"blue\") +\n  geom_point(aes(y = predicted), shape = 1) +\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_bw() + xlab(\"body weight [kg]\") + ylab(\"plasma volume [liters]\")\n\n```\n\n\n<br>\n\nLet $\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:\n\n- Then $\\epsilon_i = y_i - \\hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model\n- RSS, the **residual sum of squares** is defined as: $$RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2$$ or\nequivalently as: $$RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2$$\n- the least squares approach chooses $\\hat{\\alpha}$ and $\\hat{\\beta}$ **to minimize the RSS**. With some calculus, a good video explanation for the interested ones is [here](https://www.youtube.com/watch?v=ewnc1cXJmGA), we get @thm-lss\n\n::: {#thm-lss}\n## Least squares estimates for a simple linear regression\n\n$$\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}$$\n$$\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}$$\n\nwhere:\n\n- $\\bar{x}$: mean value of $x$\n- $\\bar{y}$: mean value of $y$\n- $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2$\n- $S_{yy}$: sum of squares of $Y$ defined as  $S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2$\n- $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$\n\n:::\n\n\nWe can further re-write the above sum of squares to obtain\n\n- sum of squares of $X$, $$S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})$$\n- sum of products of $X$ and $Y$\n\n$$S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}$$\n\n\n::: {#exm-lss}\n\n## Least squares\n\nLet's try least squares method to find coefficient estimates in the **\"body weight and plasma volume example\"**\n\n```{r}\n#| code-fold: false\n\n# initial data\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# rename variables for convenience\nx <- weight\ny <- plasma\n\n# mean values of x and y\nx.bar <- mean(x)\ny.bar <- mean(y)\n\n# Sum of squares\nSxx <-  sum((x - x.bar)^2)\nSxy <- sum((x-x.bar)*(y-y.bar))\n\n# Coefficient estimates\nbeta.hat <- Sxy / Sxx\nalpha.hat <- y.bar - Sxy/Sxx*x.bar\n\n# Print estimated coefficients alpha and beta\nprint(alpha.hat)\nprint(beta.hat)\n\n```\n\n:::\n\n\nIn R we can use `lm()`, the built-in function, to fit a linear regression model and we can replace the above code with one line\n\n```{r}\n#| code-fold: false\nlm(plasma ~ weight)\n```\n\n## Intercept and Slope\n- Linear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n- $\\alpha$ is known as the **intercept**\n- $\\beta$ is known as the **slope**\n\n```{r}\n#| label: fig-lm-parameters\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)\"\n#| echo: false\n#| fig-height: 7\n#| fig-width: 7\n\nmodel <- lm(plasma ~ weight)\nalpha <- model$coefficients[1]\nbeta <- model$coefficients[2]\n\npar(mfcol=c(2,2), mar=c(4,4,3,2))\n\n# Values from regression model: plasma_volume = 0.0857 + 0.043615*x\n\n# Fitted line\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\nabline(lm(plasma~weight), col=\"red\") # regression line\ntext(65, 3.3, \"plasma = 0.0857 + 0.0436 * weight\", cex=1)\n\n# Beta 1 example b\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid(), xlim=c(60, 70), ylim=c(2.8, 3.2))\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h = alpha + beta * 65, col = \"blue\",  lty = 3)\nabline(h = alpha + beta * 66, col = \"blue\",  lty = 3)\nsegments(x0=65, y0=alpha + beta * 65, x1=66, y1=alpha + beta * 65, col=\"blue\")\nsegments(x0=66, y0=alpha + beta * 65, x1=66, y1=alpha + beta * 66, col=\"blue\")\ntext(67, 2.94, expression(beta), cex=1.2, col=\"blue\")\ntext(61, alpha + beta * 65 - 0.02, round(alpha + beta * 65, 2), cex=1.2, col=\"blue\")\ntext(61, alpha + beta * 66 + 0.02, round(alpha + beta * 66, 2), cex=1.2, col=\"blue\")\n\n\n# Beta 1 example a\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h = alpha + beta * 65, col = \"blue\",  lty = 3)\nabline(h = alpha + beta * 70, col = \"blue\",  lty = 3)\nsegments(x0=65, y0=alpha + beta * 65, x1=70, y1=alpha + beta * 65, col=\"blue\")\nsegments(x0=70, y0=alpha + beta * 65, x1=70, y1=alpha + beta * 70, col=\"blue\")\ntext(72, 2.95, expression(beta), cex=1.2, col=\"blue\")\ntext(60, alpha + beta * 65 + 0.05, round(alpha + beta * 65, 2), cex=1.2, col=\"blue\")\ntext(60, alpha + beta * 70 + 0.05, round(alpha + beta * 70, 2), cex=1.2, col=\"blue\")\n\n\n# Beta 0 example a\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid(), xlim=c(-20, 80), ylim=c(0, 5))\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h=alpha, col=\"blue\", lty = 3) # regression line\nsegments(x0=65, y0=2.92, x1=66, y1=2.92, col=\"blue\")\nsegments(x0=66, y0=2.92, x1=66, y1=2.964, col=\"blue\")\ntext(15, 0.4, expression(alpha), cex=1.2, col=\"blue\")\ntext(33, 0.5, paste(\"= \", round(alpha,3), sep=\"\"), cex=1.2, col=\"blue\")\n\n```\n\n## Hypothesis testing\n\n**Is there a relationship between the response and the predictor?**\n\n- the calculated $\\hat{\\alpha}$ and $\\hat{\\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**\n- their precision is measured by their **estimated standard errors**, `e.s.e`($\\hat{\\alpha}$) and `e.s.e`($\\hat{\\beta}$)\n- these estimated standard errors are used in **hypothesis testing** and in constructing **confidence and prediction intervals**\n\n**The most common hypothesis test** involves testing the ``null hypothesis`` of:\n\n- $H_0:$ There is no relationship between $X$ and $Y$\n- versus the ``alternative hypothesis`` $H_a:$ there is some relationship between $X$ and $Y$\n\n**Mathematically**, this corresponds to testing:\n\n- $H_0: \\beta=0$\n- versus $H_a: \\beta\\neq0$\n- since if $\\beta=0$ then the model $Y_i=\\alpha+\\beta x_i + \\epsilon_i$ reduces to $Y=\\alpha + \\epsilon_i$\n\n**Under the null hypothesis** $H_0: \\beta = 0$\n<!-- we have: $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)$$  -->\n![](figures/linear-models/lm-tstatistics.png)\n\n- $n$ is number of observations\n- $p$ is number of model parameters\n- $\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}$ is the ratio of the departure of the estimated value of a parameter, $\\hat\\beta$, from its hypothesized value, $\\beta$, to its standard error, called `t-statistics`\n- the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom\n\n::: {#exm-hypothesis-testing}\n\n## Hypothesis testing\n\nLet's look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the `R summary` of the model\n\n```{r}\n#| code-fold: false\n\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n\n```\n\n:::\n\n- Under `Estimate` we see estimates of our model coefficients, $\\hat{\\alpha}$ (intercept) and $\\hat{\\beta}$ (slope, here weight), followed by their estimated standard errors, `Std. Errors`\n- If we were to test if there is an **association between weight and plasma volume** we would write under the null hypothesis $H_0: \\beta = 0$ $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582$$\n- and we would **compare** `t-statistics` to `Student's t distribution` with $n-p = 8 - 2 = 6$ degrees of freedom (as we have 8 observations and two model parameters, $\\alpha$ and $\\beta$)\n- we can use **Student's t distribution table** or **R code** to obtain the associated *P*-value\n\n```{r}\n#| code-fold: false\n2*pt(2.856582, df=6, lower=F)\n```\n\n- here the observed t-statistics is large and therefore yields a small *P*-value, meaning that **there is sufficient evidence to reject null hypothesis in favor of the alternative** and conclude that there is a significant association between weight and plasma volume\n\n\n## Vector-matrix notations\n\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of **multiple regression**, with more than one explanatory variable in the model, it is **more efficient to use vectors and matrices to define the regression model**.\n\nLet's **rewrite** our simple linear regression model $Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n$ **into vector-matrix notation** in **6 steps**.\n\n1. First we rename our $\\alpha$ to $\\beta_0$ and $\\beta$ to $\\beta_1$ as it is easier to keep tracking the number of model parameters this way\n\n2. Then we notice that we actually have $n$ equations such as:\n$$y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1$$\n$$y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2$$\n$$y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3$$\n$$\\dots$$\n$$y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n$$\n\n3. We group all $Y_i$ and $\\epsilon_i$ into column vectors:\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$ and\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n\n4. We stack two parameters $\\beta_0$ and $\\beta_1$ into another column vector:$$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$$\n\n5. We append a vector of ones with the single predictor for each $i$ and create a matrix with two columns called **design matrix** $$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}$$\n\n6. We write our linear model in a vector-matrix notations as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n::: {#def-vector-matrix-lm}\n\n## vector matrix form of the linear model\n\nThe vector-matrix representation of a linear model with $p-1$ predictors can be written as\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nwhere:\n\n- $\\mathbf{Y}$ is $n \\times1$ vector of observations\n- $\\mathbf{X}$ is $n \\times p$ **design matrix**\n- $\\boldsymbol\\beta$ is $p \\times1$ vector of parameters\n- $\\boldsymbol\\epsilon$ is $n \\times1$ vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, $\\sigma^2$)\n\nIn full, the above vectors and matrix have the form:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1    \\\\\n  \\vdots \\\\\n  \\beta_{p}\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_{1,1} & \\dots & x_{1,p-1} \\\\\n  1 & x_{2,1} & \\dots & x_{2,p-1} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & x_{n,1} & \\dots & x_{n,p-1}\n\\end{bmatrix}$\n\n:::\n\n\n::: {#thm-lss-vector-matrix}\n\n## Least squares in vector-matrix notation\n\nThe least squares estimates for a linear regression of the form:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nis given by:\n$$\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n\n:::\n\n\n::: {#exm-vector-matrix-notation}\n\n## vector-matrix-notation\n\nFollowing the above definition we can write the **\"weight - plasma volume model\"** as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\nwhere:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n 2.75  \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12\n\\end{bmatrix}$\n\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{8}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & 58.0 \\\\\n  1 & 70.0 \\\\\n  1 & 74.0 \\\\\n  1 & 63.5 \\\\\n  1 & 62.0 \\\\\n  1 & 70.5 \\\\\n  1 & 71.0 \\\\\n  1 & 66.0 \\\\\n\\end{bmatrix}$\n\nand we can estimate model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n\nWe can do it by hand or in `R` as follows:\n\n```{r}\n#| code-fold: false\n\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\nprint(X)\n\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n\n```\n\n:::\n\n## Confidence intervals and prediction intervals\n\n- when we estimate coefficients we can also find their **confidence intervals**, typically 95\\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter\n- we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval\n\n::: {exm-prediction-and-intervals}\n## prediction and intervals\n\nLet's:\n\n- find confidence intervals for our coefficient estimates\n- predict plasma volume for a men weighting 60 kg\n- find prediction interval\n- plot original data, fitted regression model, predicted observation and prediction interval\n\n```{r}\n#| code-fold: false\n\n# fit regression model\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n\n# find confidence intervals for the model coefficients\nconfint(model)\n\n# predict plasma volume for a new observation of 60 kg\n# we have to create data frame with a variable name matching the one used to build the model\nnew.obs <- data.frame(weight = 60)\npredict(model, newdata = new.obs)\n\n# find prediction intervals\nprediction.interval <- predict(model, newdata = new.obs,  interval = \"prediction\")\nprint(prediction.interval)\n\n# plot the original data, fitted regression and predicted value\nplot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\", ylim=c(2,4))\nlines(weight, model$fitted.values, col=\"red\") # fitted model in red\npoints(new.obs, predict(model, newdata = new.obs), pch=19, col=\"blue\") # predicted value at 60kg\nsegments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval\n\n```\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"lm-intro.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.36","bibliography":["references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"lm-intro.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{}}}}}