{"title":"Introduction to linear models","markdown":{"yaml":{"output":"html_document","editor_options":{"chunk_output_type":"console"}},"headingText":"Introduction to linear models","containsRefs":false,"markdown":"\n\n\n```{r}\n#| message: false\n#| warning: false\n#| echo: false\n\n# load libraries\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(ggplot2)\n\nfont.size <- 12\ncol.blue.light <- \"#a6cee3\"\ncol.blue.dark <- \"#1f78b4\"\nmy.ggtheme <- theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \"top\") + \n        theme_bw()\n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2m) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %>%\n  na.omit()\n\n```\n\n## Why linear models? \n\nWith linear models we can answer questions such as: \n\n  - is there a relationship between exposure and outcome, e.g. height and weight?\n  - how strong is the relationship between the two variables?\n  - what will be a predicted value of the outcome given a new set of exposure values?\n  - how accurately can we predict outcome?\n  - which variables are associated with the response, e.g. is it only height that explains weight or could it be height and age that are both associated with the response?\n  \n```{r}\n#| label: fig-scatter\n#| message: false\n#| warning: false\n#| fig-align: center\n#| fig-cap: \"Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA.\"\n#| fig-cap-location: bottom\n\ndata_diabetes %>%\n  ggplot(aes(x = height, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  my.ggtheme + \n  xlab(\"height [m]\") + \n  ylab(\"weight [kg]\")\n\n```\n\n\n## Statistical vs. deterministic relationship\n\nRelationships in probability and statistics can generally be one of three things: deterministic, random, or statistical:\n\n- a **deterministic** relationship involves **an exact relationship** between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation $Fahrenheit=\\frac{9}{5}\\cdot Celcius+32$\n- there is **no relationship** between variables in the **random relationship**, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year\n- **a statistical relationship** is a **mixture of deterministic and random relationship**, e.g. the savings that Olga has left in the bank account depend on Olga's monthly salary income (deterministic part) and the money spent on buying succulents (random part)\n\n\n```{r}\n#| label: fig-relationship\n#| fig-cap: \"Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between $x$ and $y$ is not perfect (increasing relationship), c)  statistical relationship between $x$ and $y$ is not perfect (decreasing relationship), d) random signal\"\n#| echo: false\n#| fig-height: 8\n\n# prepare plot space\npar(mfrow=c(2,2))\n\n# Deterministic relationship example\nx_celcius <- seq(from=0, to=50, by=5)\ny_fahr <- 9/5*x_celcius+32\nplot(x_celcius, y_fahr, type=\"b\", pch=19, xlab=\"Celcius\", ylab=\"Fahrenheit\", main=\"a) Deterministic\", cex.main=0.8, las=2)\n\n# Statistical relationship (increasing)\nx <- seq(from=0, to=100, by=5)\ny_increasing <- 2*x + rnorm(length(x), mean=100, sd=25)\nplot(x, y_increasing, pch=19, xlab=\"x\", ylab=\"y\", main=\"b) Statistical\", cex.main=0.8, las=1)\n\n# Statistical relationship (decreasing)\ny_decreasing <- -2*x + rnorm(length(x), mean=100, sd=25)\nplot(x, y_decreasing, pch=19, xlab=\"x\", ylab=\"y\", main=\"c) Statistical\", cex.main=0.8, las=1)\n\n# Statistical relationship (random)\ny_random <- - rnorm(length(x), mean=100, sd=25)\nplot(x, y_random, pch=19, xlab=\"x\", ylab=\"y\", main=\"d) Random\", cex.main=0.8, las=1)\n\n```\n\n## What linear models are and are not\n\n- In an linear model we model (explain) the relationship between a single continuous variable $Y$ and one or more variables $X$. The $X$ variables can be numerical, categorical or a mixture of both.\n- One very general form for the model would be: \n$$Y = f(X_1, X_2, \\dots X_p) + \\epsilon$$ where $f$ is some unknown function and $\\epsilon$ is the error in this representation.\n\n- For instance a **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \\beta \\cdot x + \\epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled.\n- Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared\n- Formally, linear models are a way of describing a response variable in terms of **linear combination** of predictor variables, i.e. expression constructed from a a set of terms by multiplying each term by a constant and/or adding the results. \n- For instance these are all models that can be constructed using linear combinations of predictors:\n\n```{r}\n#| label: fig-linear-adv\n#| fig-cap: \"Examples of a linear models: A) $y_i = x_1 + e_i$, B) $x_1 + I_{x_i} + e_i$ C) $y_i = x_i^2 + e_i$, D) $y_i = x + x_i^3 + e_i$ showing that linear models can get more complex and/or capture more than a straight line relationship.\"\n#| fig-cap-location: margin\n#| warning: false\n#| echo: false\n#| fig-height: 7\n\nmy.ggtheme <- theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \"none\", \n        axis.title.y=element_text(angle=0))\n\n# simple linear regression\nx <- seq(-10, 10, 1)\ny <- x + rnorm(length(x), mean(x), 2)\ndata.xy <- data.frame(x=x, y = y, ymodel = x)\np.simple <-  data.xy %>% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel), color = col.blue.dark) + \n  my.ggtheme + \n  ggtitle(\"A\")\n\n# simple linear regression with group\nx <- seq(0, 10, length.out = 20)\ny1 <- 0 + x + rnorm(length(x), 0, 2)\ny2 <- 0 + 4*x + rnorm(length(x), 0, 2)\n\nx.all <- c(x, x)\ny.all <- c(y1, y2)\ngroup <- c(rep(\"CTRL\", length(x)), rep(\"TX\", length(x)))\nymodel <- c(0+x, 0+4*x)\n\ndata.xy <- data.frame(x=x.all, y = y.all, ymodel = ymodel)\n\np.group <- data.xy %>% ggplot(aes(x = x, y = y, colour = group)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel)) + \n  theme_classic() +\n  scale_color_brewer(palette = \"Set2\") + \n  my.ggtheme + \n  ggtitle(\"B\")\n\n# advanced 1\nx <- seq(-10, 10, 1)\ny <- x^2 + rnorm(length(x), mean(x), 10)\ndata.xy <- data.frame(x=x, y = y, ymodel = x^2)\np.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel), color = col.blue.dark) + \n  theme_classic() +\n  my.ggtheme + \n  ggtitle(\"C\")\n\n\n# advanced 2\nx <- seq(-10, 10, 1)\ny <- (x + (x^3))/1000 + rnorm(length(x), mean(x), 0.05)\ndata.xy <- data.frame(x=x, x2=x^2, x3=x^3, y = y, ymodel = (x + x^3)/1000)\np.adv2 <- data.xy %>% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel), color = col.blue.dark) + \n  theme_classic() +\n  my.ggtheme + \n  ggtitle(\"D\")\n\ngrid.arrange(p.simple, p.group, p.adv1, p.adv2, ncol = 2)\n\n```\n\n\n- $Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i$\n- $Y_i = \\alpha + \\beta x_i^2 \\epsilon$\n- $Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon$\n- $Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon$\n\nvs. an example of a non-linear model where parameter $\\beta$ appears in the exponent of $x_i$\n\n- $Y_i = \\alpha + x_i^\\beta +  \\epsilon$\n\n\n## Terminology\nThere are many terms and notations used interchangeably:\n\n- $y$ is being called:\n  - response\n  - outcome\n  - dependent variable\n\n- $x$ is being called:\n  - exposure\n  - explanatory variable\n  - dependent variable\n  - predictor\n  - covariate\n\n\n\n## Simple linear regression\n- It is used to check the association between **the numerical outcome and one numerical explanatory variable**\n- In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n\n\n:::{#exm-simple-lm}\n## Weight and plasma volume\n\nLet's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\n\nExample data:\n```{r}\n#| code-fold: false\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n```\n:::\n\n```{r}\n#| label: fig-lm-intro-example\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*.\"\n#| fig-cap-location: margin\n#| echo: false\n#| fig-width: 4\n#| fig-heigth: 4\n\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\n\n```\n\n```{r}\n#| label: fig-lm-example-reg\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable\"\n#| fig-cap-location: margin\n#| echo: false\n#| fig-width: 4\n#| fig-heigth: 4\n\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\", panel.first = grid())\n\nreg1 <- lm(plasma ~ weight)\na <- reg1$coefficients[1]\nb <- reg1$coefficients[2]\n\nabline(a=a+0.1 , b + 0.001, col=\"gray\")\nabline(a=a+0.1 , b + 0.0001, col=\"gray\")\n#abline(a=a , b + 0.00015, col=\"gray\")\nabline(a=a+0.1 , b + 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.001, col=\"gray\")\nabline(a=a, b - 0.001, col=\"gray\")\nabline(a=a+0.5 , b , col=\"gray\")\nabline(a=a-0.5 , b , col=\"gray\")\n\nabline(lm(plasma~weight), col=\"red\") # regression line\npoints(weight, plasma, pch=19)\n\n\n```\n\nThe equation for the red line is:\n$$Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8$$\nand in general:\n$$Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n$$\n\n- In other words, by finding the best-fitting straight line we are **building a statistical model** to represent the relationship between plasma volume ($Y$) and explanatory body weight variable ($x$)\n- If we were to use our model $Y_i=0.086 + 0.044 \\cdot x_i$ to find plasma volume given a weight of 58 kg (our first observation, $i=1$), we would notice that we would get $Y=0.086 +  0.044 \\cdot 58 = 2.638$, not exactly $2.75$ as we have for our first man in our dataset that we started with, i.e. $2.75 - 2.638 = 0.112 \\neq 0$.\n- We thus add to the above equation an **error term** to account for this and now we can write our **simple regression model** more formally as:\n\n$$Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$$ {#eq-lm}\nwhere:\n\n- $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n- $y$: is called: response, outcome, dependent variable\n- $\\alpha$ and $\\beta$ are **model coefficients**\n- and $\\epsilon_i$ is an **error terms**\n\n\n## Least squares\n- in the above **\"body weight - plasma volume\"** example, the values of $\\alpha$ and $\\beta$ have just appeared\n- in practice, $\\alpha$ and $\\beta$ values are unknown and we use data to **estimate these coefficients**, noting the estimates with a **hat**, $\\hat{\\alpha}$ and $\\hat{\\beta}$\n- **least squares** is one of the methods of parameters estimation, i.e. finding $\\hat{\\alpha}$ and $\\hat{\\beta}$\n\n```{r}\n#| label: fig-reg-errors\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line\"\n#| fig-cap-location: margin\n#| echo: false\n#| warning: false\n#| message: false\n#| fig-width: 4\n#| fig-height: 3\n\ndata.reg <- data.frame(plasma=plasma, weight=weight)\nfit.reg <- lm(plasma~weight, data=data.reg)\ndata.reg$predicted <- predict(fit.reg)\ndata.reg$residuals <- residuals((fit.reg))\n\nggplot(data.reg, aes(x=weight, plasma)) + geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  geom_segment(aes(xend = weight, yend = predicted), color=\"blue\") +\n  geom_point(aes(y = predicted), shape = 1) +\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_bw() + xlab(\"body weight [kg]\") + ylab(\"plasma volume [liters]\")\n\n```\n\n\n<br>\n\nLet $\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:\n\n- Then $\\epsilon_i = y_i - \\hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model\n- RSS, the **residual sum of squares** is defined as: $$RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2$$ or\nequivalently as: $$RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2$$\n- the least squares approach chooses $\\hat{\\alpha}$ and $\\hat{\\beta}$ **to minimize the RSS**. With some calculus, a good video explanation for the interested ones is [here](https://www.youtube.com/watch?v=ewnc1cXJmGA), we get @thm-lss\n\n::: {#thm-lss}\n## Least squares estimates for a simple linear regression\n\n$$\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}$$\n$$\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}$$\n\nwhere:\n\n- $\\bar{x}$: mean value of $x$\n- $\\bar{y}$: mean value of $y$\n- $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2$\n- $S_{yy}$: sum of squares of $Y$ defined as  $S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2$\n- $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$\n\n:::\n\n\n<!-- We can further re-write the above sum of squares to obtain -->\n\n<!-- - sum of squares of $X$, $$S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})$$ -->\n<!-- - sum of products of $X$ and $Y$ -->\n\n<!-- $$S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}$$ -->\n\n\n::: {#exm-lss}\n\n## Least squares\n\nLet's try least squares method to find coefficient estimates in the **\"body weight and plasma volume example\"**\n\n```{r}\n#| code-fold: false\n\n# initial data\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# rename variables for convenience\nx <- weight\ny <- plasma\n\n# mean values of x and y\nx.bar <- mean(x)\ny.bar <- mean(y)\n\n# Sum of squares\nSxx <-  sum((x - x.bar)^2)\nSxy <- sum((x-x.bar)*(y-y.bar))\n\n# Coefficient estimates\nbeta.hat <- Sxy / Sxx\nalpha.hat <- y.bar - Sxy/Sxx*x.bar\n\n# Print estimated coefficients alpha and beta\nprint(alpha.hat)\nprint(beta.hat)\n\n```\n\n:::\n\n\nIn R we can use `lm()`, the built-in function, to fit a linear regression model and we can replace the above code with one line\n\n```{r}\n#| code-fold: false\nlm(plasma ~ weight)\n```\n\n## Intercept and Slope\n- Linear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n- $\\alpha$ is known as the **intercept**\n- $\\beta$ is known as the **slope**\n\n```{r}\n#| label: fig-lm-parameters\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)\"\n#| echo: false\n#| fig-height: 7\n#| fig-width: 7\n\nmodel <- lm(plasma ~ weight)\nalpha <- model$coefficients[1]\nbeta <- model$coefficients[2]\n\npar(mfcol=c(2,2), mar=c(4,4,3,2))\n\n# Values from regression model: plasma_volume = 0.0857 + 0.043615*x\n\n# Fitted line\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\nabline(lm(plasma~weight), col=\"red\") # regression line\ntext(65, 3.3, \"plasma = 0.0857 + 0.0436 * weight\", cex=1)\n\n# Beta 1 example b\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid(), xlim=c(60, 70), ylim=c(2.8, 3.2))\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h = alpha + beta * 65, col = \"blue\",  lty = 3)\nabline(h = alpha + beta * 66, col = \"blue\",  lty = 3)\nsegments(x0=65, y0=alpha + beta * 65, x1=66, y1=alpha + beta * 65, col=\"blue\")\nsegments(x0=66, y0=alpha + beta * 65, x1=66, y1=alpha + beta * 66, col=\"blue\")\ntext(67, 2.94, expression(beta), cex=1.2, col=\"blue\")\ntext(61, alpha + beta * 65 - 0.02, round(alpha + beta * 65, 2), cex=1.2, col=\"blue\")\ntext(61, alpha + beta * 66 + 0.02, round(alpha + beta * 66, 2), cex=1.2, col=\"blue\")\n\n\n# Beta 1 example a\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h = alpha + beta * 65, col = \"blue\",  lty = 3)\nabline(h = alpha + beta * 70, col = \"blue\",  lty = 3)\nsegments(x0=65, y0=alpha + beta * 65, x1=70, y1=alpha + beta * 65, col=\"blue\")\nsegments(x0=70, y0=alpha + beta * 65, x1=70, y1=alpha + beta * 70, col=\"blue\")\ntext(72, 2.95, expression(beta), cex=1.2, col=\"blue\")\ntext(60, alpha + beta * 65 + 0.05, round(alpha + beta * 65, 2), cex=1.2, col=\"blue\")\ntext(60, alpha + beta * 70 + 0.05, round(alpha + beta * 70, 2), cex=1.2, col=\"blue\")\n\n\n# Beta 0 example a\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid(), xlim=c(-20, 80), ylim=c(0, 5))\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h=alpha, col=\"blue\", lty = 3) # regression line\nsegments(x0=65, y0=2.92, x1=66, y1=2.92, col=\"blue\")\nsegments(x0=66, y0=2.92, x1=66, y1=2.964, col=\"blue\")\ntext(15, 0.4, expression(alpha), cex=1.2, col=\"blue\")\ntext(33, 0.5, paste(\"= \", round(alpha,3), sep=\"\"), cex=1.2, col=\"blue\")\n\n```\n\n## Hypothesis testing\n\n**Is there a relationship between the response and the predictor?**\n\n- the calculated $\\hat{\\alpha}$ and $\\hat{\\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**\n- their precision is measured by their **estimated standard errors**, `e.s.e`($\\hat{\\alpha}$) and `e.s.e`($\\hat{\\beta}$)\n- these estimated standard errors are used in **hypothesis testing** and in constructing **confidence and prediction intervals**\n\n**The most common hypothesis test** involves testing the ``null hypothesis`` of:\n\n- $H_0:$ There is no relationship between $X$ and $Y$\n- versus the ``alternative hypothesis`` $H_a:$ there is some relationship between $X$ and $Y$\n\n**Mathematically**, this corresponds to testing:\n\n- $H_0: \\beta=0$\n- versus $H_a: \\beta\\neq0$\n- since if $\\beta=0$ then the model $Y_i=\\alpha+\\beta x_i + \\epsilon_i$ reduces to $Y=\\alpha + \\epsilon_i$\n\n**Under the null hypothesis** $H_0: \\beta = 0$\n<!-- we have: $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)$$  -->\n![](figures/linear-models/lm-tstatistics.png)\n\n- $n$ is number of observations\n- $p$ is number of model parameters\n- $\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}$ is the ratio of the departure of the estimated value of a parameter, $\\hat\\beta$, from its hypothesized value, $\\beta$, to its standard error, called `t-statistics`\n- the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom\n\n::: {#exm-hypothesis-testing}\n\n## Hypothesis testing\n\nLet's look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the `R summary` of the model\n\n```{r}\n#| code-fold: false\n\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n\n```\n\n:::\n\n- Under `Estimate` we see estimates of our model coefficients, $\\hat{\\alpha}$ (intercept) and $\\hat{\\beta}$ (slope, here weight), followed by their estimated standard errors, `Std. Errors`\n- If we were to test if there is an **association between weight and plasma volume** we would write under the null hypothesis $H_0: \\beta = 0$ $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582$$\n- and we would **compare** `t-statistics` to `Student's t distribution` with $n-p = 8 - 2 = 6$ degrees of freedom (as we have 8 observations and two model parameters, $\\alpha$ and $\\beta$)\n- we can use **Student's t distribution table** or **R code** to obtain the associated *P*-value\n\n```{r}\n#| code-fold: false\n2*pt(2.856582, df=6, lower=F)\n```\n\n- here the observed t-statistics is large and therefore yields a small *P*-value, meaning that **there is sufficient evidence to reject null hypothesis in favor of the alternative** and conclude that there is a significant association between weight and plasma volume\n\n\n## Vector-matrix notations\n\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of **multiple regression**, with more than one explanatory variable in the model, it is **more efficient to use vectors and matrices to define the regression model**.\n\nLet's **rewrite** our simple linear regression model $Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n$ **into vector-matrix notation** in **6 steps**.\n\n1. First we rename our $\\alpha$ to $\\beta_0$ and $\\beta$ to $\\beta_1$ as it is easier to keep tracking the number of model parameters this way\n\n2. Then we notice that we actually have $n$ equations such as:\n$$y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1$$\n$$y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2$$\n$$y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3$$\n$$\\dots$$\n$$y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n$$\n\n3. We group all $Y_i$ and $\\epsilon_i$ into column vectors:\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$ and\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n\n4. We stack two parameters $\\beta_0$ and $\\beta_1$ into another column vector:$$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$$\n\n5. We append a vector of ones with the single predictor for each $i$ and create a matrix with two columns called **design matrix** $$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}$$\n\n6. We write our linear model in a vector-matrix notations as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n::: {#def-vector-matrix-lm}\n\n## vector matrix form of the linear model\n\nThe vector-matrix representation of a linear model with $p-1$ predictors can be written as\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nwhere:\n\n- $\\mathbf{Y}$ is $n \\times1$ vector of observations\n- $\\mathbf{X}$ is $n \\times p$ **design matrix**\n- $\\boldsymbol\\beta$ is $p \\times1$ vector of parameters\n- $\\boldsymbol\\epsilon$ is $n \\times1$ vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, $\\sigma^2$)\n\nIn full, the above vectors and matrix have the form:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1    \\\\\n  \\vdots \\\\\n  \\beta_{p}\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_{1,1} & \\dots & x_{1,p-1} \\\\\n  1 & x_{2,1} & \\dots & x_{2,p-1} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & x_{n,1} & \\dots & x_{n,p-1}\n\\end{bmatrix}$\n\n:::\n\n\n::: {#thm-lss-vector-matrix}\n\n## Least squares in vector-matrix notation\n\nThe least squares estimates for a linear regression of the form:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nis given by:\n$$\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n\n:::\n\n\n::: {#exm-vector-matrix-notation}\n\n## vector-matrix-notation\n\nFollowing the above definition we can write the **\"weight - plasma volume model\"** as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\nwhere:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n 2.75  \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12\n\\end{bmatrix}$\n\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{8}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & 58.0 \\\\\n  1 & 70.0 \\\\\n  1 & 74.0 \\\\\n  1 & 63.5 \\\\\n  1 & 62.0 \\\\\n  1 & 70.5 \\\\\n  1 & 71.0 \\\\\n  1 & 66.0 \\\\\n\\end{bmatrix}$\n\nand we can estimate model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n\nWe can do it by hand or in `R` as follows:\n\n```{r}\n#| code-fold: false\n\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\nprint(X)\n\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n\n```\n\n:::\n\n## Confidence intervals and prediction intervals\n\n- when we estimate coefficients we can also find their **confidence intervals**, typically 95\\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter\n- we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval\n\n::: {#exm-prediction-and-intervals}\n\n## Prediction and intervals\n\nLet's:\n\n- find confidence intervals for our coefficient estimates\n- predict plasma volume for a men weighting 60 kg\n- find prediction interval\n- plot original data, fitted regression model, predicted observation and prediction interval\n\n```{r}\n#| code-fold: false\n\n# fit regression model\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n\n# find confidence intervals for the model coefficients\nconfint(model)\n\n# predict plasma volume for a new observation of 60 kg\n# we have to create data frame with a variable name matching the one used to build the model\nnew.obs <- data.frame(weight = 60)\npredict(model, newdata = new.obs)\n\n# find prediction intervals\nprediction.interval <- predict(model, newdata = new.obs,  interval = \"prediction\")\nprint(prediction.interval)\n\n# plot the original data, fitted regression and predicted value\nplot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\", ylim=c(2,4))\nlines(weight, model$fitted.values, col=\"red\") # fitted model in red\npoints(new.obs, predict(model, newdata = new.obs), pch=19, col=\"blue\") # predicted value at 60kg\nsegments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval\n\n```\n\n:::\n\n## Assessing model fit\n\n- Earlier we learned how to estimate parameters in a liner model using least squares estimation.\n- Now we will consider how to assess the goodness of fit of a model, i.e. how well does the model explain our data. \n- We do that by calculating the amount of variability in the response that is explained by the model.\n\n### $R^2$: summary of the fitted model {-}\n- considering a simple linear regression, the simplest model, **Model 0**, we could consider fitting is $$Y_i = \\beta_0+ \\epsilon_i$$ that corresponds to a line that run through the data but lies parallel to the horizontal axis\n- in our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple)\n\n```{r}\n#| collapse: true\n#| echo: false\n#| fig-width: 6\n#| fig-height: 5\n#| fig-align: center\n\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nplot(weight, plasma, pch=19, xlab=\"Weight [kg]\", ylab=\"Plasma volume [l]\")\nabline(h=mean(plasma), col=\"purple\")\n\n```\n\n- TSS, denoted **Total corrected sum-of-squares** is the residual sum-of-squares for Model 0\n$$S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}$$ corresponding the to the sum of squared distances to the purple line\n\n```{r}\n#| echo: false\n#| fig-width: 6\n#| fig-height: 5\n#| fig-align: center\n\nplot(weight, plasma, pch=19, xlab=\"Weight [kg]\", ylab=\"Plasma volume [l]\")\nabline(h=mean(plasma), col=\"purple\")\n\nfor (i in 1:length(weight)){\n  segments(weight[i], plasma[i], weight[i], mean(plasma))\n}\n\n```\n\n- Fitting **Model 1** of the form $$Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$$ we have earlier defined\n- **RSS**, the residual sum-of-squares as:\n$$RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n- that corresponds to the squared distances between the observed values $y_i, \\dots,y_n$ to fitted values $\\hat{y_1}, \\dots \\hat{y_n}$, i.e. distances to the red fitted line\n\n```{r, echo=F, fig.align=\"center\", fig.width=5, fig.height=4}\n#| echo: false\n#| fig-width: 6\n#| fig-height: 5\n#| fig-align: center\n\nplot(weight, plasma, pch=19, xlab=\"Weight [kg]\", ylab=\"Plasma volume [l]\")\nabline(lm(plasma ~ weight), col=\"red\")\nmodel <- lm(plasma ~ weight)\n\nfor (i in 1:length(weight)){\n  segments(weight[i], plasma[i], weight[i], model$fitted.values[i])\n}\n\n```\n\n::: {#def-r2}\n\n## $R^2$ {-}\n\nA simple but useful measure of model fit is given by $$R^2 = 1 - \\frac{RSS}{TSS}$$ where:\n\n- RSS is the residual sum-of-squares for Model 1, the fitted model of interest\n- TSS is the sum of squares of the **null model**\n\n:::\n\n- $R^2$ quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model\n- $R^2$ is also referred as **coefficient of determination**\n- It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data\n- Values of $R^2$ approaching 1 indicate the model to be a good fit\n- Values of $R^2$ less than 0.5 suggest that the model gives rather a poor fit to the data\n\n### $R^2$ and correlation coefficient {-}\n\n::: {#thm-r2}\n## $R^2$\n\nIn the case of simple linear regression:\n\nModel 1: $Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$\n$$R^2 = r^2$$\nwhere:\n\n- $R^2$ is the coefficient of determination\n- $r^2$ is the sample correlation coefficient\n\n:::\n\n\n### $R^2(adj)$ {-}\n- in the case of multiple linear regression, where there is more than one explanatory variable in the model\n- we are using the adjusted version of $R^2$ to assess the model fit\n- as the number of explanatory variables increase, $R^2$ also increases\n- $R^2(adj)$ takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model\n\n\n::: {#thm-r2adj}\n\n## $R^2(adj)$\n\nFor any multiple linear regression\n$$Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i$$ $R^2(adj)$ is defined as\n$$R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}$$ where\n\n- $p$ is the number of independent predictors, i.e. the number of variables in the model, excluding the constant\n\n$R^2(adj)$ can also be calculated from $R^2$:\n$$R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}$$\n\n:::\n\n<!-- We can calculate the values in R and compare the results to the output of linear regression -->\n\n```{r}\n#| collapse: true\n#| code-fold: false\n#| include: false\n#| eval: false\n\nhtwtgen <- read.csv(\"data/lm/heights_weights_genders.csv\")\nhead(htwtgen)\nattach(htwtgen)\n\n## Simple linear regression\nmodel.simple <- lm(Height ~ Weight, data=htwtgen)\n\n# TSS\nTSS <- sum((Height - mean(Height))^2)\n\n# RSS\n# residuals are returned in the model type names(model.simple)\nRSS <- sum((model.simple$residuals)^2)\nR2 <- 1 - (RSS/TSS)\n\nprint(R2)\nprint(summary(model.simple))\n\n## Multiple regression\nmodel.multiple <- lm(Height ~ Weight + Gender, data=htwtgen)\nn <- length(Weight)\np <- 1\n\nRSS <- sum((model.multiple$residuals)^2)\nR2_adj <- 1 - (RSS/(n-p-1))/(TSS/(n-1))\n\nprint(R2_adj)\nprint(summary(model.multiple))\n\n```\n\n## The assumptions of a linear model\n\n```{r}\n#| message: false\n#| warning: false\n#| echo: false\n\n# load libraries\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(ggplot2)\n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2m) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %>%\n  na.omit()\n\n# reset rownames\nrownames(data_diabetes) <- NULL\n\n```\n\nUp until now we were fitting models and discussed how to assess the model fit. Before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data. Informally we have been using box plots and scatter plots to look at the data. There are however formal definitions of the assumptions.\n\n**Assumption A: The deterministic part of the model captures all the non-random structure in the data**\n\n- This implies that the **mean of the errors $\\epsilon_i$** is zero.\n- Tt applies only over the range of explanatory variables.\n\n**Assumption B: the scale of variability of the errors is constant at all values of the explanatory variables**\n\n- Practically we are looking at whether the observations are equally spread on both side of the regression line.\n\n**Assumption C: the errors are independent**\n\n- Broadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another.\n\n**Assumptions D: the errors are normally distributed**\n\n- This will allow us to describe the variation in the model's parameters estimates and therefore make inferences about the population from which our sample was taken.\n\n**Assumption E: the values of the explanatory variables are recorded without error**\n\n- This one is not possible to check via examining the data, instead we have to consider the nature of the experiment.\n\n\n### Checking assumptions\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**. We use plots such as:\n\n1. Histograms or normal probability plots of $\\hat{\\epsilon_i}$\n- useful to check the assumption of normality\n\n2. Plots of $\\hat{\\epsilon_i}$ versus the fitted values $\\hat{y_i}$\n- used to detect changes in error variance\n- used to check if the mean of the errors is zero\n\n3. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{ij}$\n- this helps to check that the variable $x_j$ has a linear relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{kj}$ that is **not** in the model\n- this helps to check whether the additional variable $x_k$ might have a relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ in the order of the observations were collected\n- this is useful to check whether errors might be correlated over time\n\nLet's fit a simple model to predict `BMI` given `waist` for the diabetes study and see if the model meets the assumptions of linear models.\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-cap: \"Default diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Simple regression to model BMI with waist explanatory variable.\"\n#| fig-cap-location: margin\n#| fig-height: 8\n\n# fit simple linear regression model\nmodel <- lm(BMI ~ waist, data = data_diabetes)\n\n# plot diagnostic plots of the linear model\n# by default plot(model) calls four diagnostics plots\n# par() divides plot window in 2 x 2 grid\npar(mfrow=c(2,2))\nplot(model)\n```\n\n\n- The residual plots provides examples of a situation where the assumptions appear to be met.\n- The linear regression appears to describe data quite well.\n- There is no obvious trend of any kind in the residuals vs. fitted values (the shape is scattered) with potential few outliers that we may want to decided to exclude later.\n- Points lie reasonably well along the line in the normal probability plot, hence normality appears to be met.\n\n**Examples of assumptions not being met**\n\n```{r}\n#| fig-cap: Example of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case\n#| fig-align: center\n#| fig-cap-location: margin\n#| echo: false\n\nknitr::include_graphics(\"figures/linear-models/lm-assumptions-01.png\", dpi = 100)\n\n```\n\n```{r}\n#| fig-cap: Example of non-constant variance\n#| fig-align: center\n#| fig-cap-location: margin\n#| echo: false\nknitr::include_graphics(\"figures/linear-models/lm-assumptions-02.png\", dpi = 80)\n\n```\n\n```{r}\n#| fig-cap: Example of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected\n#| fig-align: center\n#| fig-cap-location: margin\n#| echo: false\nknitr::include_graphics(\"figures/linear-models/lm-assumptions-03.png\", dpi = 100)\n\n```\n\n\n### Influential observations\n- Sometimes individual observations can exert a great deal of influence on the fitted model.\n- One routine way of checking for this is to fit the model $n$ times, missing out each observation in turn.\n- If we removed i-th observation and compared the fitted value from the full model, say $\\hat{y_j}$ to those obtained by removing this point, denoted $\\hat{y_{j(i)}}$ then\n- observations with a high Cook's distance (measuring the effect of deleting a given observation) could be influential.\n\nLet's remove some observation with higher Cook's distance from protein data set, re-fit our model and compare the diagnostics plots\n\n```{r}\n#| code-fold: false\n#| fig-width: 8\n#| fig-height: 8\n\n# observations to be removed (based on Residuals vs. Leverage plot)\nobs <- c(13, 78, 83, 84)\n\n# fit models removing observations\ndata_diabetes_flr <- data_diabetes[-obs, ]\n\nmodel_flr <- lm(BMI ~ waist, data = data_diabetes_flr)\n\n# plot diagnostics plot\npar(mfrow=c(2,2))\nplot(model_flr)\n\n```\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n# Introduction to linear models\n\n```{r}\n#| message: false\n#| warning: false\n#| echo: false\n\n# load libraries\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(ggplot2)\n\nfont.size <- 12\ncol.blue.light <- \"#a6cee3\"\ncol.blue.dark <- \"#1f78b4\"\nmy.ggtheme <- theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \"top\") + \n        theme_bw()\n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2m) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %>%\n  na.omit()\n\n```\n\n## Why linear models? \n\nWith linear models we can answer questions such as: \n\n  - is there a relationship between exposure and outcome, e.g. height and weight?\n  - how strong is the relationship between the two variables?\n  - what will be a predicted value of the outcome given a new set of exposure values?\n  - how accurately can we predict outcome?\n  - which variables are associated with the response, e.g. is it only height that explains weight or could it be height and age that are both associated with the response?\n  \n```{r}\n#| label: fig-scatter\n#| message: false\n#| warning: false\n#| fig-align: center\n#| fig-cap: \"Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA.\"\n#| fig-cap-location: bottom\n\ndata_diabetes %>%\n  ggplot(aes(x = height, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  my.ggtheme + \n  xlab(\"height [m]\") + \n  ylab(\"weight [kg]\")\n\n```\n\n\n## Statistical vs. deterministic relationship\n\nRelationships in probability and statistics can generally be one of three things: deterministic, random, or statistical:\n\n- a **deterministic** relationship involves **an exact relationship** between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation $Fahrenheit=\\frac{9}{5}\\cdot Celcius+32$\n- there is **no relationship** between variables in the **random relationship**, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year\n- **a statistical relationship** is a **mixture of deterministic and random relationship**, e.g. the savings that Olga has left in the bank account depend on Olga's monthly salary income (deterministic part) and the money spent on buying succulents (random part)\n\n\n```{r}\n#| label: fig-relationship\n#| fig-cap: \"Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between $x$ and $y$ is not perfect (increasing relationship), c)  statistical relationship between $x$ and $y$ is not perfect (decreasing relationship), d) random signal\"\n#| echo: false\n#| fig-height: 8\n\n# prepare plot space\npar(mfrow=c(2,2))\n\n# Deterministic relationship example\nx_celcius <- seq(from=0, to=50, by=5)\ny_fahr <- 9/5*x_celcius+32\nplot(x_celcius, y_fahr, type=\"b\", pch=19, xlab=\"Celcius\", ylab=\"Fahrenheit\", main=\"a) Deterministic\", cex.main=0.8, las=2)\n\n# Statistical relationship (increasing)\nx <- seq(from=0, to=100, by=5)\ny_increasing <- 2*x + rnorm(length(x), mean=100, sd=25)\nplot(x, y_increasing, pch=19, xlab=\"x\", ylab=\"y\", main=\"b) Statistical\", cex.main=0.8, las=1)\n\n# Statistical relationship (decreasing)\ny_decreasing <- -2*x + rnorm(length(x), mean=100, sd=25)\nplot(x, y_decreasing, pch=19, xlab=\"x\", ylab=\"y\", main=\"c) Statistical\", cex.main=0.8, las=1)\n\n# Statistical relationship (random)\ny_random <- - rnorm(length(x), mean=100, sd=25)\nplot(x, y_random, pch=19, xlab=\"x\", ylab=\"y\", main=\"d) Random\", cex.main=0.8, las=1)\n\n```\n\n## What linear models are and are not\n\n- In an linear model we model (explain) the relationship between a single continuous variable $Y$ and one or more variables $X$. The $X$ variables can be numerical, categorical or a mixture of both.\n- One very general form for the model would be: \n$$Y = f(X_1, X_2, \\dots X_p) + \\epsilon$$ where $f$ is some unknown function and $\\epsilon$ is the error in this representation.\n\n- For instance a **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \\beta \\cdot x + \\epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled.\n- Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared\n- Formally, linear models are a way of describing a response variable in terms of **linear combination** of predictor variables, i.e. expression constructed from a a set of terms by multiplying each term by a constant and/or adding the results. \n- For instance these are all models that can be constructed using linear combinations of predictors:\n\n```{r}\n#| label: fig-linear-adv\n#| fig-cap: \"Examples of a linear models: A) $y_i = x_1 + e_i$, B) $x_1 + I_{x_i} + e_i$ C) $y_i = x_i^2 + e_i$, D) $y_i = x + x_i^3 + e_i$ showing that linear models can get more complex and/or capture more than a straight line relationship.\"\n#| fig-cap-location: margin\n#| warning: false\n#| echo: false\n#| fig-height: 7\n\nmy.ggtheme <- theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \"none\", \n        axis.title.y=element_text(angle=0))\n\n# simple linear regression\nx <- seq(-10, 10, 1)\ny <- x + rnorm(length(x), mean(x), 2)\ndata.xy <- data.frame(x=x, y = y, ymodel = x)\np.simple <-  data.xy %>% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel), color = col.blue.dark) + \n  my.ggtheme + \n  ggtitle(\"A\")\n\n# simple linear regression with group\nx <- seq(0, 10, length.out = 20)\ny1 <- 0 + x + rnorm(length(x), 0, 2)\ny2 <- 0 + 4*x + rnorm(length(x), 0, 2)\n\nx.all <- c(x, x)\ny.all <- c(y1, y2)\ngroup <- c(rep(\"CTRL\", length(x)), rep(\"TX\", length(x)))\nymodel <- c(0+x, 0+4*x)\n\ndata.xy <- data.frame(x=x.all, y = y.all, ymodel = ymodel)\n\np.group <- data.xy %>% ggplot(aes(x = x, y = y, colour = group)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel)) + \n  theme_classic() +\n  scale_color_brewer(palette = \"Set2\") + \n  my.ggtheme + \n  ggtitle(\"B\")\n\n# advanced 1\nx <- seq(-10, 10, 1)\ny <- x^2 + rnorm(length(x), mean(x), 10)\ndata.xy <- data.frame(x=x, y = y, ymodel = x^2)\np.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel), color = col.blue.dark) + \n  theme_classic() +\n  my.ggtheme + \n  ggtitle(\"C\")\n\n\n# advanced 2\nx <- seq(-10, 10, 1)\ny <- (x + (x^3))/1000 + rnorm(length(x), mean(x), 0.05)\ndata.xy <- data.frame(x=x, x2=x^2, x3=x^3, y = y, ymodel = (x + x^3)/1000)\np.adv2 <- data.xy %>% ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y=ymodel), color = col.blue.dark) + \n  theme_classic() +\n  my.ggtheme + \n  ggtitle(\"D\")\n\ngrid.arrange(p.simple, p.group, p.adv1, p.adv2, ncol = 2)\n\n```\n\n\n- $Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i$\n- $Y_i = \\alpha + \\beta x_i^2 \\epsilon$\n- $Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon$\n- $Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon$\n\nvs. an example of a non-linear model where parameter $\\beta$ appears in the exponent of $x_i$\n\n- $Y_i = \\alpha + x_i^\\beta +  \\epsilon$\n\n\n## Terminology\nThere are many terms and notations used interchangeably:\n\n- $y$ is being called:\n  - response\n  - outcome\n  - dependent variable\n\n- $x$ is being called:\n  - exposure\n  - explanatory variable\n  - dependent variable\n  - predictor\n  - covariate\n\n\n\n## Simple linear regression\n- It is used to check the association between **the numerical outcome and one numerical explanatory variable**\n- In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n\n\n:::{#exm-simple-lm}\n## Weight and plasma volume\n\nLet's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\n\nExample data:\n```{r}\n#| code-fold: false\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n```\n:::\n\n```{r}\n#| label: fig-lm-intro-example\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*.\"\n#| fig-cap-location: margin\n#| echo: false\n#| fig-width: 4\n#| fig-heigth: 4\n\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\n\n```\n\n```{r}\n#| label: fig-lm-example-reg\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable\"\n#| fig-cap-location: margin\n#| echo: false\n#| fig-width: 4\n#| fig-heigth: 4\n\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\", panel.first = grid())\n\nreg1 <- lm(plasma ~ weight)\na <- reg1$coefficients[1]\nb <- reg1$coefficients[2]\n\nabline(a=a+0.1 , b + 0.001, col=\"gray\")\nabline(a=a+0.1 , b + 0.0001, col=\"gray\")\n#abline(a=a , b + 0.00015, col=\"gray\")\nabline(a=a+0.1 , b + 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.002, col=\"gray\")\nabline(a=a+0.1 , b - 0.001, col=\"gray\")\nabline(a=a, b - 0.001, col=\"gray\")\nabline(a=a+0.5 , b , col=\"gray\")\nabline(a=a-0.5 , b , col=\"gray\")\n\nabline(lm(plasma~weight), col=\"red\") # regression line\npoints(weight, plasma, pch=19)\n\n\n```\n\nThe equation for the red line is:\n$$Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8$$\nand in general:\n$$Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n$$\n\n- In other words, by finding the best-fitting straight line we are **building a statistical model** to represent the relationship between plasma volume ($Y$) and explanatory body weight variable ($x$)\n- If we were to use our model $Y_i=0.086 + 0.044 \\cdot x_i$ to find plasma volume given a weight of 58 kg (our first observation, $i=1$), we would notice that we would get $Y=0.086 +  0.044 \\cdot 58 = 2.638$, not exactly $2.75$ as we have for our first man in our dataset that we started with, i.e. $2.75 - 2.638 = 0.112 \\neq 0$.\n- We thus add to the above equation an **error term** to account for this and now we can write our **simple regression model** more formally as:\n\n$$Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$$ {#eq-lm}\nwhere:\n\n- $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n- $y$: is called: response, outcome, dependent variable\n- $\\alpha$ and $\\beta$ are **model coefficients**\n- and $\\epsilon_i$ is an **error terms**\n\n\n## Least squares\n- in the above **\"body weight - plasma volume\"** example, the values of $\\alpha$ and $\\beta$ have just appeared\n- in practice, $\\alpha$ and $\\beta$ values are unknown and we use data to **estimate these coefficients**, noting the estimates with a **hat**, $\\hat{\\alpha}$ and $\\hat{\\beta}$\n- **least squares** is one of the methods of parameters estimation, i.e. finding $\\hat{\\alpha}$ and $\\hat{\\beta}$\n\n```{r}\n#| label: fig-reg-errors\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line\"\n#| fig-cap-location: margin\n#| echo: false\n#| warning: false\n#| message: false\n#| fig-width: 4\n#| fig-height: 3\n\ndata.reg <- data.frame(plasma=plasma, weight=weight)\nfit.reg <- lm(plasma~weight, data=data.reg)\ndata.reg$predicted <- predict(fit.reg)\ndata.reg$residuals <- residuals((fit.reg))\n\nggplot(data.reg, aes(x=weight, plasma)) + geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  geom_segment(aes(xend = weight, yend = predicted), color=\"blue\") +\n  geom_point(aes(y = predicted), shape = 1) +\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_bw() + xlab(\"body weight [kg]\") + ylab(\"plasma volume [liters]\")\n\n```\n\n\n<br>\n\nLet $\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:\n\n- Then $\\epsilon_i = y_i - \\hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model\n- RSS, the **residual sum of squares** is defined as: $$RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2$$ or\nequivalently as: $$RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2$$\n- the least squares approach chooses $\\hat{\\alpha}$ and $\\hat{\\beta}$ **to minimize the RSS**. With some calculus, a good video explanation for the interested ones is [here](https://www.youtube.com/watch?v=ewnc1cXJmGA), we get @thm-lss\n\n::: {#thm-lss}\n## Least squares estimates for a simple linear regression\n\n$$\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}$$\n$$\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}$$\n\nwhere:\n\n- $\\bar{x}$: mean value of $x$\n- $\\bar{y}$: mean value of $y$\n- $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2$\n- $S_{yy}$: sum of squares of $Y$ defined as  $S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2$\n- $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$\n\n:::\n\n\n<!-- We can further re-write the above sum of squares to obtain -->\n\n<!-- - sum of squares of $X$, $$S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})$$ -->\n<!-- - sum of products of $X$ and $Y$ -->\n\n<!-- $$S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}$$ -->\n\n\n::: {#exm-lss}\n\n## Least squares\n\nLet's try least squares method to find coefficient estimates in the **\"body weight and plasma volume example\"**\n\n```{r}\n#| code-fold: false\n\n# initial data\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# rename variables for convenience\nx <- weight\ny <- plasma\n\n# mean values of x and y\nx.bar <- mean(x)\ny.bar <- mean(y)\n\n# Sum of squares\nSxx <-  sum((x - x.bar)^2)\nSxy <- sum((x-x.bar)*(y-y.bar))\n\n# Coefficient estimates\nbeta.hat <- Sxy / Sxx\nalpha.hat <- y.bar - Sxy/Sxx*x.bar\n\n# Print estimated coefficients alpha and beta\nprint(alpha.hat)\nprint(beta.hat)\n\n```\n\n:::\n\n\nIn R we can use `lm()`, the built-in function, to fit a linear regression model and we can replace the above code with one line\n\n```{r}\n#| code-fold: false\nlm(plasma ~ weight)\n```\n\n## Intercept and Slope\n- Linear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n- $\\alpha$ is known as the **intercept**\n- $\\beta$ is known as the **slope**\n\n```{r}\n#| label: fig-lm-parameters\n#| fig-cap: \"Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)\"\n#| echo: false\n#| fig-height: 7\n#| fig-width: 7\n\nmodel <- lm(plasma ~ weight)\nalpha <- model$coefficients[1]\nbeta <- model$coefficients[2]\n\npar(mfcol=c(2,2), mar=c(4,4,3,2))\n\n# Values from regression model: plasma_volume = 0.0857 + 0.043615*x\n\n# Fitted line\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\nabline(lm(plasma~weight), col=\"red\") # regression line\ntext(65, 3.3, \"plasma = 0.0857 + 0.0436 * weight\", cex=1)\n\n# Beta 1 example b\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid(), xlim=c(60, 70), ylim=c(2.8, 3.2))\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h = alpha + beta * 65, col = \"blue\",  lty = 3)\nabline(h = alpha + beta * 66, col = \"blue\",  lty = 3)\nsegments(x0=65, y0=alpha + beta * 65, x1=66, y1=alpha + beta * 65, col=\"blue\")\nsegments(x0=66, y0=alpha + beta * 65, x1=66, y1=alpha + beta * 66, col=\"blue\")\ntext(67, 2.94, expression(beta), cex=1.2, col=\"blue\")\ntext(61, alpha + beta * 65 - 0.02, round(alpha + beta * 65, 2), cex=1.2, col=\"blue\")\ntext(61, alpha + beta * 66 + 0.02, round(alpha + beta * 66, 2), cex=1.2, col=\"blue\")\n\n\n# Beta 1 example a\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid())\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h = alpha + beta * 65, col = \"blue\",  lty = 3)\nabline(h = alpha + beta * 70, col = \"blue\",  lty = 3)\nsegments(x0=65, y0=alpha + beta * 65, x1=70, y1=alpha + beta * 65, col=\"blue\")\nsegments(x0=70, y0=alpha + beta * 65, x1=70, y1=alpha + beta * 70, col=\"blue\")\ntext(72, 2.95, expression(beta), cex=1.2, col=\"blue\")\ntext(60, alpha + beta * 65 + 0.05, round(alpha + beta * 65, 2), cex=1.2, col=\"blue\")\ntext(60, alpha + beta * 70 + 0.05, round(alpha + beta * 70, 2), cex=1.2, col=\"blue\")\n\n\n# Beta 0 example a\nplot(weight, plasma, pch=19, las=1, xlab = \"body weight [kg]\", ylab=\"plasma volume [l]\",  panel.first = grid(), xlim=c(-20, 80), ylim=c(0, 5))\nabline(lm(plasma~weight), col=\"red\") # regression line\nabline(h=alpha, col=\"blue\", lty = 3) # regression line\nsegments(x0=65, y0=2.92, x1=66, y1=2.92, col=\"blue\")\nsegments(x0=66, y0=2.92, x1=66, y1=2.964, col=\"blue\")\ntext(15, 0.4, expression(alpha), cex=1.2, col=\"blue\")\ntext(33, 0.5, paste(\"= \", round(alpha,3), sep=\"\"), cex=1.2, col=\"blue\")\n\n```\n\n## Hypothesis testing\n\n**Is there a relationship between the response and the predictor?**\n\n- the calculated $\\hat{\\alpha}$ and $\\hat{\\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**\n- their precision is measured by their **estimated standard errors**, `e.s.e`($\\hat{\\alpha}$) and `e.s.e`($\\hat{\\beta}$)\n- these estimated standard errors are used in **hypothesis testing** and in constructing **confidence and prediction intervals**\n\n**The most common hypothesis test** involves testing the ``null hypothesis`` of:\n\n- $H_0:$ There is no relationship between $X$ and $Y$\n- versus the ``alternative hypothesis`` $H_a:$ there is some relationship between $X$ and $Y$\n\n**Mathematically**, this corresponds to testing:\n\n- $H_0: \\beta=0$\n- versus $H_a: \\beta\\neq0$\n- since if $\\beta=0$ then the model $Y_i=\\alpha+\\beta x_i + \\epsilon_i$ reduces to $Y=\\alpha + \\epsilon_i$\n\n**Under the null hypothesis** $H_0: \\beta = 0$\n<!-- we have: $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)$$  -->\n![](figures/linear-models/lm-tstatistics.png)\n\n- $n$ is number of observations\n- $p$ is number of model parameters\n- $\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}$ is the ratio of the departure of the estimated value of a parameter, $\\hat\\beta$, from its hypothesized value, $\\beta$, to its standard error, called `t-statistics`\n- the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom\n\n::: {#exm-hypothesis-testing}\n\n## Hypothesis testing\n\nLet's look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the `R summary` of the model\n\n```{r}\n#| code-fold: false\n\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n\n```\n\n:::\n\n- Under `Estimate` we see estimates of our model coefficients, $\\hat{\\alpha}$ (intercept) and $\\hat{\\beta}$ (slope, here weight), followed by their estimated standard errors, `Std. Errors`\n- If we were to test if there is an **association between weight and plasma volume** we would write under the null hypothesis $H_0: \\beta = 0$ $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582$$\n- and we would **compare** `t-statistics` to `Student's t distribution` with $n-p = 8 - 2 = 6$ degrees of freedom (as we have 8 observations and two model parameters, $\\alpha$ and $\\beta$)\n- we can use **Student's t distribution table** or **R code** to obtain the associated *P*-value\n\n```{r}\n#| code-fold: false\n2*pt(2.856582, df=6, lower=F)\n```\n\n- here the observed t-statistics is large and therefore yields a small *P*-value, meaning that **there is sufficient evidence to reject null hypothesis in favor of the alternative** and conclude that there is a significant association between weight and plasma volume\n\n\n## Vector-matrix notations\n\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of **multiple regression**, with more than one explanatory variable in the model, it is **more efficient to use vectors and matrices to define the regression model**.\n\nLet's **rewrite** our simple linear regression model $Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n$ **into vector-matrix notation** in **6 steps**.\n\n1. First we rename our $\\alpha$ to $\\beta_0$ and $\\beta$ to $\\beta_1$ as it is easier to keep tracking the number of model parameters this way\n\n2. Then we notice that we actually have $n$ equations such as:\n$$y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1$$\n$$y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2$$\n$$y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3$$\n$$\\dots$$\n$$y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n$$\n\n3. We group all $Y_i$ and $\\epsilon_i$ into column vectors:\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$ and\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n\n4. We stack two parameters $\\beta_0$ and $\\beta_1$ into another column vector:$$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$$\n\n5. We append a vector of ones with the single predictor for each $i$ and create a matrix with two columns called **design matrix** $$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}$$\n\n6. We write our linear model in a vector-matrix notations as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n::: {#def-vector-matrix-lm}\n\n## vector matrix form of the linear model\n\nThe vector-matrix representation of a linear model with $p-1$ predictors can be written as\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nwhere:\n\n- $\\mathbf{Y}$ is $n \\times1$ vector of observations\n- $\\mathbf{X}$ is $n \\times p$ **design matrix**\n- $\\boldsymbol\\beta$ is $p \\times1$ vector of parameters\n- $\\boldsymbol\\epsilon$ is $n \\times1$ vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, $\\sigma^2$)\n\nIn full, the above vectors and matrix have the form:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1    \\\\\n  \\vdots \\\\\n  \\beta_{p}\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_{1,1} & \\dots & x_{1,p-1} \\\\\n  1 & x_{2,1} & \\dots & x_{2,p-1} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & x_{n,1} & \\dots & x_{n,p-1}\n\\end{bmatrix}$\n\n:::\n\n\n::: {#thm-lss-vector-matrix}\n\n## Least squares in vector-matrix notation\n\nThe least squares estimates for a linear regression of the form:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nis given by:\n$$\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n\n:::\n\n\n::: {#exm-vector-matrix-notation}\n\n## vector-matrix-notation\n\nFollowing the above definition we can write the **\"weight - plasma volume model\"** as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\nwhere:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n 2.75  \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12\n\\end{bmatrix}$\n\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{8}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & 58.0 \\\\\n  1 & 70.0 \\\\\n  1 & 74.0 \\\\\n  1 & 63.5 \\\\\n  1 & 62.0 \\\\\n  1 & 70.5 \\\\\n  1 & 71.0 \\\\\n  1 & 66.0 \\\\\n\\end{bmatrix}$\n\nand we can estimate model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n\nWe can do it by hand or in `R` as follows:\n\n```{r}\n#| code-fold: false\n\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\nprint(X)\n\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n\n```\n\n:::\n\n## Confidence intervals and prediction intervals\n\n- when we estimate coefficients we can also find their **confidence intervals**, typically 95\\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter\n- we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval\n\n::: {#exm-prediction-and-intervals}\n\n## Prediction and intervals\n\nLet's:\n\n- find confidence intervals for our coefficient estimates\n- predict plasma volume for a men weighting 60 kg\n- find prediction interval\n- plot original data, fitted regression model, predicted observation and prediction interval\n\n```{r}\n#| code-fold: false\n\n# fit regression model\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n\n# find confidence intervals for the model coefficients\nconfint(model)\n\n# predict plasma volume for a new observation of 60 kg\n# we have to create data frame with a variable name matching the one used to build the model\nnew.obs <- data.frame(weight = 60)\npredict(model, newdata = new.obs)\n\n# find prediction intervals\nprediction.interval <- predict(model, newdata = new.obs,  interval = \"prediction\")\nprint(prediction.interval)\n\n# plot the original data, fitted regression and predicted value\nplot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\", ylim=c(2,4))\nlines(weight, model$fitted.values, col=\"red\") # fitted model in red\npoints(new.obs, predict(model, newdata = new.obs), pch=19, col=\"blue\") # predicted value at 60kg\nsegments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval\n\n```\n\n:::\n\n## Assessing model fit\n\n- Earlier we learned how to estimate parameters in a liner model using least squares estimation.\n- Now we will consider how to assess the goodness of fit of a model, i.e. how well does the model explain our data. \n- We do that by calculating the amount of variability in the response that is explained by the model.\n\n### $R^2$: summary of the fitted model {-}\n- considering a simple linear regression, the simplest model, **Model 0**, we could consider fitting is $$Y_i = \\beta_0+ \\epsilon_i$$ that corresponds to a line that run through the data but lies parallel to the horizontal axis\n- in our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple)\n\n```{r}\n#| collapse: true\n#| echo: false\n#| fig-width: 6\n#| fig-height: 5\n#| fig-align: center\n\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nplot(weight, plasma, pch=19, xlab=\"Weight [kg]\", ylab=\"Plasma volume [l]\")\nabline(h=mean(plasma), col=\"purple\")\n\n```\n\n- TSS, denoted **Total corrected sum-of-squares** is the residual sum-of-squares for Model 0\n$$S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}$$ corresponding the to the sum of squared distances to the purple line\n\n```{r}\n#| echo: false\n#| fig-width: 6\n#| fig-height: 5\n#| fig-align: center\n\nplot(weight, plasma, pch=19, xlab=\"Weight [kg]\", ylab=\"Plasma volume [l]\")\nabline(h=mean(plasma), col=\"purple\")\n\nfor (i in 1:length(weight)){\n  segments(weight[i], plasma[i], weight[i], mean(plasma))\n}\n\n```\n\n- Fitting **Model 1** of the form $$Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$$ we have earlier defined\n- **RSS**, the residual sum-of-squares as:\n$$RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n- that corresponds to the squared distances between the observed values $y_i, \\dots,y_n$ to fitted values $\\hat{y_1}, \\dots \\hat{y_n}$, i.e. distances to the red fitted line\n\n```{r, echo=F, fig.align=\"center\", fig.width=5, fig.height=4}\n#| echo: false\n#| fig-width: 6\n#| fig-height: 5\n#| fig-align: center\n\nplot(weight, plasma, pch=19, xlab=\"Weight [kg]\", ylab=\"Plasma volume [l]\")\nabline(lm(plasma ~ weight), col=\"red\")\nmodel <- lm(plasma ~ weight)\n\nfor (i in 1:length(weight)){\n  segments(weight[i], plasma[i], weight[i], model$fitted.values[i])\n}\n\n```\n\n::: {#def-r2}\n\n## $R^2$ {-}\n\nA simple but useful measure of model fit is given by $$R^2 = 1 - \\frac{RSS}{TSS}$$ where:\n\n- RSS is the residual sum-of-squares for Model 1, the fitted model of interest\n- TSS is the sum of squares of the **null model**\n\n:::\n\n- $R^2$ quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model\n- $R^2$ is also referred as **coefficient of determination**\n- It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data\n- Values of $R^2$ approaching 1 indicate the model to be a good fit\n- Values of $R^2$ less than 0.5 suggest that the model gives rather a poor fit to the data\n\n### $R^2$ and correlation coefficient {-}\n\n::: {#thm-r2}\n## $R^2$\n\nIn the case of simple linear regression:\n\nModel 1: $Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$\n$$R^2 = r^2$$\nwhere:\n\n- $R^2$ is the coefficient of determination\n- $r^2$ is the sample correlation coefficient\n\n:::\n\n\n### $R^2(adj)$ {-}\n- in the case of multiple linear regression, where there is more than one explanatory variable in the model\n- we are using the adjusted version of $R^2$ to assess the model fit\n- as the number of explanatory variables increase, $R^2$ also increases\n- $R^2(adj)$ takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model\n\n\n::: {#thm-r2adj}\n\n## $R^2(adj)$\n\nFor any multiple linear regression\n$$Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i$$ $R^2(adj)$ is defined as\n$$R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}$$ where\n\n- $p$ is the number of independent predictors, i.e. the number of variables in the model, excluding the constant\n\n$R^2(adj)$ can also be calculated from $R^2$:\n$$R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}$$\n\n:::\n\n<!-- We can calculate the values in R and compare the results to the output of linear regression -->\n\n```{r}\n#| collapse: true\n#| code-fold: false\n#| include: false\n#| eval: false\n\nhtwtgen <- read.csv(\"data/lm/heights_weights_genders.csv\")\nhead(htwtgen)\nattach(htwtgen)\n\n## Simple linear regression\nmodel.simple <- lm(Height ~ Weight, data=htwtgen)\n\n# TSS\nTSS <- sum((Height - mean(Height))^2)\n\n# RSS\n# residuals are returned in the model type names(model.simple)\nRSS <- sum((model.simple$residuals)^2)\nR2 <- 1 - (RSS/TSS)\n\nprint(R2)\nprint(summary(model.simple))\n\n## Multiple regression\nmodel.multiple <- lm(Height ~ Weight + Gender, data=htwtgen)\nn <- length(Weight)\np <- 1\n\nRSS <- sum((model.multiple$residuals)^2)\nR2_adj <- 1 - (RSS/(n-p-1))/(TSS/(n-1))\n\nprint(R2_adj)\nprint(summary(model.multiple))\n\n```\n\n## The assumptions of a linear model\n\n```{r}\n#| message: false\n#| warning: false\n#| echo: false\n\n# load libraries\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(ggplot2)\n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2m) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %>%\n  na.omit()\n\n# reset rownames\nrownames(data_diabetes) <- NULL\n\n```\n\nUp until now we were fitting models and discussed how to assess the model fit. Before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data. Informally we have been using box plots and scatter plots to look at the data. There are however formal definitions of the assumptions.\n\n**Assumption A: The deterministic part of the model captures all the non-random structure in the data**\n\n- This implies that the **mean of the errors $\\epsilon_i$** is zero.\n- Tt applies only over the range of explanatory variables.\n\n**Assumption B: the scale of variability of the errors is constant at all values of the explanatory variables**\n\n- Practically we are looking at whether the observations are equally spread on both side of the regression line.\n\n**Assumption C: the errors are independent**\n\n- Broadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another.\n\n**Assumptions D: the errors are normally distributed**\n\n- This will allow us to describe the variation in the model's parameters estimates and therefore make inferences about the population from which our sample was taken.\n\n**Assumption E: the values of the explanatory variables are recorded without error**\n\n- This one is not possible to check via examining the data, instead we have to consider the nature of the experiment.\n\n\n### Checking assumptions\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**. We use plots such as:\n\n1. Histograms or normal probability plots of $\\hat{\\epsilon_i}$\n- useful to check the assumption of normality\n\n2. Plots of $\\hat{\\epsilon_i}$ versus the fitted values $\\hat{y_i}$\n- used to detect changes in error variance\n- used to check if the mean of the errors is zero\n\n3. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{ij}$\n- this helps to check that the variable $x_j$ has a linear relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{kj}$ that is **not** in the model\n- this helps to check whether the additional variable $x_k$ might have a relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ in the order of the observations were collected\n- this is useful to check whether errors might be correlated over time\n\nLet's fit a simple model to predict `BMI` given `waist` for the diabetes study and see if the model meets the assumptions of linear models.\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-cap: \"Default diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Simple regression to model BMI with waist explanatory variable.\"\n#| fig-cap-location: margin\n#| fig-height: 8\n\n# fit simple linear regression model\nmodel <- lm(BMI ~ waist, data = data_diabetes)\n\n# plot diagnostic plots of the linear model\n# by default plot(model) calls four diagnostics plots\n# par() divides plot window in 2 x 2 grid\npar(mfrow=c(2,2))\nplot(model)\n```\n\n\n- The residual plots provides examples of a situation where the assumptions appear to be met.\n- The linear regression appears to describe data quite well.\n- There is no obvious trend of any kind in the residuals vs. fitted values (the shape is scattered) with potential few outliers that we may want to decided to exclude later.\n- Points lie reasonably well along the line in the normal probability plot, hence normality appears to be met.\n\n**Examples of assumptions not being met**\n\n```{r}\n#| fig-cap: Example of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case\n#| fig-align: center\n#| fig-cap-location: margin\n#| echo: false\n\nknitr::include_graphics(\"figures/linear-models/lm-assumptions-01.png\", dpi = 100)\n\n```\n\n```{r}\n#| fig-cap: Example of non-constant variance\n#| fig-align: center\n#| fig-cap-location: margin\n#| echo: false\nknitr::include_graphics(\"figures/linear-models/lm-assumptions-02.png\", dpi = 80)\n\n```\n\n```{r}\n#| fig-cap: Example of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected\n#| fig-align: center\n#| fig-cap-location: margin\n#| echo: false\nknitr::include_graphics(\"figures/linear-models/lm-assumptions-03.png\", dpi = 100)\n\n```\n\n\n### Influential observations\n- Sometimes individual observations can exert a great deal of influence on the fitted model.\n- One routine way of checking for this is to fit the model $n$ times, missing out each observation in turn.\n- If we removed i-th observation and compared the fitted value from the full model, say $\\hat{y_j}$ to those obtained by removing this point, denoted $\\hat{y_{j(i)}}$ then\n- observations with a high Cook's distance (measuring the effect of deleting a given observation) could be influential.\n\nLet's remove some observation with higher Cook's distance from protein data set, re-fit our model and compare the diagnostics plots\n\n```{r}\n#| code-fold: false\n#| fig-width: 8\n#| fig-height: 8\n\n# observations to be removed (based on Residuals vs. Leverage plot)\nobs <- c(13, 78, 83, 84)\n\n# fit models removing observations\ndata_diabetes_flr <- data_diabetes[-obs, ]\n\nmodel_flr <- lm(BMI ~ waist, data = data_diabetes_flr)\n\n# plot diagnostics plot\npar(mfrow=c(2,2))\nplot(model_flr)\n\n```\n\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":"html_document","warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"lm-intro.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["references.bib"],"theme":"cosmo","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}