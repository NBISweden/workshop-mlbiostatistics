---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Generalized linear models

```{r}
#| message: false
#| include: false
#| echo: false

library(tidyverse)
library(magrittr)
library(faraway)
library(ggplot2)  
library(gridExtra)
library(kableExtra)
library(ggiraphExtra)

font.size <- 12
col.blue.light <- "#a6cee3"
col.blue.dark <- "#1f78b4"
my.ggtheme <- 
  theme_bw() + 
  theme(axis.title = element_text(size = font.size), 
        axis.text = element_text(size = font.size), 
        legend.text = element_text(size = font.size), 
        legend.title = element_blank(), 
        legend.position = "top") 
        

# add obesity and diabetes status to diabetes faraway data
inch2m <- 2.54/100
pound2kg <- 0.45
data_diabetes <- diabetes %>%
  mutate(height  = height * inch2m, height = round(height, 2)) %>% 
  mutate(waist = waist * inch2m) %>%  
  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%
  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% 
  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c("No", "Yes"))) %>% 
  mutate(diabetic = ifelse(glyhb > 7, "Yes", "No"), diabetic = factor(diabetic, levels = c("No", "Yes"))) %>%
  na.omit()

```

## Why Generalized Linear Models (GLMs)
- GLMs extend linear model framework to outcome variables that do not follow normal distribution.
- They are most frequently used to model binary, categorical or count data.
- For instance, fitting a regression line to binary data yields predicted values that could take any value, including $<0$, 
- not to mention that it is hard to argue that the values of 0 and 1s are normally distributed.


```{r}
#| label: fig-log-example
#| message: false
#| warning: false
#| fig-cap: "Example of fitting linear model to binary data, to model obesity status coded as 1 (Yes) and 0 (No) with waist variable. Linear model does not fit the data well in this case and the predicted values can take any value, not only 0 and 1."
#| fig-height: 4
#| fig-width: 5

data_diabetes %>%
  mutate(obese = as.numeric(obese) - 1) %>%
  ggplot(aes(y=obese, x=waist)) +
  geom_jitter(width=0, height = 0) +
  geom_smooth(method="lm", se=FALSE, color=col.blue.dark) + 
  my.ggtheme

```


## Logistic regression

Let's look again at the binary `obesity` status data and try to fit logistic regression model using `waist` as explanatory variable instead of fitting inappropriate here simple linear model.

```{r}
#| label: fig-obesity
#| fig-cap: "Obesity status data: jittered plot (left) and box plot of waist stratified by obesity status for the 130 study participants."
#| fig-cap-location: margin

p1 <- data_diabetes %>%
  mutate(obese = as.numeric(obese) - 1) %>%
  ggplot(aes(y=obese, x=waist)) +
  geom_jitter(width=0, height = 0.05, alpha = 0.7) +
  my.ggtheme

p2 <- data_diabetes %>%
  ggplot(aes(x = obese, y = waist, fill = obese)) + 
  geom_boxplot() + 
  scale_fill_brewer(palette = "Set2") + 
  my.ggtheme

grid.arrange(p1, p2, ncol = 2)
```

- Since the response variable takes only two values (Yes/No) we use GLM model 
- to fit **logistic regression** model for the **probability of suffering from obesity (Yes).**
- We let $p_i=P(Y_i=1)$ denote the probability of suffering from obesity (success)
- and we assume that the response follows binomial distribution: $Y_i \sim Bi(1, p_i)$ distribution.
- We can then write the regression model now as: 
$$log(\frac{p_i}{1-p_i})=\beta_0 + \beta_1x_i$$
and given the properties of logarithms this is also equivalent to:
$$p_i = \frac{exp(\beta_0 + \beta_1x_i)}{1 + exp(\beta_0 + \beta_1x_i)}$$

<br>

- In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a **link function**.
- Here, the **link function** $log(\frac{p_i}{1-p_i})$ provides the link between the binomial distribution of $Y_i$ (suffering from obesity) and the linear predictor (waist) 
- Thus the **GLM model** can be written as $$g(\mu_i)=\mathbf{X}\boldsymbol\beta$$ where `g()` is the link function.

<br>

In R we can use `glm()` function to fit GLM models:
```{r}
#| code-fold: false
#| collapse: true
#| message: false
#| warning: false
#| fig-cap: "Fitted logistic model to diabetes data given the 130 study participants and using waist as explantatory variable to model obesity status."

# re-code obese status from Yes/No to 1/0
data_diabetes <- 
  data_diabetes %>%
  mutate(obese = as.numeric(obese) - 1)

# fit logistic regression model
logmodel_1 <- glm(obese ~ waist, family = binomial(link="logit"), data = data_diabetes)

# print model summary
print(summary(logmodel_1))

# plot
ggPredict(logmodel_1) + 
  my.ggtheme
# to get predictions use predict() functions
# if no new observations is specified predictions are returned for the values of exploratory variables used
# we specify response to return prediction on the probability scale
obese_predicted <- predict(logmodel_1, type="response")
print(head(obese_predicted))

```

- The regression equation for the fitted model is:
$$log(\frac{\hat{p_i}}{1-\hat{p_i}})=-17.357  +  17.174\cdot x_i$$
- We see from the output that $\hat{\beta_0} = -17.357$ and $\hat{\beta_1} = 17.174$.
- These estimates are arrived at via **maximum likelihood estimation**, something that is out of scope here.

### Hypothesis testing {-}

- Similarly to linear models, we can determine whether each variable is related to the outcome of interest by testing the null hypothesis that the relevant logistic regression coefficient is zero.
- This can be performed by **Wald test** which is equals to estimated logistic regression coefficient divided by its standard error and follows the Standard Normal distribution:
$$W = \frac{\hat\beta-\beta}{e.s.e.(\hat\beta)} \sim N(0,1)$$.
- Alternatively, its square approximates the Chi-squared distribution with 1 degree of freedom:
$$W^2 = \frac{(\hat\beta-\beta)^2}{\hat {var}(\hat\beta)} \sim \chi_1^2$$.


In our example, we can check whether `waist` is associated with `obesity status` by testing null hypothesis: $H_0:\beta_1=0$. We calculate Wald statistics as $W^2 = \frac{(\hat\beta-0)}{\hat {e.s.e}(\hat\beta)} = \frac{17.174}{2.974} = 5.774714$ and we can find the corresponding p-value using standard normal distribution:

```{r}
#| code-fold: false
#| collapse: true
2*pnorm(5.774714, lower.tail = F)
```

which confirms the summary output shown previously above and shows that there is enough evidence to reject the null hypothesis at 5% significance level $p-value << 0.05$ and conclude that there is a significant association between `waist` and `obesity status`.

### Deviance {-}

- Deviance is the number that measures the goodness of fit of a logistic regression model.
- We use saturated and residual deviance to assess model, instead of $R^2$ or $R^2(adj)$.
- We can also use deviance to check the association between explanatory variable and the outcome, an alternative and slightly more powerful test than Wald test (although these two test give similar results when sample size is large).
- In the **likelihood ratio test** the test statistics is the **deviance** for the full model minus the deviance for the full model excluding the relevant explanatory variable. This test statistics follow a Chi-squared distribution with 1 degree of freedom.

<br>

For instance, given our model we have null deviance of 274.4 and residual deviance of 268.7. The difference 5.7 is larger than than 95th percentile of $\chi^2(129-128)$ = 3.841459, where 129 is degrees of freedom for null model and 128 is degrees of freedom for null model excluding the `waist` explanatory variable. 
```{r}
#| code-fold: false
#| collapse: true
qchisq(df=1, p=0.95)
```
- Again $5.7 > 3.84$ and we can conclude that `waist` is a significant term in the model.

### Odds ratios {-}

- In logistic regression we often interpret the model coefficients by taking $e^{\hat{\beta}}$
- and we talk about **odd ratios**.
- For instance we can say, given our above model, $e^{17.174} = 28745736$ that for each unit increase in `waist` the odds of suffering from obesity get multiplied by 28745736. 


These odds ratios are very high as here we are modeling obesity status with waist measurements, and increasing one unit in waist would mean adding up additional 1m to waist measurements. Typically:

- Odd ratios of 1.0 (or close to 1.0) indicates that exposure is not associated with the outcome.
- Odds ratios $> 1.0$ indicates that the odds of exposure among cases are greater than the odds of
exposure among controls. 
- Odds raitos $< 1.0$ indicates that the odds of exposure among cases are lower than the odds of
exposure among controls. 
- The magnitude of the odds ratio is called the **strength of the association**. The further away an odds ratio is from 1.0, the more likely it is that the relationship between the exposure and the disease is causal. For example, an odds ratio of 1.2 is above 1.0, but is not a strong association. An odds ratio of 10 suggests a stronger association.

### Other covariates {-}

- We can use the same logic as in multiple regression to expand by models by additional variables, numerical, binary or categorical. 
- For instance, we can test whether there is a gender effect when suffering from obesity:

```{r}
#| message: false
#| warning: false
#| code-fold: false
#| collapse: true
#| fig-cap: "Obesity status model with logistic regression given waist and gender exploratory variables. There is some seperation between the fitted lines for men and women and the model summary shows that there is enough evidence to reject a null hypothesis o no association between gender and obesity status."
#| fig-cap-location: margin

# fit logistic regression including age and gender
logmodel_2 <- glm(obese ~ waist + gender, family = binomial(link="logit"), data = data_diabetes)

# print model summary
print(summary(logmodel_2))

# plot model
ggPredict(logmodel_2) + 
  my.ggtheme + 
  scale_color_brewer(palette = "Set2")

```


## Poisson regression

- GLMs can be also applied to count data
- For instance to model hospital admissions due to respiratory disease or number of bird nests in a certain habitat. 
- Here, we commonly assume that data follow the Poisson distribution $Y_i \sim Pois(\mu_i)$
- and the corresponding model is:
$$E(Y_i)=\mu_i = \eta_ie^{\mathbf{x_i}^T\boldsymbol\beta}$$ with a log link $\ln\mu_i = \ln \eta_i + \mathbf{x_i}^T\boldsymbol\beta$
- Hypothesis testing and assessing model fit follows the same logic as in logistic regression.

::: {#exm-poisson}

## Number of cancer cases

Suppose we wish to model $Y_i$ the number of cancer cases in the i-th intermediate geographical location (IG) in Glasgow. We have collected data for 271 small regions with between 2500 and 6000 people living in them. Together with cancer occurrence with have the following data:

- Y\_all: number of cases of all types of cancer in the IG in 2013
- E\_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013
- pm10: air pollution
- smoke: percentage of people in an area that smoke
- ethnic: percentage of people who are non-white
- log.price: natural log of average house price
- easting and northing: co-ordinates of the central point of the IG divided by 10000

We can model the **rate of occurrence of cancer** using the very same `glm` function:Â¨
- now we use **poisson family distribution** to model counts
- and we will include an **offset term** to the model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions.


:::



```{r}
#| code-fold: false
#| collapse: true

# Read in and preview data
cancer <- read.csv("data/lm/cancer.csv")
head(cancer)

# fit Poisson regression
epid1 <- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), 
             family = poisson, 
             data = cancer)

print(summary(epid1))
```


**Rate ratio**

- Similarly to logistic regression, it is common to look at the $e^\beta$.
- For instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient.
- The ppm10 coefficient is positive, 0.0500269, indicating that cancer incidence rate increases with increased air pollution.
- The rate ratio allows us to quantify by how much, here by a factor of $e^{0.0500269} = 1.05$.

