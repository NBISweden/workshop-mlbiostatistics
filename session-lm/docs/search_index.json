[["index.html", "Linear Models Preface", " Linear Models Olga Dethlefsen 2022-08-30 Preface Linear models allows us to answer questions such as: is there a relationship between exposure and outcome, e.g. body weight and plasma volume? how strong is the relationship between the two variables? what will be a predicted value of the outcome given a new set of exposure values? how accurately can we predict outcome? which variables are associated with the response, e.g. is it body weight and height that can explain the plasma volume or is it just the body weight? Do you see a mistake or a typo? I would be grateful if you let me know via olga.dethlefsen@nbis.se This repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/ "],["introduction-to-linear-models.html", "Chapter 1 Introduction to linear models 1.1 Why linear models? With linear models we can answer questions such as: 1.2 Statistical vs. deterministic relationship 1.3 What linear models are and are not 1.4 Terminology 1.5 Simple linear regression 1.6 Least squares 1.7 Intercept and Slope 1.8 Hypothesis testing 1.9 Vector-matrix notations 1.10 Confidence intervals and prediction intervals 1.11 Exercises: linear models I Answers to selected exercises (linear models)", " Chapter 1 Introduction to linear models Aims to introduce concept of linear models using simple linear regression Learning outcomes to understand what a linear model is and be familiar with the terminology to be able to state linear model in the general vector-matrix notation to be able to use the general vector-matrix notation to numerically estimate model parameters to be able to use lm() function for model fitting, parameter estimation, hypothesis testing and prediction 1.1 Why linear models? With linear models we can answer questions such as: is there a relationship between exposure and outcome, e.g. body weight and plasma volume? how strong is the relationship between the two variables? what will be a predicted value of the outcome given a new set of exposure values? how accurately can we predict outcome? which variables are associated with the response, e.g. is it body weight and height that can explain the plasma volume or is it just the body weight? 1.2 Statistical vs. deterministic relationship Relationships in probability and statistics can generally be one of three things: deterministic, random, or statistical: a deterministic relationship involves an exact relationship between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation \\(Fahrenheit=\\frac{9}{5}\\cdot Celcius+32\\) there is no relationship between variables in the random relationship, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year a statistical relationship is a mixture of deterministic and random relationship, e.g. the savings that Olga has left in the bank account depend on Olga’s monthly salary income (deterministic part) and the money spent on buying succulents (random part) Figure 1.1: Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship ; b) statistical relationship between \\(x\\) and \\(y\\) is not perfect (increasing relationship), c) statistical relationship between \\(x\\) and \\(y\\) is not perfect (decreasing relationship), d) random signal 1.3 What linear models are and are not A linear model is one in which the parameters appear linearly in the deterministic part of the model e.g. simple linear regression through the origin is a simple linear model of the form \\[Y_i = \\beta x + \\epsilon\\] often used to express a relationship of one numerical variable to another, e.g. the calories burnt and the kilometers cycled linear models can become quite advanced by including many variables, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of bike, or the transformation of the variables, e.g. a function of kilometers cycled squared More examples where model parameters appear linearly: \\(Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i\\) \\(Y_i = \\alpha + \\beta x_i^2 \\epsilon\\) \\(Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon\\) \\(Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon\\) and an example on a non-linear model where parameter \\(\\beta\\) appears in the exponent of \\(x_i\\) \\(Y_i = \\alpha + x_i^\\beta + \\epsilon\\) 1.4 Terminology There are many terms and notations used interchangeably: \\(y\\) is being called: response outcome dependent variable \\(x\\) is being called: exposure explanatory variable dependent variable predictor covariate 1.5 Simple linear regression It is used to check the association between the numerical outcome and one numerical explanatory variable In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure For example, let’s look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is. Example data: weight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) plasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) Figure 1.2: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Figure 1.3: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable The equation for the red line is: \\[Y_i=0.086 + 0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8\\] and in general: \\[Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n\\] In other words, by finding the best-fitting straight line we are building a statistical model to represent the relationship between plasma volume (\\(Y\\)) and explanatory body weight variable (\\(x\\)) If we were to use our model \\(Y_i=0.086 + 0.044 \\cdot x_i\\) to find plasma volume given a weight of 58 kg (our first observation, \\(i=1\\)), we would notice that we would get \\(Y=0.086 + 0.044 \\cdot 58 = 2.638\\), not exactly \\(2.75\\) as we have for our first man in our dataset that we started with, i.e. \\(2.75 - 2.638 = 0.112 \\neq 0\\). We thus add to the above equation an error term to account for this and now we can write our simple regression model more formally as: \\[\\begin{equation} Y_i=\\alpha + \\beta \\cdot x_i + \\epsilon_i \\tag{1.1} \\end{equation}\\] where: \\(x\\): is called: exposure variable, explanatory variable, dependent variable, predictor, covariate \\(y\\): is called: response, outcome, dependent variable \\(\\alpha\\) and \\(\\beta\\) are model coefficients and \\(\\epsilon_i\\) is an error terms 1.6 Least squares in the above “body weight - plasma volume” example, the values of \\(\\alpha\\) and \\(\\beta\\) have just appeared in practice, \\(\\alpha\\) and \\(\\beta\\) values are unknown and we use data to estimate these coefficients, noting the estimates with a hat, \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) least squares is one of the methods of parameters estimation, i.e. finding \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) Figure 1.4: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line Let \\(\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i\\) be the prediction \\(y_i\\) based on the \\(i\\)-th value of \\(x\\): Then \\(\\epsilon_i = y_i - \\hat{y_i}\\) represents the \\(i\\)-th residual, i.e. the difference between the \\(i\\)-th observed response value and the \\(i\\)-th response value that is predicted by the linear model RSS, the residual sum of squares is defined as: \\[RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2\\] or equivalently as: \\[RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2\\] the least squares approach chooses \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) to minimize the RSS. With some calculus, a good video explanation for the interested ones is here, we get Theorem 1.1 Theorem 1.1 \\iffalse (Least squares estimates for a simple linear regression) \\[\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}\\] \\[\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}\\] where: \\(\\bar{x}\\): mean value of \\(x\\) \\(\\bar{y}\\): mean value of \\(y\\) \\(S_{xx}\\): sum of squares of \\(X\\) defined as \\(S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2\\) \\(S_{yy}\\): sum of squares of \\(Y\\) defined as \\(S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2\\) \\(S_{xy}\\): sum of products of \\(X\\) and \\(Y\\) defined as \\(S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})\\) We can further re-write the above sum of squares to obtain sum of squares of \\(X\\), \\[S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})\\] sum of products of \\(X\\) and \\(Y\\) \\[S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}\\] Example: Least squares Let’s try least squares method to find coefficient estimates in the “body weight and plasma volume example” # initial data weight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) plasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) # rename variables for convenience x &lt;- weight y &lt;- plasma # mean values of x and y x.bar &lt;- mean(x) y.bar &lt;- mean(y) # Sum of squares Sxx &lt;- sum((x - x.bar)^2) Sxy &lt;- sum((x-x.bar)*(y-y.bar)) # Coefficient estimates beta.hat &lt;- Sxy / Sxx alpha.hat &lt;- y.bar - Sxy/Sxx*x.bar # Print estimated coefficients alpha and beta print(alpha.hat) ## [1] 0.08572428 print(beta.hat) ## [1] 0.04361534 In R we can use lm, the built-in function, to fit a linear regression model and we can replace the above code with one line lm(plasma ~ weight) ## ## Call: ## lm(formula = plasma ~ weight) ## ## Coefficients: ## (Intercept) weight ## 0.08572 0.04362 1.7 Intercept and Slope Linear regression gives us estimates of model coefficient \\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\) \\(\\alpha\\) is known as the intercept \\(\\beta\\) is known as the slope Figure 1.5: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red) 1.8 Hypothesis testing Is there a relationship between the response and the predictor? the calculated \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are estimates of the population values of the intercept and slope and are therefore subject to sampling variation their precision is measured by their estimated standard errors, e.s.e(\\(\\hat{\\alpha}\\)) and e.s.e(\\(\\hat{\\beta}\\)) these estimated standard errors are used in hypothesis testing and in constructing confidence and prediction intervals The most common hypothesis test involves testing the null hypothesis of: \\(H_0:\\) There is no relationship between \\(X\\) and \\(Y\\) versus the alternative hypothesis \\(H_a:\\) there is some relationship between \\(X\\) and \\(Y\\) Mathematically, this corresponds to testing: \\(H_0: \\beta=0\\) versus \\(H_a: \\beta\\neq0\\) since if \\(\\beta=0\\) then the model \\(Y_i=\\alpha+\\beta x_i + \\epsilon_i\\) reduces to \\(Y=\\alpha + \\epsilon_i\\) Under the null hypothesis \\(H_0: \\beta = 0\\) \\(n\\) is number of observations \\(p\\) is number of model parameters \\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}\\) is the ratio of the departure of the estimated value of a parameter, \\(\\hat\\beta\\), from its hypothesized value, \\(\\beta\\), to its standard error, called t-statistics the t-statistics follows Student’s t distribution with \\(n-p\\) degrees of freedom Example: Hypothesis testing Let’s look again at our example data. This time we will not only fit the linear regression model but look a bit more closely at the R summary of the model weight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) plasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) model &lt;- lm(plasma ~ weight) print(summary(model)) ## ## Call: ## lm(formula = plasma ~ weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27880 -0.14178 -0.01928 0.13986 0.32939 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.08572 1.02400 0.084 0.9360 ## weight 0.04362 0.01527 2.857 0.0289 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2188 on 6 degrees of freedom ## Multiple R-squared: 0.5763, Adjusted R-squared: 0.5057 ## F-statistic: 8.16 on 1 and 6 DF, p-value: 0.02893 Under Estimate we see estimates of our model coefficients, \\(\\hat{\\alpha}\\) (intercept) and \\(\\hat{\\beta}\\) (slope, here weight), followed by their estimated standard errors, Std. Errors If we were to test if there is an association between weight and plasma volume we would write under the null hypothesis \\(H_0: \\beta = 0\\) \\[\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582\\] and we would compare t-statistics to Student's t distribution with \\(n-p = 8 - 2 = 6\\) degrees of freedom (as we have 8 observations and two model parameters, \\(\\alpha\\) and \\(\\beta\\)) we can use Student’s t distribution table or R code to obtain the associated p-value 2*pt(2.856582, df=6, lower=F) ## [1] 0.02893095 here the observed t-statistics is large and therefore yields a small p-value, meaning that there is sufficient evidence to reject null hypothesis in favor of the alternative and conclude that there is a significant association between weight and plasma volume 1.9 Vector-matrix notations While in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of multiple regression, with more than one explanatory variable in the model, it is more efficient to use vectors and matrices to define the regression model. Let’s rewrite our simple linear regression model \\(Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n\\) into vector-matrix notation in 6 steps. First we rename our \\(\\alpha\\) to \\(\\beta_0\\) and \\(\\beta\\) to \\(\\beta_1\\) as it is easier to keep tracking the number of model parameters this way Then we notice that we actually have \\(n\\) equations such as: \\[y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1\\] \\[y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2\\] \\[y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3\\] \\[\\dots\\] \\[y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n\\] We group all \\(Y_i\\) and \\(\\epsilon_i\\) into column vectors: \\(\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}\\) and \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}\\) We stack two parameters \\(\\beta_0\\) and \\(\\beta_1\\) into another column vector:\\[\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\] We append a vector of ones with the single predictor for each \\(i\\) and create a matrix with two columns called design matrix \\[\\mathbf{X}=\\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n} \\end{bmatrix}\\] We write our linear model in a vector-matrix notations as: \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\] Definition: vector matrix form of the linear model The vector-matrix representation of a linear model with \\(p-1\\) predictors can be written as \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\] where: \\(\\mathbf{Y}\\) is \\(n \\times1\\) vector of observations \\(\\mathbf{X}\\) is \\(n \\times p\\) design matrix \\(\\boldsymbol\\beta\\) is \\(p \\times1\\) vector of parameters \\(\\boldsymbol\\epsilon\\) is \\(n \\times1\\) vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, \\(\\sigma^2\\)) In full, the above vectors and matrix have the form: \\(\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}\\) \\(\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix}\\) \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}\\) \\(\\mathbf{X}=\\begin{bmatrix} 1 &amp; x_{1,1} &amp; \\dots &amp; x_{1,p-1} \\\\ 1 &amp; x_{2,1} &amp; \\dots &amp; x_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n,1} &amp; \\dots &amp; x_{n,p-1} \\end{bmatrix}\\) Theorem 1.2 \\iffalse (Least squares in vector-matrix notation) The least squares estimates for a linear regression of the form: \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\] is given by: \\[\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\] Example: vector-matrix notation Following the above definition we can write the “weight - plasma volume model” as: \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\] where: \\(\\mathbf{Y}=\\begin{bmatrix} 2.75 \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12 \\end{bmatrix}\\) \\(\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{8} \\end{bmatrix}\\) \\(\\mathbf{X}=\\begin{bmatrix} 1 &amp; 58.0 \\\\ 1 &amp; 70.0 \\\\ 1 &amp; 74.0 \\\\ 1 &amp; 63.5 \\\\ 1 &amp; 62.0 \\\\ 1 &amp; 70.5 \\\\ 1 &amp; 71.0 \\\\ 1 &amp; 66.0 \\\\ \\end{bmatrix}\\) and we can estimate model parameters using \\(\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\). We can do it by hand or in R as follows: n &lt;- length(plasma) # no. of observation Y &lt;- as.matrix(plasma, ncol=1) X &lt;- cbind(rep(1, length=n), weight) X &lt;- as.matrix(X) # print Y and X to double-check that the format is according to the definition print(Y) ## [,1] ## [1,] 2.75 ## [2,] 2.86 ## [3,] 3.37 ## [4,] 2.76 ## [5,] 2.62 ## [6,] 3.49 ## [7,] 3.05 ## [8,] 3.12 print(X) ## weight ## [1,] 1 58.0 ## [2,] 1 70.0 ## [3,] 1 74.0 ## [4,] 1 63.5 ## [5,] 1 62.0 ## [6,] 1 70.5 ## [7,] 1 71.0 ## [8,] 1 66.0 # least squares estimate # solve() finds inverse of matrix beta.hat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y print(beta.hat) ## [,1] ## 0.08572428 ## weight 0.04361534 1.10 Confidence intervals and prediction intervals when we estimate coefficients we can also find their confidence intervals, typically 95% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter we can also use linear regression models to predict the response value given a new observation and find prediction intervals. Here, we look at any specific value of \\(x_i\\), and find an interval around the predicted value \\(y_i&#39;\\) for \\(x_i\\) such that there is a 95% probability that the real value of y (in the population) corresponding to \\(x_i\\) is within this interval Example: prediction and intervals Let’s: find confidence intervals for our coefficient estimates predict plasma volume for a men weighting 60 kg find prediction interval plot original data, fitted regression model, predicted observation and prediction interval # fit regression model model &lt;- lm(plasma ~ weight) print(summary(model)) ## ## Call: ## lm(formula = plasma ~ weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27880 -0.14178 -0.01928 0.13986 0.32939 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.08572 1.02400 0.084 0.9360 ## weight 0.04362 0.01527 2.857 0.0289 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2188 on 6 degrees of freedom ## Multiple R-squared: 0.5763, Adjusted R-squared: 0.5057 ## F-statistic: 8.16 on 1 and 6 DF, p-value: 0.02893 # find confidence intervals for the model coefficients confint(model) ## 2.5 % 97.5 % ## (Intercept) -2.419908594 2.59135716 ## weight 0.006255005 0.08097567 # predict plasma volume for a new observation of 60 kg # we have to create data frame with a variable name matching the one used to build the model new.obs &lt;- data.frame(weight = 60) predict(model, newdata = new.obs) ## 1 ## 2.702645 # find prediction intervals prediction.interval &lt;- predict(model, newdata = new.obs, interval = &quot;prediction&quot;) print(prediction.interval) ## fit lwr upr ## 1 2.702645 2.079373 3.325916 # plot the original data, fitted regression and predicted value plot(weight, plasma, pch=19, xlab=&quot;weight [kg]&quot;, ylab=&quot;plasma [l]&quot;, ylim=c(2,4)) lines(weight, model$fitted.values, col=&quot;red&quot;) # fitted model in red points(new.obs, predict(model, newdata = new.obs), pch=19, col=&quot;blue&quot;) # predicted value at 60kg segments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediciton interval 1.11 Exercises: linear models I Before you begin: Focus on Exercise 1.1. If time allows, feel free to check the remaining exercises. Data for exercises can be downloaded from Github using Link 1 or from Canvas under Files -&gt; data_exercises/linear-models Exercise 1.1 Protein levels in pregnancy The researchers were interested whether protein levels in expectant mothers are changing throughout the pregnancy. Observations have been taken on 19 healthy women and each woman was at different stage of pregnancy (gestation). Assuming linear model: \\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\), where \\(Y_i\\) corresponds to protein levels in i-th observation and taking summary statisitcs: \\(\\sum_{i=1}^{n}x_i = 456\\) \\(\\sum_{i=1}^{n}x_i^2 = 12164\\) \\(\\sum_{i=1}^{n}x_iy_i = 369.87\\) \\(\\sum_{i=1}^{n}y_i = 14.25\\) \\(\\sum_{i=1}^{n}y_i^2 = 11.55\\) find the least square estimates of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) knowing that e.s.e(\\(\\hat{\\beta}) = 0.003295\\) can we: reject the null hypothesis that the is no relationship between protein level and gestation, i.e. perform a hypothesis test to test \\(H_0:\\beta = 0\\); can we reject the null hypothesis that \\(\\beta = 0.02\\), i.e. perform a hypothesis test to test \\(H_0:\\beta = 0.02\\) write down the linear model in the vector-matrix notation and identify response, parameter, design and error matrices read in “protein.csv” data into R, set Y as protein (response) and calculate using matrix functions the least squares estimates of model coefficients use lm() function in R to check your calculations use the fitted model in R to predict the value of protein levels at week 20. Try plotting the data, fitted linear model and the predicted value to assess whether your prediction is to be expected. a) write down the model in vector-matrix notation b) load data from “potatoes.csv” and use least squares estimates to obtain estimates of model coefficients c) use lm() function to verify your calculations d) perform a hypothesis test to test \\(H_0:\\gamma=0\\); and comment whether there is a significant quadratic relationship e) predict glucose concentration at storage time 4 and 16 weeks. Plot the data, the fitted model and the predicted values Exercise 1.2 Linear models form Which of the following models are linear models and why? \\(Y_i=\\alpha + \\beta x_i + \\epsilon_i\\) \\(Y_i=\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\epsilon_i\\) \\(Y_i=\\alpha + \\beta x_i + \\gamma x_i^2 + \\epsilon_i\\) \\(Y_i=\\alpha + \\gamma x_i^\\beta + \\epsilon_i\\) Answers to selected exercises (linear models) Exr. 1.1 \\(S_{xx} = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n} = 12164 - \\frac{456^2}{19} = 1220\\) \\(S_{xy} = \\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n} = 369.87 - \\frac{(456 \\cdot 14.25)}{19} = 27.87\\) \\(\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}} = 27.87 / 1220 = 0.02284\\) \\(\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x} = \\frac{14.25}{19}-\\frac{27.87}{1220}\\cdot \\frac{456}{19} = 0.20174\\) We can calculate test statistics following: \\(\\frac{\\hat{\\beta} - \\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p) = \\frac{0.02284 - 0}{0.003295} = 6.934\\) where the value follows Student’s t distribution with \\(n-p = 19 - 2 = 17\\) degrees of freedom. We can now estimate the a p-value using Student’s t distribution table or use R function 2*pt(6.934, df=17, lower=F) ## [1] 2.414315e-06 As p-value &lt;&lt; 0.001 there is sufficient evidence to reject \\(H_0\\) in favor of \\(H_1\\), thus we can conclude that there is a significant relationship between protein levels and gestation Similarly, we can test \\(H_0:\\beta = 0.02\\), i.e. \\(\\frac{\\hat{\\beta} - \\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p) = \\frac{0.02284 - 0.02}{0.20174} = 0.01407753\\). Now the test statistics is small 2*pt(0.01407753, df=17, lower=F) ## [1] 0.988932 p-value is large and hence there is no sufficient evidence to reject \\(H_0\\) and we can conclude that \\(\\beta = 0.02\\) We can rewrite the linear model in vector-matrix formation as \\(\\mathbf{Y}= \\mathbf{\\beta}\\mathbf{X} + \\mathbf{\\epsilon}\\) where: response \\(\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{19} \\end{bmatrix}\\) parameters \\(\\boldsymbol\\beta=\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\\) design matrix \\(\\mathbf{X}=\\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{19} \\end{bmatrix}\\) errors \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{19} \\end{bmatrix}\\) The least squares estimates in vector-matrix notation is \\(\\hat{\\boldsymbol\\beta}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and we can calculate this in R # read in data data.protein &lt;- read.csv(&quot;data/lm/protein.csv&quot;) # print out top observations head(data.protein) ## Protein Gestation ## 1 0.38 11 ## 2 0.58 12 ## 3 0.51 13 ## 4 0.38 15 ## 5 0.58 17 ## 6 0.67 18 # define Y and X matrices given the data n &lt;- nrow(data.protein) # nu. of observations Y &lt;- as.matrix(data.protein$Protein, ncol=1) # response X &lt;- as.matrix(cbind(rep(1, length=n), data.protein$Gestation)) # design matrix head(X) # double check that the design matrix looks like it should ## [,1] [,2] ## [1,] 1 11 ## [2,] 1 12 ## [3,] 1 13 ## [4,] 1 15 ## [5,] 1 17 ## [6,] 1 18 # least squares estimate beta.hat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y # beta.hat is a matrix that contains our alpha and beta in the model print(beta.hat) ## [,1] ## [1,] 0.20173770 ## [2,] 0.02284426 We use lm() function to check our calculations # fit linear regression model and print model summary protein &lt;- data.protein$Protein # our Y gestation &lt;- data.protein$Gestation # our X model &lt;- lm(protein ~ gestation) print(summary(model)) ## ## Call: ## lm(formula = protein ~ gestation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.16853 -0.08720 -0.01009 0.08578 0.20422 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.201738 0.083363 2.420 0.027 * ## gestation 0.022844 0.003295 6.934 2.42e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1151 on 17 degrees of freedom ## Multiple R-squared: 0.7388, Adjusted R-squared: 0.7234 ## F-statistic: 48.08 on 1 and 17 DF, p-value: 2.416e-06 new.obs &lt;- data.frame(gestation = 20) y.pred &lt;- predict(model, newdata = new.obs) # we can visualize the data, fitted linear model (red), and the predicted value (blue) plot(gestation, protein, pch=19, xlab=&quot;gestation [weeks]&quot;, ylab=&quot;protein levels [mgml-1]&quot;) lines(gestation, model$fitted.values, col=&quot;red&quot;) points(new.obs, y.pred, col=&quot;blue&quot;, pch=19, cex = 1) Exr. ?? We can rewrite the linear model in vector-matrix formation as \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\] where: response \\(\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{14} \\end{bmatrix}\\) parameters \\(\\boldsymbol\\beta=\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{bmatrix}\\) design matrix \\(\\mathbf{X}=\\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2\\\\ 1 &amp; x_2 &amp; x_2^2\\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{14} &amp; x_{14}^2 \\end{bmatrix}\\) errors \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{14} \\end{bmatrix}\\) load data to from “potatoes.csv” and use least squares estimates for obtain estimates of model coefficients data.potatoes &lt;- read.csv(&quot;data/lm/potatoes.csv&quot;) # define matrices n &lt;- nrow(data.potatoes) Y &lt;- data.potatoes$Glucose X1 &lt;- data.potatoes$Weeks X2 &lt;- (data.potatoes$Weeks)^2 X &lt;- cbind(rep(1, length(n)), X1, X2) X &lt;- as.matrix(X) # least squares estimate # beta here refers to the matrix of model coefficients incl. alpha, beta and gamma beta.hat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y print(beta.hat) ## [,1] ## 200.169312 ## X1 -19.443122 ## X2 1.030423 we use lm() function to verify our calculations: model &lt;- lm(Y ~ X1 + X2) print(summary(model)) ## ## Call: ## lm(formula = Y ~ X1 + X2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.405 -11.250 -8.071 12.911 29.286 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 200.1693 15.0527 13.298 4.02e-08 *** ## X1 -19.4431 3.1780 -6.118 7.54e-05 *** ## X2 1.0304 0.1406 7.329 1.49e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.4 on 11 degrees of freedom ## Multiple R-squared: 0.8694, Adjusted R-squared: 0.8457 ## F-statistic: 36.61 on 2 and 11 DF, p-value: 1.373e-05 perform a hypothesis test to test \\(H_0:\\gamma=0\\); and comment whether we there is a significant quadratic term \\(\\frac{\\hat{\\gamma} - \\gamma}{e.s.e(\\hat{\\gamma})} \\sim t(n-p) = \\frac{1.030423 - 0}{0.1406} = 7.328755\\) where the value follows Student’s t distribution with \\(n-p = 19 - 2 = 17\\) degrees of freedom. We can now estimate the a p-value using Student’s t distribution table or use a function in R 2*pt(7.328755, df=14-3, lower=F) ## [1] 1.487682e-05 As p-value &lt;&lt; 0.001 there is sufficient evidence to reject \\(H_0\\) in favor of \\(H_1\\), thus we can conclude that there is a significant quadratic relationship between glucose and storage time predict glucose concentration at storage time 4 and 16 weeks new.obs &lt;- data.frame(X1 = c(4, 16), X2 = c(4^2, 16^2)) pred.y &lt;- predict(model, newdata = new.obs) plot(data.potatoes$Weeks, data.potatoes$Glucose, xlab=&quot;Storage time [weeks]&quot;, ylab=&quot;Glucose [g/kg]&quot;, pch=19) lines(data.potatoes$Weeks, model$fitted.values, col=&quot;red&quot;) points(new.obs[,1], pred.y, pch=19, col=&quot;blue&quot;) "],["regression-coefficients.html", "Chapter 2 Regression coefficients 2.1 Interpreting and using linear regression models 2.2 Example: plasma volume 2.3 Example: Galapagos Islands 2.4 Example: Height and gender 2.5 Example: Heigth, weight and gender I 2.6 Example: Heigth, weight and gender II 2.7 Exercises: linear models II Answers to selected exercises (linear models II)", " Chapter 2 Regression coefficients Aims to clarify the interpretation of the fitted linear models Learning outcomes to use lm() function to fit multiple linear regression model to be able to interpret the output of the model to be able to use lm() function to check for association between variables, group effects and interaction terms 2.1 Interpreting and using linear regression models In previous section we have seen how to find estimates of model coefficients, using theorems and vector-matrix notations. Now, we will focus on what model coefficient values tell us and how to interpret them And we will look at the common cases of using linear regression models We will do this via analyzing some examples 2.2 Example: plasma volume # data weight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) plasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) # fit regression model model &lt;- lm(plasma ~ weight) # plot the original data and fitted regression line plot(weight, plasma, pch=19, xlab=&quot;weight [kg]&quot;, ylab=&quot;plasma [l]&quot;) lines(weight, model$fitted.values, col=&quot;red&quot;) # fitted model in red grid() # print model summary print(summary(model)) ## ## Call: ## lm(formula = plasma ~ weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27880 -0.14178 -0.01928 0.13986 0.32939 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.08572 1.02400 0.084 0.9360 ## weight 0.04362 0.01527 2.857 0.0289 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2188 on 6 degrees of freedom ## Multiple R-squared: 0.5763, Adjusted R-squared: 0.5057 ## F-statistic: 8.16 on 1 and 6 DF, p-value: 0.02893 Model: \\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\) where \\(x_i\\) corresponds to \\(weight_i\\) Slope The value of slope tells us how and by much the outcome changes with a unit change in \\(x\\) If we go up in weight 1 kg what would be our expected change in plasma volume\\(^1\\)? And if we go up in weight 10 kg what would be our expected change in plasma volume\\(^2\\)? Intercept the intercept, often labeled the constant, is the value of Y when \\(x_i=0\\) in models where \\(x_i\\) can be equal 0, the intercept is simply the expected mean value of response in models where \\(x_i\\) cannot be equal 0, like in our plasma example no weight makes no sense for healthy men, the intercept has no intrinsic meaning the intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome \\(^1\\): If we go up in weight 1 kg we would expect our plasma volume to increase by 0.04 liter since \\(\\hat{\\beta} = 0.04\\) \\(^2\\) If we go up in weight 10 kg we would expect our plasma voluem to increase by \\(0.04 \\cdot 10 = 0.4\\) liter 2.3 Example: Galapagos Islands Researchers were interested in biological diversity on the Galapagos islands. They’ve collected data on number of plant species (Species) and number of endemic species on 30 islands as well as some descriptors of the islands such as area [\\(\\mathrm{km^2}\\)], elevation [m], distance to nearest island [km], distance to Santa Cruz [km] and the area of the adjacent island [\\(\\mathrm{km^2}\\)]. The preview of data is here: # Data is available via faraway package if(!require(faraway)){ install.packages(&quot;faraway&quot;) library(faraway) } head(gala) ## Species Endemics Area Elevation Nearest Scruz Adjacent ## Baltra 58 23 25.09 346 0.6 0.6 1.84 ## Bartolome 31 21 1.24 109 0.6 26.3 572.33 ## Caldwell 3 3 0.21 114 2.8 58.7 0.78 ## Champion 25 9 0.10 46 1.9 47.4 0.18 ## Coamano 2 1 0.05 77 1.9 1.9 903.82 ## Daphne.Major 18 11 0.34 119 8.0 8.0 1.84 And we can fit a linear regression model to model number of Species given the remaining variables. Let’s keep aside for now that number of Species is actually a count variable, not a continuous numerical variable, we just want to estimate the number of Species for now. Fitted Model \\(Y_i = \\beta_0 + \\beta_1 Area_i + \\beta_2 Elevation_i + \\beta_3 Nearest_i + \\beta_4 Scruz_i + \\beta_5 Adjacent_i + \\epsilon_i\\) # fit multiple linear regression and print model summary model1 &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala) print(summary(model1)) ## ## Call: ## lm(formula = Species ~ Area + Elevation + Nearest + Scruz + Adjacent, ## data = gala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -111.679 -34.898 -7.862 33.460 182.584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.068221 19.154198 0.369 0.715351 ## Area -0.023938 0.022422 -1.068 0.296318 ## Elevation 0.319465 0.053663 5.953 3.82e-06 *** ## Nearest 0.009144 1.054136 0.009 0.993151 ## Scruz -0.240524 0.215402 -1.117 0.275208 ## Adjacent -0.074805 0.017700 -4.226 0.000297 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60.98 on 24 degrees of freedom ## Multiple R-squared: 0.7658, Adjusted R-squared: 0.7171 ## F-statistic: 15.7 on 5 and 24 DF, p-value: 6.838e-07 Using the model compare two islands in terms of number of species if the second island has an elevation 1 m higher than the first one?\\(^1\\) if the second island has an elevation 100 m higher than the first one?\\(^2\\) if the second island is 100 km closer to Santa Cruz?\\(^3\\) overall, is there a relationship between the response \\(Y\\) (Species) and predictors?\\(^4\\) \\(^1\\) the second island will have 0.32 species more than the first one, \\(\\hat{\\beta_2} = 0.319465 \\approx 0.32\\) \\(^2\\) the second island will have \\(0.32 \\cdot 100 = 32\\) more species than the first one \\(^3\\) the second island would have \\(-0.24 \\cdot 100 = -24\\) less species than the first if there was enough evidence to reject the null hypothesis of \\(\\beta_4 = 0\\); It is not appropriate to try to interpret non-significant coefficients. \\(^4\\) we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of \\(H_0: \\beta=0\\) versus \\(H_0: \\beta\\neq0\\) to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of \\[H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\] versus the alternative \\[H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero\\]. This hypothesis test is performed by computing F-statistics reported in the model summary and calculated as \\(F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\\) where \\(TSS = \\sum(y_i - \\bar{y})^2\\) and \\(RSS = \\sum(y_i - \\hat{y_i})^2\\). Here, the \\(F-statsitics = 15.7\\) and the associated \\(p-value &lt; 0.05\\) so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall signficiant relationship between response (Species) and predictors. Not so easy: alternative model Consider an alternative model where we only use elevation to model the number of species \\[Y_i = \\beta_0 + \\beta_1 Elevation_i + \\epsilon_i\\] We fit the model in R and look at the model summary model2 &lt;- lm(Species ~ Elevation, data = gala) print(summary(model2)) ## ## Call: ## lm(formula = Species ~ Elevation, data = gala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -218.319 -30.721 -14.690 4.634 259.180 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.33511 19.20529 0.590 0.56 ## Elevation 0.20079 0.03465 5.795 3.18e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 78.66 on 28 degrees of freedom ## Multiple R-squared: 0.5454, Adjusted R-squared: 0.5291 ## F-statistic: 33.59 on 1 and 28 DF, p-value: 3.177e-06 Using the alternative model compare again two islands in terms of number of species if the second island has an elevation 1 m higher than the first one?\\(^1\\) if the second island has an elevation 100 m higher than the first one?\\(^2\\) \\(^1\\) the second island will have 0.20 species more than the first one \\(^2\\) the seond island will have \\(0.20 \\cdot 100 = 20\\) more species Specific interpretation Obviously there is difference between 32 and 20 times more species given the same elevation difference as obtained by the multiple regression (first model) and simple regression (alternative model). Our interpretations need to be more specific and we say that a unit increase in \\(x\\) with other predictors held constant will produce a change equal to \\(\\hat{\\beta}\\) in the response \\(y\\) It is of course often quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in evaluation is most likely associated with other variables, even though they are not included in the model. Further, our explanation contains no notation of causation, even though the two models are showing a strong association between elevation and number of species. We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model. 2.4 Example: Height and gender Data are available containing the weight [lbs] and height [inches] of 10000 men and women # read in data htwtgen &lt;- read.csv(&quot;data/lm/heights_weights_genders.csv&quot;) head(htwtgen) ## Gender Height Weight ## 1 Male 73.84702 241.8936 ## 2 Male 68.78190 162.3105 ## 3 Male 74.11011 212.7409 ## 4 Male 71.73098 220.0425 ## 5 Male 69.88180 206.3498 ## 6 Male 67.25302 152.2122 # # boxplot for females and males boxplot(htwtgen$Height ~ htwtgen$Gender, xlab=&quot;&quot;, ylab=&quot;Height&quot;, col=&quot;lightblue&quot;) We want to compare the average height of men and women. We can do that using linear regression and including gender as binary variable Model \\[Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i\\] where \\[\\begin{equation} I_{x_i} = \\left\\{ \\begin{array}{cc} 1 &amp; \\mathrm{if\\ } x_i=1 \\\\ 0 &amp; \\mathrm{if\\ } x_i=0 \\\\ \\end{array} \\right. \\end{equation}\\] for some coding, e.g. we choose to set “Female=1” and “Male=0” or vice versa. In R we write: # Note: check that Gender is indeed non-numeric print(class(htwtgen$Gender)) ## [1] &quot;character&quot; # # fit linear regression and print model summary model1 &lt;- lm(Height ~ Gender, data = htwtgen) print(summary(model1)) ## ## Call: ## lm(formula = Height ~ Gender, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.6194 -1.8374 0.0088 1.9185 9.9724 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 63.70877 0.03933 1619.8 &lt;2e-16 *** ## GenderMale 5.31757 0.05562 95.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.781 on 9998 degrees of freedom ## Multiple R-squared: 0.4776, Adjusted R-squared: 0.4775 ## F-statistic: 9140 on 1 and 9998 DF, p-value: &lt; 2.2e-16 Estimates \\[\\hat{\\alpha} = 63.71\\] \\[\\hat{\\beta} = 5.32\\] the lm() function chooses automatically one of the category as baseline, here Females model summary prints the output of the model with the baseline category “hidden” i.e. notice the only label we have is “GenderMale” meaning that we ended-up having a model coded as below: \\[\\begin{equation} I_{x_i} = \\left\\{ \\begin{array}{cc} 1 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\ 0 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\ \\end{array} \\right. \\end{equation}\\] Consequently, if observation \\(i\\) is male then the expected value of height is: \\[E(Height_i|Male) = 63.71 + 5.32 = 69.03\\] and if observation \\(i\\) is female then the expected value of height is: \\[E(Height_i|Male) = 63.71\\] 2.5 Example: Heigth, weight and gender I So as expected, there is a difference in average height between the men and women. Is there a relationship between weight and height? If so, does this relationship depend on gender? library(ggplot2) # plot the data separately for Male and Female ggplot(data=htwtgen, aes(x = Weight, y=Height, col = Gender)) + geom_point(alpha = 0.5) + theme_light() From the plot we can see that height increases with weight. On average, men are taller than women. On average, men weight more than women. The relationship between height and weight appears to be the same for males and females, i.e. height increases with weight for both men and women. To assess the relationship we use a model containing height and gender. Model \\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation} I_{x_i} = \\left\\{ \\begin{array}{cc} 1 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\ 0 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\ \\end{array} \\right. \\end{equation}\\] and \\(x_{2,i}\\) is the weight of person \\(i\\) In R we write: # fit linear model and print model summary model2 &lt;- lm(Height ~ Gender + Weight, data = htwtgen) print(summary(model2)) ## ## Call: ## lm(formula = Height ~ Gender + Weight, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.4956 -0.9583 0.0126 0.9867 5.8358 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.0306678 0.1025161 458.76 &lt;2e-16 *** ## GenderMale -0.9628643 0.0474947 -20.27 &lt;2e-16 *** ## Weight 0.1227594 0.0007396 165.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.435 on 9997 degrees of freedom ## Multiple R-squared: 0.8609, Adjusted R-squared: 0.8609 ## F-statistic: 3.093e+04 on 2 and 9997 DF, p-value: &lt; 2.2e-16 Model together with estimates \\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation} I_{x_i} = \\left\\{ \\begin{array}{cc} 1 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\ 0 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\ \\end{array} \\right. \\end{equation}\\] and \\(x_{2,i}\\) is the weight of person \\(i\\) Estimates \\[\\hat{\\alpha} = 47.031\\] \\[\\hat{\\beta} = -0.963\\] \\[\\hat{\\gamma} = 0.123\\] Using our estimates, for a male of with an example weight of 161.4 we would predict a height of: \\[E(Height_i|Male, Weight = 161.4) = 47.031 - 0.963 + (0.123 \\cdot 161.4) = 65.9\\] and for a female of weight 161.4 we would predict a height of \\[E(Height_i|Female, Weight = 161.4) = 47.031 + (0.123 \\cdot 161.4) = 66.9\\] In R we can plot our data and the fitted moded to verify our calculations: # plot the data separately for men and women # using ggplot() and geom_smooth() ggplot(data=htwtgen, aes(x = Weight, y=Height, col = Gender)) + geom_point(alpha = 0.1) + geom_smooth(method=lm) + theme_light() + guides(color=guide_legend(override.aes=list(fill=NA))) 2.6 Example: Heigth, weight and gender II The fitted lines in the above example are parallel, the slope is modeled to be the same for men and women, and the intercept denotes the group differences It is also possible to allow both intercept and slope being fitted separately for each group This is done when we except that the relationships are different in different groups. And we then talk about including interaction effect, as the two lines may interact (cross). Model \\[Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}\\] where: \\(Y_{i,j}\\) is the height of person \\(j\\) of gender \\(i\\) \\(x_{ij}\\) is the weight of person \\(j\\) of gender \\(i\\) \\(i=1\\) corresponds to men in our example (keeping the same coding as above) \\(i=2\\) corresponds to women In R we define the interaction term with *: # fit linear model with interaction model3 &lt;- lm(Height ~ Gender * Weight, data = htwtgen) print(summary(model3)) ## ## Call: ## lm(formula = Height ~ Gender * Weight, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.4698 -0.9568 0.0092 0.9818 5.7544 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.347783 0.146325 323.579 &lt; 2e-16 *** ## GenderMale -1.683668 0.242119 -6.954 3.78e-12 *** ## Weight 0.120425 0.001067 112.903 &lt; 2e-16 *** ## GenderMale:Weight 0.004493 0.001480 3.036 0.0024 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.435 on 9996 degrees of freedom ## Multiple R-squared: 0.861, Adjusted R-squared: 0.861 ## F-statistic: 2.064e+04 on 3 and 9996 DF, p-value: &lt; 2.2e-16 Now, based on the regression output we would expect: for a men of weight \\(x\\), a height of: \\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\] for a women of weight \\(x\\), a height of \\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\] Estimates \\[\\hat{\\alpha_1} = 45.7\\] \\[\\hat{\\beta_1} = 0.125\\] \\[\\hat{\\alpha_2} = 47.34778\\] \\[\\hat{\\beta_2} = 0.12043\\] We can see from the regression output that the interaction term, “GenderMale:Weight, is significant and therefore the relationship between weight and height is different for men and women. We can plot the fitted model and see that the lines are no longer parallel. We will see clearer example of the interactions in the exercises. # ggiraphExtra makes it easy to visualize fitted models if(!require(ggiraphExtra)){ install.packages(&quot;ggiraphExtra&quot;) library(ggiraphExtra) } ggPredict(model3) + theme_light() + guides(color=guide_legend(override.aes=list(fill=NA))) 2.7 Exercises: linear models II Before you begin: Try to complete Exr. 2.1 and Exr. 2.2 to practice the material above. Exr. 2.3 includes three groups and can be used as a supplementary practice. Data for exercises can be downloaded from Github using Link 1 or from Canvas under Files -&gt; data_exercises/linear-models Exercise 2.1 Given the “height-weight-gender” data: repeat fitting the models with a) gender, b) weight and gender and c) interaction between weight and gender given the model with the interaction term, what is expected height of a man and a women given a weight of 120 lbs? can you use predict() function to check your calculations? Exercise 2.2 When the behavior of a group of trout is studied, some fish are observed to become dominant and others to become subordinate. Dominant fish have freedom of movement whereas subordinate fish tend to congregate in the periphery of the waterway to avoid crossing the path of the dominant fish. Data on energy expenditure and ration of blood obtained were collected as part of a laboratory experiment for 20 trout. Energy and ration is measured in calories per kilo-calorie per trout per day. Use the below code to load the data to R and use linear regression models to answer: is there a relationship between ration obtained and energy expenditure is the relationship between ration obtained and energy expenditure different for each type of fish? Hint: it is good to start with some explanatory plots between every pair of variable # read in data and show preview trout &lt;- read.csv(&quot;data/lm/trout.csv&quot;) # recode the Group variable and treat like categories (factor) trout$Group &lt;- factor(trout$Group, labels=c(&quot;Dominant&quot;, &quot;Subordinate&quot;)) Exercise 2.3 A clinical trial A clinical trial has been carried out to compare three drug treatments which are intended to lower blood pressure in hypertensive patients. The data contains initial values fo systolic blood pressure (bp) in mmHg for each patient and the reduction achieved during the course of the trial. For each patient, allocation to treatment (drug) was carried out randomly and conditions such as the length of the treatment and dose of the drug were standardized as far as possible. Use linear regression to answer questions: is there an association between the reduction in blood pressure and initial blood pressure is reduction in blood pressure different across the treatment (in three drug groups)? is reduction in blood pressure different across the treatment when accounting for initial blood pressure? is reduction in blood pressure changing differently under different treatment? Hint: here we have three categories which can be seen as expanding the model with two categories by an additional one: one category will be treated as baseline blooddrug &lt;- read.csv(&quot;data/lm/bloodrug.csv&quot;) blooddrug$drug &lt;- factor(blooddrug$drug) head(blooddrug) ## initial redn drug ## 1 158 4 1 ## 2 176 21 1 ## 3 174 36 1 ## 4 168 14 1 ## 5 174 34 1 ## 6 186 37 1 Answers to selected exercises (linear models II) Exr. 2.1 htwtgen &lt;- read.csv(&quot;data/lm/heights_weights_genders.csv&quot;) head(htwtgen) ## Gender Height Weight ## 1 Male 73.84702 241.8936 ## 2 Male 68.78190 162.3105 ## 3 Male 74.11011 212.7409 ## 4 Male 71.73098 220.0425 ## 5 Male 69.88180 206.3498 ## 6 Male 67.25302 152.2122 # a) model1 &lt;- lm(Height ~ Gender, data = htwtgen) model2 &lt;- lm(Height ~ Gender + Weight, data = htwtgen) model3 &lt;- lm(Height ~ Gender * Weight, data = htwtgen) # print(summary(model1)) # print(summary(model2)) # print(summary(model3)) use equations to find the height for men and women respectively: \\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\] \\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\] # for men new.obs &lt;- data.frame(Weight=120, Gender=&quot;Male&quot;) predict(model3, newdata = new.obs) ## 1 ## 60.65427 # for female new.obs &lt;- data.frame(Weight=120, Gender=&quot;Female&quot;) predict(model3, newdata = new.obs) ## 1 ## 61.79882 Exr. 2.2 # read in data and show preview trout &lt;- read.csv(&quot;data/lm/trout.csv&quot;) # recode the Group variable and treat like categories (factor) trout$Group &lt;- factor(trout$Group, labels=c(&quot;Dominant&quot;, &quot;Subordinate&quot;)) head(trout) ## Energy Ration Group ## 1 44.26 81.35 Dominant ## 2 67.16 91.68 Dominant ## 3 48.15 58.00 Dominant ## 4 34.53 58.63 Dominant ## 5 67.93 91.93 Dominant ## 6 72.45 96.56 Dominant # plot data # boxplots of Energy and Ration per group boxplot(trout$Energy ~ trout$Group, xlab=&quot;&quot;, ylab=&quot;Energy&quot;) boxplot(trout$Ration ~ trout$Group, xlab=&quot;&quot;, ylab=&quot;Ration&quot;) # scatter plot of Ration vs. Energy plot(trout$Ration, trout$Energy, pch=19, xlab=&quot;Ration&quot;, ylab=&quot;Energy&quot;) From the exploratory plots we see that there is some sort of relationship between ratio and energy, i.e. energy increase while ration obtained increases From boxplots we see that the ration obtained may be different in two groups # Is there a relationship between ration obtained and energy expenditure model1 &lt;- lm(Energy ~ Ration, data = trout) print(summary(model1)) ## ## Call: ## lm(formula = Energy ~ Ration, data = trout) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.704 -4.703 -0.578 2.432 33.506 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.3037 12.5156 0.344 0.734930 ## Ration 0.7211 0.1716 4.203 0.000535 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.05 on 18 degrees of freedom ## Multiple R-squared: 0.4953, Adjusted R-squared: 0.4673 ## F-statistic: 17.66 on 1 and 18 DF, p-value: 0.0005348 # from the regression output we can see that yes, a unit increase in ratio increase energy expenditure by 0.72 # Is there a relationship between ration obtained and energy expenditure different for each type of fish? # we first check if there is a group effect model2 &lt;- lm(Energy ~ Ration + Group, data = trout) print(summary(model2)) ## ## Call: ## lm(formula = Energy ~ Ration + Group, data = trout) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.130 -5.139 -0.870 2.199 25.622 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -24.8506 13.3031 -1.868 0.07910 . ## Ration 1.0109 0.1626 6.218 9.36e-06 *** ## GroupSubordinate 17.0120 5.1075 3.331 0.00396 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.647 on 17 degrees of freedom ## Multiple R-squared: 0.6946, Adjusted R-squared: 0.6587 ## F-statistic: 19.33 on 2 and 17 DF, p-value: 4.182e-05 ggPredict(model2) + theme_light() + guides(color=guide_legend(override.aes=list(fill=NA))) # and whether there is an interaction effect model3 &lt;- lm(Energy ~ Ration * Group, data = trout) print(summary(model3)) ## ## Call: ## lm(formula = Energy ~ Ration * Group, data = trout) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.7951 -6.0981 -0.1554 3.9612 23.5946 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.2330 15.9394 -0.579 0.570483 ## Ration 0.8149 0.1968 4.141 0.000767 *** ## GroupSubordinate -18.9558 22.6934 -0.835 0.415848 ## Ration:GroupSubordinate 0.5200 0.3204 1.623 0.124148 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.214 on 16 degrees of freedom ## Multiple R-squared: 0.7378, Adjusted R-squared: 0.6886 ## F-statistic: 15 on 3 and 16 DF, p-value: 6.537e-05 ggPredict(model3) + theme_light() + guides(color=guide_legend(override.aes=list(fill=NA))) Based on the regression output and plots we can say: there is a relationship between ration obtained and energy expenditure that this relationship is the same in the two groups although the energy expenditure is higher in the dominant fish Exr. 2.3 Yes. The redn and initial were significantly associated (p-value = 0.00312, linear regression). model1 &lt;- lm(redn ~ initial, data = blooddrug) summary(model1) ## ## Call: ## lm(formula = redn ~ initial, data = blooddrug) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.476 -11.705 1.558 9.197 24.392 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -72.7302 29.1879 -2.492 0.02036 * ## initial 0.5902 0.1788 3.301 0.00312 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.79 on 23 degrees of freedom ## Multiple R-squared: 0.3214, Adjusted R-squared: 0.2919 ## F-statistic: 10.89 on 1 and 23 DF, p-value: 0.003125 No. The drug2 and drug3 were not significantly different from drug1 (p-value = 0.714 and p-value = 0.628, respectively). The patients of the drug 1 group had 2.750 higher blood pressure drop (redn) than those of the drug 2 group. However, the difference was relatively small comparing to the standard error of the estimate, which was 7.402. The difference between drug 1 and 3 was relatively small, too. model2 &lt;- lm(redn ~ drug, data = blooddrug) summary(model2) ## ## Call: ## lm(formula = redn ~ drug, data = blooddrug) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.000 -9.286 0.000 12.714 26.000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.250 5.517 4.214 0.000358 *** ## drug2 2.750 7.402 0.372 0.713796 ## drug3 -3.964 8.076 -0.491 0.628379 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.6 on 22 degrees of freedom ## Multiple R-squared: 0.03349, Adjusted R-squared: -0.05437 ## F-statistic: 0.3812 on 2 and 22 DF, p-value: 0.6875 Yes. The redn of the drug2 group was significantly higher than that of the drug1 group after adjustment for the effects of the initial (P = 0.018). The reduction of the patients who got the drug 2 was much higher (13.6906) than the drug 1, comparing to the standard error of the difference (5.3534) after accounting for initial blood pressure. model3 &lt;- lm(redn ~ drug + initial, data = blooddrug) summary(model3) ## ## Call: ## lm(formula = redn ~ drug + initial, data = blooddrug) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.8114 -10.5842 -0.4959 6.2834 16.4265 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -124.8488 28.0674 -4.448 0.000223 *** ## drug2 13.6906 5.3534 2.557 0.018346 * ## drug3 -7.2045 5.4275 -1.327 0.198625 ## initial 0.8895 0.1671 5.323 2.81e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.42 on 21 degrees of freedom ## Multiple R-squared: 0.5886, Adjusted R-squared: 0.5298 ## F-statistic: 10.01 on 3 and 21 DF, p-value: 0.0002666 "],["model-diagnostics.html", "Chapter 3 Model diagnostics 3.1 Assessing model fit 3.2 \\(R^2\\): summary of the fitted model 3.3 \\(R^2\\) and correlation coefficient 3.4 \\(R^2(adj)\\) 3.5 The assumptions of a linear model 3.6 Checking assumptions 3.7 Influential observations 3.8 Selecting best model 3.9 Exercises: linear models III Answers to selected exercises (linear models III)", " Chapter 3 Model diagnostics Aims to introduce concepts of linear models summary and assumptions Learning outcomes to able to interpret \\(R^2\\) and \\(R^2(adj)\\) values state the assumptions of a linear model and assess them using residual plots 3.1 Assessing model fit earlier we learned how to estimate parameters in a liner model using least squares now we will consider how to assess the goodness of fit of a model we do that by calculating the amount of variability in the response that is explained by the model 3.2 \\(R^2\\): summary of the fitted model considering a simple linear regression, the simplest model, Model 0, we could consider fitting is \\[Y_i = \\beta_0+ \\epsilon_i\\] that corresponds to a line that run through the data but lies parallel to the horizontal axis in our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple) TSS, denoted Total corrected sum-of-squares is the residual sum-of-squares for Model 0 \\[S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}\\] corresponding the to the sum of squared distances to the purple line Fitting Model 1 of the form \\[Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\] we have earlier defined RSS, the residual sum-of-squares as: \\[RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\] that corresponds to the squared distances between the observed values \\(y_i, \\dots,y_n\\) to fitted values \\(\\hat{y_1}, \\dots \\hat{y_n}\\), i.e. distances to the red fitted line Definition 3.1 A simple but useful measure of model fit is given by \\[R^2 = 1 - \\frac{RSS}{TSS}\\] where: RSS is the residual sum-of-squares for Model 1, the fitted model of interest TSS is the sum of squares of the null model \\(R^2\\) quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model \\(R^2\\) is also referred as coefficient of determination It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data Values of \\(R^2\\) approaching 1 indicate he model to be a good fit Values of \\(R^2\\) less than 0.5 suggest that the model gives rather a poor fit to the data 3.3 \\(R^2\\) and correlation coefficient Theorem 3.1 In the case of simple linear regression: Model 1: \\(Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\) \\[R^2 = r^2\\] where: \\(R^2\\) is the coefficient of determination \\(r^2\\) is the sample correlation coefficient 3.4 \\(R^2(adj)\\) in the case of multiple linear regression, where there is more than one explanatory variable in the model we are using the adjusted version of \\(R^2\\) to assess the model fit as the number of explanatory variables increase, \\(R^2\\) also increases \\(R^2(adj)\\) takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model Theorem 3.2 For any multiple linear regression \\[Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} + \\epsilon_i\\] \\(R^2(adj)\\) is defined as \\[R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}\\] where \\(p\\) is the number of independent predictors, i.e. the number of variables in the model, excluding the constant \\(R^2(adj)\\) can also be calculated from \\(R^2\\): \\[R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}\\] We can calculate the values in R and compare the results to the output of linear regression htwtgen &lt;- read.csv(&quot;data/lm/heights_weights_genders.csv&quot;) head(htwtgen) ## Gender Height Weight ## 1 Male 73.84702 241.8936 ## 2 Male 68.78190 162.3105 ## 3 Male 74.11011 212.7409 ## 4 Male 71.73098 220.0425 ## 5 Male 69.88180 206.3498 ## 6 Male 67.25302 152.2122 attach(htwtgen) ## Simple linear regression model.simple &lt;- lm(Height ~ Weight, data=htwtgen) # TSS TSS &lt;- sum((Height - mean(Height))^2) # RSS # residuals are returned in the model type names(model.simple) RSS &lt;- sum((model.simple$residuals)^2) R2 &lt;- 1 - (RSS/TSS) print(R2) ## [1] 0.8551742 print(summary(model.simple)) ## ## Call: ## lm(formula = Height ~ Weight, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8142 -0.9907 0.0263 0.9918 5.5950 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.848e+01 7.507e-02 645.8 &lt;2e-16 *** ## Weight 1.108e-01 4.561e-04 243.0 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.464 on 9998 degrees of freedom ## Multiple R-squared: 0.8552, Adjusted R-squared: 0.8552 ## F-statistic: 5.904e+04 on 1 and 9998 DF, p-value: &lt; 2.2e-16 ## Multiple regression model.multiple &lt;- lm(Height ~ Weight + Gender, data=htwtgen) n &lt;- length(Weight) p &lt;- 1 RSS &lt;- sum((model.multiple$residuals)^2) R2_adj &lt;- 1 - (RSS/(n-p-1))/(TSS/(n-1)) print(R2_adj) ## [1] 0.8608793 print(summary(model.multiple)) ## ## Call: ## lm(formula = Height ~ Weight + Gender, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.4956 -0.9583 0.0126 0.9867 5.8358 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.0306678 0.1025161 458.76 &lt;2e-16 *** ## Weight 0.1227594 0.0007396 165.97 &lt;2e-16 *** ## GenderMale -0.9628643 0.0474947 -20.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.435 on 9997 degrees of freedom ## Multiple R-squared: 0.8609, Adjusted R-squared: 0.8609 ## F-statistic: 3.093e+04 on 2 and 9997 DF, p-value: &lt; 2.2e-16 3.5 The assumptions of a linear model up until now we were fitting models and discussed how to assess the model fit before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data informally we have been using box plots and scatter plots to look at the data there are however formal definitions of the assumptions Assumption A: The deterministic part of the model captures all the non-random structure in the data this implies that the mean of the errors \\(\\epsilon_i\\) is zero it applies only over the range of explanatory variables Assumption B: the scale of variability of the errors is constant at all values of the explanatory variables practically we are looking at whether the observations are equally spread on both side of the regression line Assumption C: the errors are independent broadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another Assumptions D: the errors are normally distributed this will allow us to describe the variation in the model’s parameters estimates and therefore make inferences about the population from which our sample was taken Assumption E: the values of the explanatory variables are recorded without error this one is not possible to check via examining the data, instead we have to consider the nature of the experiment 3.6 Checking assumptions Residuals, \\(\\hat{\\epsilon_i} = y_i - \\hat{y_i}\\) are the main ingredient to check model assumptions. We use plots such as: Histograms or normal probability plots of \\(\\hat{\\epsilon_i}\\) useful to check the assumption of normality Plots of \\(\\hat{\\epsilon_i}\\) versus the fitted values \\(\\hat{y_i}\\) used to detect changes in error variance used to check if the mean of the errors is zero Plots of \\(\\hat{\\epsilon_i}\\) vs. an explanatory variable \\(x_{ij}\\) this helps to check that the variable \\(x_j\\) has a linear relationship with the response variable Plots of \\(\\hat{\\epsilon_i}\\) vs. an explanatory variable \\(x_{kj}\\) that is not in the model this helps to check whether the additional variable \\(x_k\\) might have a relationship with the response variable Plots of \\(\\hat{\\epsilon_i}\\) in the order of the observations were collected this is useful to check whether errors might be correlated over time Let’s look at the “good” example going back to our data of protein levels during pregnancy # read in data data.protein &lt;- read.csv(&quot;data/lm/protein.csv&quot;) protein &lt;- data.protein$Protein # our Y gestation &lt;- data.protein$Gestation # our X model &lt;- lm(protein ~ gestation) # plot diagnostic plots of the linear model # by default plot(model) calls four diagnostics plots # par() divides plot window in 2 x 2 grid par(mfrow=c(2,2)) plot(model) the residual plots provides examples of a situation where the assumptions appear to be met the linear regression appears to describe data quite well there is no obvious trend of any kind in the residuals vs. fitted values (the shape is scatted) points lie reasonably well along the line in the normal probability plot, hence normality appears to be met Examples of assumptions not being met Figure 3.1: Example of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case. Figure 3.2: Example of non-constant variance Figure 3.3: Example of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected 3.7 Influential observations Sometimes individual observations can exert a great deal of influence on the fitted model One routine way of checking for this is to fit the model \\(n\\) times, missing out each observation in turn If we removed i-th observation and compared the fitted value from the full model, say \\(\\hat{y_j}\\) to those obtained by removing this point, denoted \\(\\hat{y_{j(i)}}\\) then observations with a high Cook’s distance (measuring the effect of deleting a given observation) could be influential Let’s remove some observation with higher Cook’s distance from protein data set, re-fit our model and compare the diagnostics plots # observations to be removed (based on Residuals vs. Leverage plot) obs &lt;- c(18,7) # fit models removing observations model.2 &lt;- lm(protein[-obs] ~ gestation[-obs]) # plot diagnostics plot par(mfrow=c(2,2)) plot(model.2) 3.8 Selecting best model We have learned what linear models are, how to find estimates and interpret model coefficients and how to check for the overall relationship between response and predictors. We also know how to assess model fit, check model assumptions and find potential outliers. Given a set of predictors, e.g. many genes, how do we arrive at the best model? As a rule of thumb, we want a model that fits the data best and is as simple as possible, meaning it contains only relevant predictors. In practice, this means, that for smaller data sets, e.g. with up to 10 predictors, one works with manually trying different models, including different subsets of predictors, interactions terms and/or their transformations. When the number of predictors is large, one can try automated approaches of feature selection like forward selection or stepwise regression, the last one demonstrated in the exercises below. Finally, as we will learn later in the course, we can use regularization techniques that allow including all parameters in the model but constrain (regularizes) coefficient estimates towards zero for the less relevant predictors, preventing building complex models and thus overfitting. 3.9 Exercises: linear models III Data for exercises can be downloaded from Github using Link 1 or from Canvas under Files -&gt; data_exercises/linear-models Exercise 3.1 Brozek score Researchers collected age, weight, height and 10 body circumference measurements for 252 men in an attempt to find an alternative way of calculate body fat as oppose to measuring someone weight and volume, the latter one by submerging in a water tank. Is it possible to predict body fat using easy-to-record measurements? Use lm() function and fit a linear method to model brozek, score estimate of percent body fat find \\(R^2\\) and \\(R^2(adj)\\) assess the diagnostics plots to check for model assumptions delete observation #86 with the highest Cook’s distance and re-fit the model (model.clean) look at the model summary. Are all variables associated with brozek score? try improving the model fit by removing variables with the highest p-value first and re-fitting the model until all the variables are significantly associated with the response (p value less than 0.1); note down the \\(R^2(adj)\\) values while doing so compare the output models for model.clean and final model To access and preview the data: data(fat, package = &quot;faraway&quot;) Answers to selected exercises (linear models III) Exr. 3.1 # access and preview data data(fat, package = &quot;faraway&quot;) head(fat) ## brozek siri density age weight height adipos free neck chest abdom hip ## 1 12.6 12.3 1.0708 23 154.25 67.75 23.7 134.9 36.2 93.1 85.2 94.5 ## 2 6.9 6.1 1.0853 22 173.25 72.25 23.4 161.3 38.5 93.6 83.0 98.7 ## 3 24.6 25.3 1.0414 22 154.00 66.25 24.7 116.0 34.0 95.8 87.9 99.2 ## 4 10.9 10.4 1.0751 26 184.75 72.25 24.9 164.7 37.4 101.8 86.4 101.2 ## 5 27.8 28.7 1.0340 24 184.25 71.25 25.6 133.1 34.4 97.3 100.0 101.9 ## 6 20.6 20.9 1.0502 24 210.25 74.75 26.5 167.0 39.0 104.5 94.4 107.8 ## thigh knee ankle biceps forearm wrist ## 1 59.0 37.3 21.9 32.0 27.4 17.1 ## 2 58.7 37.3 23.4 30.5 28.9 18.2 ## 3 59.6 38.9 24.0 28.8 25.2 16.6 ## 4 60.1 37.3 22.8 32.4 29.4 18.2 ## 5 63.2 42.2 24.0 32.2 27.7 17.7 ## 6 66.0 42.0 25.6 35.7 30.6 18.8 # fit linear regression model model.all &lt;- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) # print model summary print(summary(model.all)) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + abdom + ## hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2664 -2.5658 -0.0798 2.8976 9.3204 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.063433 14.489336 -1.178 0.24011 ## age 0.056520 0.029888 1.891 0.05983 . ## weight -0.085513 0.045170 -1.893 0.05954 . ## height -0.059703 0.086695 -0.689 0.49171 ## neck -0.439315 0.214802 -2.045 0.04193 * ## abdom 0.875779 0.070589 12.407 &lt; 2e-16 *** ## hip -0.192118 0.132655 -1.448 0.14885 ## thigh 0.237304 0.131793 1.801 0.07303 . ## knee -0.006595 0.222832 -0.030 0.97642 ## ankle 0.164831 0.204681 0.805 0.42144 ## biceps 0.149530 0.157693 0.948 0.34397 ## forearm 0.424885 0.182801 2.324 0.02095 * ## wrist -1.474317 0.494475 -2.982 0.00316 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.98 on 239 degrees of freedom ## Multiple R-squared: 0.7489, Adjusted R-squared: 0.7363 ## F-statistic: 59.4 on 12 and 239 DF, p-value: &lt; 2.2e-16 # diagnostics plots par(mfrow=c(2,2)) plot(model.all) # remove potentially influential observations obs &lt;- c(86) fat2 &lt;- fat[-obs, ] # re-fit the model model.clean &lt;- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) # diagnostics plots par(mfrow=c(2,2)) plot(model.clean) # model summary print(summary(model.clean)) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + abdom + ## hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2664 -2.5658 -0.0798 2.8976 9.3204 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.063433 14.489336 -1.178 0.24011 ## age 0.056520 0.029888 1.891 0.05983 . ## weight -0.085513 0.045170 -1.893 0.05954 . ## height -0.059703 0.086695 -0.689 0.49171 ## neck -0.439315 0.214802 -2.045 0.04193 * ## abdom 0.875779 0.070589 12.407 &lt; 2e-16 *** ## hip -0.192118 0.132655 -1.448 0.14885 ## thigh 0.237304 0.131793 1.801 0.07303 . ## knee -0.006595 0.222832 -0.030 0.97642 ## ankle 0.164831 0.204681 0.805 0.42144 ## biceps 0.149530 0.157693 0.948 0.34397 ## forearm 0.424885 0.182801 2.324 0.02095 * ## wrist -1.474317 0.494475 -2.982 0.00316 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.98 on 239 degrees of freedom ## Multiple R-squared: 0.7489, Adjusted R-squared: 0.7363 ## F-statistic: 59.4 on 12 and 239 DF, p-value: &lt; 2.2e-16 # re-fit the model (no height) model.red1 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) print(summary(model.red1)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + ## knee + ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2830 -2.6162 -0.1017 2.8789 9.3713 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -22.66569 11.97691 -1.892 0.05963 . ## age 0.05948 0.02954 2.013 0.04521 * ## weight -0.09829 0.04114 -2.389 0.01765 * ## neck -0.43444 0.21445 -2.026 0.04389 * ## abdom 0.88762 0.06839 12.979 &lt; 2e-16 *** ## hip -0.17180 0.12919 -1.330 0.18483 ## thigh 0.25327 0.12960 1.954 0.05183 . ## knee -0.02318 0.22128 -0.105 0.91665 ## ankle 0.17300 0.20411 0.848 0.39752 ## biceps 0.15695 0.15715 0.999 0.31894 ## forearm 0.43091 0.18239 2.363 0.01895 * ## wrist -1.51011 0.49120 -3.074 0.00235 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.976 on 240 degrees of freedom ## Multiple R-squared: 0.7484, Adjusted R-squared: 0.7369 ## F-statistic: 64.9 on 11 and 240 DF, p-value: &lt; 2.2e-16 # re-fit the model (no knee) model.red2 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + ankle + biceps + forearm + wrist, data = fat) print(summary(model.red2)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + ## ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2552 -2.5979 -0.1133 2.8693 9.3584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -23.08716 11.25781 -2.051 0.04137 * ## age 0.05875 0.02864 2.051 0.04134 * ## weight -0.09965 0.03897 -2.557 0.01117 * ## neck -0.43088 0.21131 -2.039 0.04253 * ## abdom 0.88875 0.06740 13.186 &lt; 2e-16 *** ## hip -0.17231 0.12884 -1.337 0.18234 ## thigh 0.24942 0.12403 2.011 0.04544 * ## ankle 0.16946 0.20089 0.844 0.39974 ## biceps 0.15847 0.15616 1.015 0.31123 ## forearm 0.42946 0.18150 2.366 0.01876 * ## wrist -1.51470 0.48823 -3.102 0.00215 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.968 on 241 degrees of freedom ## Multiple R-squared: 0.7484, Adjusted R-squared: 0.738 ## F-statistic: 71.69 on 10 and 241 DF, p-value: &lt; 2.2e-16 # re-fit the model (no ankle) model.red3 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + biceps + forearm + wrist, data = fat) print(summary(model.red3)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + ## biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0740 -2.5615 -0.1021 2.7999 9.3199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -20.61247 10.86240 -1.898 0.0589 . ## age 0.05727 0.02857 2.004 0.0461 * ## weight -0.09141 0.03770 -2.424 0.0161 * ## neck -0.45458 0.20931 -2.172 0.0308 * ## abdom 0.88098 0.06673 13.203 &lt;2e-16 *** ## hip -0.17575 0.12870 -1.366 0.1733 ## thigh 0.25504 0.12378 2.061 0.0404 * ## biceps 0.15178 0.15587 0.974 0.3311 ## forearm 0.42805 0.18138 2.360 0.0191 * ## wrist -1.40948 0.47175 -2.988 0.0031 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.965 on 242 degrees of freedom ## Multiple R-squared: 0.7477, Adjusted R-squared: 0.7383 ## F-statistic: 79.67 on 9 and 242 DF, p-value: &lt; 2.2e-16 # re-fit the model (no biceps) model.red4 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + forearm + wrist, data = fat) print(summary(model.red4)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + ## forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0574 -2.7411 -0.1912 2.6929 9.4977 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -20.06213 10.84654 -1.850 0.06558 . ## age 0.05922 0.02850 2.078 0.03876 * ## weight -0.08414 0.03695 -2.277 0.02366 * ## neck -0.43189 0.20799 -2.077 0.03889 * ## abdom 0.87721 0.06661 13.170 &lt; 2e-16 *** ## hip -0.18641 0.12821 -1.454 0.14727 ## thigh 0.28644 0.11949 2.397 0.01727 * ## forearm 0.48255 0.17251 2.797 0.00557 ** ## wrist -1.40487 0.47167 -2.978 0.00319 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.965 on 243 degrees of freedom ## Multiple R-squared: 0.7467, Adjusted R-squared: 0.7383 ## F-statistic: 89.53 on 8 and 243 DF, p-value: &lt; 2.2e-16 # re-fit the model (no hip) model.red5 &lt;- lm(brozek ~ age + weight + neck + abdom + thigh + forearm + wrist, data = fat) print(summary(model.red5)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + ## wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0193 -2.8016 -0.1234 2.9387 9.0019 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -30.17420 8.34200 -3.617 0.000362 *** ## age 0.06149 0.02852 2.156 0.032047 * ## weight -0.11236 0.03151 -3.565 0.000437 *** ## neck -0.37203 0.20434 -1.821 0.069876 . ## abdom 0.85152 0.06437 13.229 &lt; 2e-16 *** ## thigh 0.20973 0.10745 1.952 0.052099 . ## forearm 0.51824 0.17115 3.028 0.002726 ** ## wrist -1.40081 0.47274 -2.963 0.003346 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.974 on 244 degrees of freedom ## Multiple R-squared: 0.7445, Adjusted R-squared: 0.7371 ## F-statistic: 101.6 on 7 and 244 DF, p-value: &lt; 2.2e-16 # compare model.clean and final model print(summary(model.clean)) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + abdom + ## hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2664 -2.5658 -0.0798 2.8976 9.3204 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.063433 14.489336 -1.178 0.24011 ## age 0.056520 0.029888 1.891 0.05983 . ## weight -0.085513 0.045170 -1.893 0.05954 . ## height -0.059703 0.086695 -0.689 0.49171 ## neck -0.439315 0.214802 -2.045 0.04193 * ## abdom 0.875779 0.070589 12.407 &lt; 2e-16 *** ## hip -0.192118 0.132655 -1.448 0.14885 ## thigh 0.237304 0.131793 1.801 0.07303 . ## knee -0.006595 0.222832 -0.030 0.97642 ## ankle 0.164831 0.204681 0.805 0.42144 ## biceps 0.149530 0.157693 0.948 0.34397 ## forearm 0.424885 0.182801 2.324 0.02095 * ## wrist -1.474317 0.494475 -2.982 0.00316 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.98 on 239 degrees of freedom ## Multiple R-squared: 0.7489, Adjusted R-squared: 0.7363 ## F-statistic: 59.4 on 12 and 239 DF, p-value: &lt; 2.2e-16 print(summary(model.red5)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + ## wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0193 -2.8016 -0.1234 2.9387 9.0019 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -30.17420 8.34200 -3.617 0.000362 *** ## age 0.06149 0.02852 2.156 0.032047 * ## weight -0.11236 0.03151 -3.565 0.000437 *** ## neck -0.37203 0.20434 -1.821 0.069876 . ## abdom 0.85152 0.06437 13.229 &lt; 2e-16 *** ## thigh 0.20973 0.10745 1.952 0.052099 . ## forearm 0.51824 0.17115 3.028 0.002726 ** ## wrist -1.40081 0.47274 -2.963 0.003346 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.974 on 244 degrees of freedom ## Multiple R-squared: 0.7445, Adjusted R-squared: 0.7371 ## F-statistic: 101.6 on 7 and 244 DF, p-value: &lt; 2.2e-16 Note: we have just run a very simple feature selection using stepwise regression. In this method, using backward elimination, we build a model containing all the variables and remove them one by one based on defined criteria (here we have used p-values) and we stop when we have a justifiable model or when removing a predictor does not change the chosen criterion significantly. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
