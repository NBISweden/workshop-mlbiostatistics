<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to linear models - 2&nbsp; Linear models: regression and classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./lm-coeff.html" rel="next">
<link href="./lm-intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./lm-reg-cls.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear models: regression and classification</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to linear models</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lm-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to linear models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lm-reg-cls.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear models: regression and classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lm-coeff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Common cases</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lm-intro-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises (introduction to linear models)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lm-diagn-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises (model diagnostics)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lm-lasso-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises (regularization)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lm-coeff-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises (regression coefficients)</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#linear-models-in-ml-context" id="toc-linear-models-in-ml-context" class="nav-link active" data-scroll-target="#linear-models-in-ml-context"><span class="header-section-number">2.1</span> Linear models in ML context</a></li>
  <li><a href="#evaluating-regression" id="toc-evaluating-regression" class="nav-link" data-scroll-target="#evaluating-regression"><span class="header-section-number">2.2</span> Evaluating regression</a>
  <ul class="collapse">
  <li><a href="#model-fit" id="toc-model-fit" class="nav-link" data-scroll-target="#model-fit">model fit</a></li>
  <li><a href="#predictions" id="toc-predictions" class="nav-link" data-scroll-target="#predictions">predictions</a></li>
  </ul></li>
  <li><a href="#feature-selection" id="toc-feature-selection" class="nav-link" data-scroll-target="#feature-selection"><span class="header-section-number">2.3</span> Feature selection</a></li>
  <li><a href="#regularized-regression" id="toc-regularized-regression" class="nav-link" data-scroll-target="#regularized-regression"><span class="header-section-number">2.4</span> Regularized regression</a></li>
  <li><a href="#bias-variance-trade-off" id="toc-bias-variance-trade-off" class="nav-link" data-scroll-target="#bias-variance-trade-off"><span class="header-section-number">2.5</span> Bias-variance trade-off</a></li>
  <li><a href="#ridge-lasso-and-elastic-nets" id="toc-ridge-lasso-and-elastic-nets" class="nav-link" data-scroll-target="#ridge-lasso-and-elastic-nets"><span class="header-section-number">2.6</span> Ridge, Lasso and Elastic Nets</a></li>
  <li><a href="#generalized-linear-models" id="toc-generalized-linear-models" class="nav-link" data-scroll-target="#generalized-linear-models"><span class="header-section-number">2.7</span> Generalized linear models</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">2.8</span> Logistic regression</a>
  <ul class="collapse">
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing">Hypothesis testing</a></li>
  <li><a href="#deviance" id="toc-deviance" class="nav-link" data-scroll-target="#deviance">Deviance</a></li>
  <li><a href="#odds-ratios" id="toc-odds-ratios" class="nav-link" data-scroll-target="#odds-ratios">Odds ratios</a></li>
  <li><a href="#other-covariates" id="toc-other-covariates" class="nav-link" data-scroll-target="#other-covariates">Other covariates</a></li>
  </ul></li>
  <li><a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression"><span class="header-section-number">2.9</span> Poisson regression</a></li>
  <li><a href="#logistic-lasso" id="toc-logistic-lasso" class="nav-link" data-scroll-target="#logistic-lasso"><span class="header-section-number">2.10</span> Logistic Lasso</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear models: regression and classification</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="linear-models-in-ml-context" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="linear-models-in-ml-context"><span class="header-section-number">2.1</span> Linear models in ML context</h2>
<p>We can think of linear models in machine learning context, as linear models are often used for both building both regression and classification machine learning models and used for predictions It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response, and there are <strong>feature selection</strong> approaches that enable us to exclude <strong>irrelevant</strong> variables and as a consequence improve prediction results.</p>
<ul>
<li>Indeed, it is known that including irrelevant variables leads to unnecessary complexity in the resulting model, and often worse prediction results.</li>
<li>There are few approaches to perform <strong>feature selection</strong> or <strong>variable selection</strong>, that is for excluding irrelevant variables from a multiple regression model.</li>
<li>Here, we can group the feature selection methods by differences classes such as: subset selection, <strong>Shrinkage methods</strong> and dimension reduction. Another classification is by filter methods, wrapper methods and embedded methods.</li>
<li>Before we can divide more into these methods, we need to discuss how to evaluate regression results. We will need a way of comparing the models and evaluating the predictions outcomes.</li>
</ul>
</section>
<section id="evaluating-regression" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="evaluating-regression"><span class="header-section-number">2.2</span> Evaluating regression</h2>
<ul>
<li>Regression models can be evaluated by assessing a model fit, something that we have seen previously with adjusted <span class="math inline">\(R^2\)</span>. Other metrics than can also be expressed in terms of RSS include <strong>Akaike information criterion (AIC)</strong> and <strong>Bayesian information criterion (BIC)</strong>.</li>
<li>Alternatively, by using data splitting strategies such as validation and cross-validation we can directly evaluate the prediction error. Here, we use metrics such as <strong>Mean Squared Error (MSE)</strong>, <strong>Root Mean Squared Error (RMSE)</strong>, <strong>Mean Absolute Error (MAE)</strong> or <strong>Mean Absolute Percentage Error (MAPE)</strong>-</li>
</ul>
<section id="model-fit" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="model-fit">model fit</h3>
<p><strong>Adjusted R-squared</strong> (as seen before) <span class="math display">\[
R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
\]</span></p>
<p><strong>AIC</strong> and <strong>BIC</strong></p>
<p>AIC is grounded in information theory and BIC is derived from a Bayesian point of view. Both are formally defined in likelihood functions. For regression models they can be expressed in terms of RSS because the likelihood of a model in the context of normal errors is directly related to the RSS.</p>
<p><span class="math display">\[\text{AIC} = n \ln(\text{RSS}/n) + 2p\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations.</li>
<li><span class="math inline">\(\text{RSS}\)</span> is the residual sum of squares.</li>
<li><span class="math inline">\(p\)</span> is the number of parameters in the model (including the intercept).</li>
</ul>
<p><span class="math display">\[\text{BIC} = n \ln(\text{RSS}/n) + p \ln(n)\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations.</li>
<li><span class="math inline">\(\text{RSS}\)</span> is the residual sum of squares.</li>
<li><span class="math inline">\(p\)</span> is the number of parameters in the model.</li>
</ul>
<p>Both criteria, AIC and BIC, introduce penalties for the number of parameters to avoid overfitting. BIC introduces a stronger penalty based on the sample size, making it more conservative than AIC. When comparing models using AIC or BIC that incorporate RSS, the objective remains the same: select the model that provides the best balance between goodness of fit and model simplicity. The model with the lower AIC or BIC value is generally preferred, as it indicates either a more parsimonious model or a model that better fits the data (or both).</p>
</section>
<section id="predictions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="predictions">predictions</h3>
<ul>
<li><strong>Mean Squared Error (MSE)</strong>: average squared difference between the predicted values and the actual values. <span class="math display">\[MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2\]</span></li>
<li><strong>Root Mean Squared Error (RMSE)</strong>: square root of the MSE <span class="math display">\[RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}\]</span></li>
<li><strong>MAE</strong>: average absolute difference between the predicted values and the actual values <span class="math display">\[MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|\]</span></li>
<li><strong>Mean Absolute Percentage Error (MAPE)</strong>: average percentage difference between the predicted values and the actual values.</li>
</ul>
<p>The smaller the difference between the predicted values vs.&nbsp;the validation or cross-validation predicted values, the better the model.</p>
</section>
</section>
<section id="feature-selection" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="feature-selection"><span class="header-section-number">2.3</span> Feature selection</h2>
<p>Feature selection is the process of selecting the most relevant and informative subset of features from a larger set of potential features in order to improve the performance and interpretability of a machine learning model. There are generally three main groups of feature selection methods:</p>
<ul>
<li><strong>Filter methods</strong> use statistical measures to score the features and select the most relevant ones, e.g.&nbsp;based on correlation coefficient or <span class="math inline">\(\chi^2\)</span> test. They tend to be computationally efficient but may overlook complex interactions between features and can be sensitive to the choice of metric used to evaluate the feature importance.</li>
<li><strong>Wrapper methods</strong> use a machine learning algorithm to evaluate the performance of different subsets of features, e.g.&nbsp;forward/backward feature selection. They tend to be computationally heavy.</li>
<li><strong>Embedded methods</strong> incorporate feature selection as part of the machine learning algorithm itself, e.g.&nbsp;<strong>regularized regression</strong> or <strong>Random Forest</strong>. These methods are computationally efficient and can be more accurate than filter methods.</li>
</ul>
</section>
<section id="regularized-regression" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="regularized-regression"><span class="header-section-number">2.4</span> Regularized regression</h2>
<p>Regularized regression expands on the regression by adding a penalty term or terms to shrink the model coefficients of less important features towards zero. This can help to prevent overfitting and improve the accuracy of the predictive model. Depending on the penalty added, we talk about <strong>Ridge</strong>, <strong>Lasso</strong> or <strong>Elastic Nets</strong> regression.</p>
<p>Previously when talking about regression, we saw that the least squares fitting procedure estimates model coefficients <span class="math inline">\(\beta_0, \beta_1, \cdots, \beta_p\)</span> using the values that minimize the residual sum of squares: <span id="eq-lm"><span class="math display">\[RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 \tag{2.1}\]</span></span></p>
<p>In <strong>regularized regression</strong> the coefficients are estimated by minimizing slightly different quantity. In <strong>Ridge regression</strong> we estimate <span class="math inline">\(\hat\beta^{L}\)</span> that minimizes <span id="eq-ridge"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \tag{2.2}\]</span></span></p>
<p>where:</p>
<p><span class="math inline">\(\lambda \ge 0\)</span> is a <strong>tuning parameter</strong> to be determined separately e.g.&nbsp;via cross-validation</p>
<p><a href="#eq-ridge">Equation&nbsp;<span>2.2</span></a> trades two different criteria:</p>
<ul>
<li>as with least squares, lasso regression seeks coefficient estimates that fit the data well, by making RSS small</li>
<li>however, the second term <span class="math inline">\(\lambda \sum_{j=1}^{p}\beta_j^2\)</span>, called <strong>shrinkage penalty</strong> is small when <span class="math inline">\(\beta_1, \cdots, \beta_p\)</span> are close to zero, so it has the effect of <strong>shrinking</strong> the estimates of <span class="math inline">\(\beta_j\)</span> towards zero.</li>
<li>the tuning parameter <span class="math inline">\(\lambda\)</span> controls the relative impact of these two terms on the regression coefficient estimates
<ul>
<li>when <span class="math inline">\(\lambda = 0\)</span>, the penalty term has no effect</li>
<li>as <span class="math inline">\(\lambda \rightarrow \infty\)</span> the impact of the shrinkage penalty grows and the ridge regression coefficient estimates approach zero</li>
</ul></li>
</ul>
<div class="cell" data-fig-cap-location="margin">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(latex2exp)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># select subset data</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and scale: since regression puts constraints on the size of the coefficient</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>data_input <span class="ot">&lt;-</span> data_diabetes <span class="sc">%&gt;%</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(BMI, chol, hdl, age, stab.glu) <span class="sc">%&gt;%</span> </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>() </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit ridge regression for a series of lambda values </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># note: lambda values were chosen by experimenting to show lambda effect on beta coefficient estimates</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(BMI <span class="sc">~</span>., <span class="at">data =</span> data_input)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> data_input <span class="sc">%&gt;%</span> <span class="fu">pull</span>(BMI)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha=</span><span class="dv">0</span>, <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">1</span>))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># plot beta estimates vs. lambda</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>betas <span class="ot">&lt;-</span> model<span class="sc">$</span>beta <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>data_plot <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="fu">data.frame</span>(<span class="at">lambda =</span> model<span class="sc">$</span>lambda, betas)) <span class="sc">%&gt;%</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="st">"X.Intercept."</span>) <span class="sc">%&gt;%</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>lambda, <span class="at">names_to =</span> <span class="st">"variable"</span>, <span class="at">values_to =</span> <span class="st">"beta"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>data_plot <span class="sc">%&gt;%</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> lambda, <span class="at">y =</span> beta, <span class="at">color =</span> variable)) <span class="sc">+</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">2</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span> </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">"$</span><span class="sc">\\</span><span class="st">lambda$"</span>)) <span class="sc">+</span> </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">TeX</span>(<span class="st">"Standardized coefficients"</span>)) <span class="sc">+</span> </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">"Set1"</span>) <span class="sc">+</span> </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>(), <span class="at">legend.position =</span> <span class="st">"top"</span>, <span class="at">legend.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">12</span>)) <span class="sc">+</span> </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lm-reg-cls_files/figure-html/ridge-run-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Example of Ridge regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero for larger lambda values.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="bias-variance-trade-off" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="bias-variance-trade-off"><span class="header-section-number">2.5</span> Bias-variance trade-off</h2>
<p>Ridge regression’s advantages over least squares estimates stems from <strong>bias-variance trade-off</strong>, another fundamental concept in machine learning.</p>
<ul>
<li>The bias-variance trade-off describes the relationship between model complexity, prediction accuracy, and the ability of the model to generalize to new data.</li>
<li><strong>Bias</strong> refers to the error that is introduced by approximating a real-life problem with a simplified model. A high bias model is one that makes overly simplistic assumptions about the underlying data, resulting in <em>under-fitting</em> and poor accuracy.</li>
<li><strong>Variance</strong> refers to the sensitivity of a model to fluctuations in the training data. A high variance model is one that is overly complex and captures noise in the training data, resulting in <em>overfitting</em> and poor generalization to new data.</li>
<li>The goal of machine learning is to find a model with <strong>the right balance between bias and variance</strong>, which can generalize well to new data.</li>
<li>The bias-variance trade-off can be visualized in terms of MSE, means squared error of the model. The <strong>MSE</strong> can be decomposed into: <span class="math display">\[MSE(\hat\beta) := bias^2(\hat\beta) + Var(\hat\beta) + noise\]</span></li>
<li>The irreducible error is the inherent noise in the data that cannot be reduced by any model, while the bias and variance terms can be reduced by choosing an appropriate model complexity. The trade-off lies in finding the right balance between bias and variance that minimizes the total MSE.</li>
<li>In practice, this trade-off can be addressed by <strong>regularizing the model</strong>, selecting an appropriate model complexity, or by using ensemble methods that combine multiple models to reduce the variance (e.g.&nbsp;Random Forest). Ultimately, the goal is to find a model that is both accurate and generalizing.</li>
</ul>
<div class="cell" data-layout-align="center" data-fig-cap-location="margin">
<div class="cell-output-display">
<div id="fig-bias-variance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/bias-variance.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2.1: Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. al, A Introduction to statistical learning</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="ridge-lasso-and-elastic-nets" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="ridge-lasso-and-elastic-nets"><span class="header-section-number">2.6</span> Ridge, Lasso and Elastic Nets</h2>
<p>In <strong>Ridge</strong> regression we minimize: <span id="eq-ridge2"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \tag{2.3}\]</span></span> where <span class="math inline">\(\lambda \sum_{j=1}^{p}\beta_j^2\)</span> is also known as <strong>L2</strong> regularization element or <span class="math inline">\(l_2\)</span> penalty</p>
<p>In <strong>Lasso</strong> regression, that is Least Absolute Shrinkage and Selection Operator regression we change penalty term to absolute value of the regression coefficients: <span id="eq-lasso"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| = RSS + \lambda \sum_{j=1}^{p}|\beta_j| \tag{2.4}\]</span></span> where <span class="math inline">\(\lambda \sum_{j=1}^{p}|\beta_j|\)</span> is also known as <strong>L1</strong> regularization element or <span class="math inline">\(l_1\)</span> penalty</p>
<p>Lasso regression was introduced to help model interpretation. With Ridge regression we improve model performance but unless <span class="math inline">\(\lambda = \infty\)</span> all beta coefficients are non-zero, hence all variables remain in the model. By using <span class="math inline">\(l_1\)</span> penalty we can force some of the coefficients estimates to be exactly equal to 0, hence perform <strong>variable selection</strong></p>
<div class="cell" data-fig-cap-location="margin">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(latex2exp)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># select subset data</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and scale: since regression puts constraints on the size of the coefficient</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>data_input <span class="ot">&lt;-</span> data_diabetes <span class="sc">%&gt;%</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(BMI, chol, hdl, age, stab.glu) <span class="sc">%&gt;%</span> </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>() </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit ridge regression for a series of lambda values </span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># note: lambda values were chosen by experimenting to show lambda effect on beta coefficient estimates</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(BMI <span class="sc">~</span>., <span class="at">data =</span> data_input)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> data_input <span class="sc">%&gt;%</span> <span class="fu">pull</span>(BMI)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha=</span><span class="dv">1</span>, <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="fl">0.1</span>))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># plot beta estimates vs. lambda</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>betas <span class="ot">&lt;-</span> model<span class="sc">$</span>beta <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>data_plot <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="fu">data.frame</span>(<span class="at">lambda =</span> model<span class="sc">$</span>lambda, betas)) <span class="sc">%&gt;%</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="st">"X.Intercept."</span>) <span class="sc">%&gt;%</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>lambda, <span class="at">names_to =</span> <span class="st">"variable"</span>, <span class="at">values_to =</span> <span class="st">"beta"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>data_plot <span class="sc">%&gt;%</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> lambda, <span class="at">y =</span> beta, <span class="at">color =</span> variable)) <span class="sc">+</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">2</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span> </span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">"$</span><span class="sc">\\</span><span class="st">lambda$"</span>)) <span class="sc">+</span> </span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">TeX</span>(<span class="st">"Standardized coefficients"</span>)) <span class="sc">+</span> </span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">"Set1"</span>) <span class="sc">+</span> </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>(), <span class="at">legend.position =</span> <span class="st">"top"</span>, <span class="at">legend.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">12</span>)) <span class="sc">+</span> </span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lm-reg-cls_files/figure-html/lasso-run-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Example of Lasso regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero and are also set to zero for larger lambda values.</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Elastic Net</strong> use both L1 and L2 penalties to try to find middle grounds by performing parameter shrinkage and variable selection. <span id="eq-elastic-net"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2  \tag{2.5}\]</span></span></p>
<p>In the <code>glmnet</code> library we can fit Elastic Net by setting parameters <span class="math inline">\(\alpha\)</span>. Actually, under the hood <code>glmnet</code> minimizes a cost function: <span class="math display">\[\sum_{i_=1}^{n}(y_i-\hat y_i)^2 + \lambda \left ( (1-\alpha) \sum_{j=1}^{p}\beta_j^2 + \alpha \sum_{j=1}^{p}|\beta_j|\right )\]</span> where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of samples</li>
<li><span class="math inline">\(p\)</span> is the number of parameters</li>
<li><span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\alpha\)</span> hyperparameters control the shrinkage</li>
</ul>
<p>When <span class="math inline">\(\alpha = 0\)</span> this corresponds to Ridge regression and when <span class="math inline">\(\alpha=1\)</span> this corresponds to Lasso regression. A value of <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span> gives us <strong>Elastic Net regularization</strong>, combining both L1 and L2 regularization terms.</p>
<div class="cell" data-fig-cap-location="margin">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(latex2exp)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># select subset data</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and scale: since regression puts constraints on the size of the coefficient</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>data_input <span class="ot">&lt;-</span> data_diabetes <span class="sc">%&gt;%</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(BMI, chol, hdl, age, stab.glu) <span class="sc">%&gt;%</span> </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>() </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit ridge regression for a series of lambda values </span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># note: lambda values were chosen by experimenting to show lambda effect on beta coefficient estimates</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(BMI <span class="sc">~</span>., <span class="at">data =</span> data_input)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> data_input <span class="sc">%&gt;%</span> <span class="fu">pull</span>(BMI)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha=</span><span class="fl">0.1</span>, <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="fl">0.05</span>))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># plot beta estimates vs. lambda</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>betas <span class="ot">&lt;-</span> model<span class="sc">$</span>beta <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>data_plot <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="fu">data.frame</span>(<span class="at">lambda =</span> model<span class="sc">$</span>lambda, betas)) <span class="sc">%&gt;%</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="st">"X.Intercept."</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>lambda, <span class="at">names_to =</span> <span class="st">"variable"</span>, <span class="at">values_to =</span> <span class="st">"beta"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>data_plot <span class="sc">%&gt;%</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> lambda, <span class="at">y =</span> beta, <span class="at">color =</span> variable)) <span class="sc">+</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">2</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span> </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">"$</span><span class="sc">\\</span><span class="st">lambda$"</span>)) <span class="sc">+</span> </span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">TeX</span>(<span class="st">"Standardized coefficients"</span>)) <span class="sc">+</span> </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">"Set1"</span>) <span class="sc">+</span> </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>(), <span class="at">legend.position =</span> <span class="st">"top"</span>, <span class="at">legend.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">12</span>)) <span class="sc">+</span> </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lm-reg-cls_files/figure-html/elastic-net-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Example of Elastic Net regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values and alpha value 0.1, showing the changes of model coefficients as a function of lambda being somewhere between those for Ridge and Lasso regression.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="generalized-linear-models" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="generalized-linear-models"><span class="header-section-number">2.7</span> Generalized linear models</h2>
<ul>
<li>GLMs extend linear model framework to outcome variables that do not follow normal distribution.</li>
<li>They are most frequently used to model binary, categorical or count data.</li>
<li>For instance, fitting a regression line to binary data yields predicted values that could take any value, including <span class="math inline">\(&lt;0\)</span>,</li>
<li>not to mention that it is hard to argue that the values of 0 and 1s are normally distributed.</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>data_diabetes <span class="sc">%&gt;%</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">obese =</span> <span class="fu">as.numeric</span>(obese) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y=</span>obese, <span class="at">x=</span>waist)) <span class="sc">+</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">width=</span><span class="dv">0</span>, <span class="at">height =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">"lm"</span>, <span class="at">se=</span><span class="cn">FALSE</span>, <span class="at">color=</span>col.blue.dark) <span class="sc">+</span> </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  my.ggtheme</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-log-example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="lm-reg-cls_files/figure-html/fig-log-example-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption class="figure-caption">Figure&nbsp;2.2: Example of fitting linear model to binary data, to model obesity status coded as 1 (Yes) and 0 (No) with waist variable. Linear model does not fit the data well in this case and the predicted values can take any value, not only 0 and 1.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="logistic-regression" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">2.8</span> Logistic regression</h2>
<p>Let’s look again at the binary <code>obesity</code> status data and try to fit logistic regression model using <code>waist</code> as explanatory variable instead of fitting inappropriate here simple linear model.</p>
<div class="cell" data-fig-cap-location="margin">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> data_diabetes <span class="sc">%&gt;%</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">obese =</span> <span class="fu">as.numeric</span>(obese) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y=</span>obese, <span class="at">x=</span>waist)) <span class="sc">+</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">width=</span><span class="dv">0</span>, <span class="at">height =</span> <span class="fl">0.05</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  my.ggtheme</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> data_diabetes <span class="sc">%&gt;%</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> obese, <span class="at">y =</span> waist, <span class="at">fill =</span> obese)) <span class="sc">+</span> </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">"Set2"</span>) <span class="sc">+</span> </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  my.ggtheme</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-obesity" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="lm-reg-cls_files/figure-html/fig-obesity-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.3: Obesity status data: jittered plot (left) and box plot of waist stratified by obesity status for the 130 study participants.</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li>Since the response variable takes only two values (Yes/No) we use GLM model</li>
<li>to fit <strong>logistic regression</strong> model for the <strong>probability of suffering from obesity (Yes).</strong></li>
<li>We let <span class="math inline">\(p_i=P(Y_i=1)\)</span> denote the probability of suffering from obesity (success)</li>
<li>and we assume that the response follows binomial distribution: <span class="math inline">\(Y_i \sim Bi(1, p_i)\)</span> distribution.</li>
<li>We can then write the regression model now as: <span class="math display">\[log(\frac{p_i}{1-p_i})=\beta_0 + \beta_1x_i\]</span> and given the properties of logarithms this is also equivalent to: <span class="math display">\[p_i = \frac{exp(\beta_0 + \beta_1x_i)}{1 + exp(\beta_0 + \beta_1x_i)}\]</span></li>
</ul>
<p><br></p>
<ul>
<li>In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a <strong>link function</strong>.</li>
<li>Here, the <strong>link function</strong> <span class="math inline">\(log(\frac{p_i}{1-p_i})\)</span> provides the link between the binomial distribution of <span class="math inline">\(Y_i\)</span> (suffering from obesity) and the linear predictor (waist)</li>
<li>Thus the <strong>GLM model</strong> can be written as <span class="math display">\[g(\mu_i)=\mathbf{X}\boldsymbol\beta\]</span> where <code>g()</code> is the link function.</li>
</ul>
<p><br></p>
<p>In R we can use <code>glm()</code> function to fit GLM models:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># re-code obese status from Yes/No to 1/0</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>data_diabetes <span class="ot">&lt;-</span> </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  data_diabetes <span class="sc">%&gt;%</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">obese =</span> <span class="fu">as.numeric</span>(obese) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># fit logistic regression model</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>logmodel_1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(obese <span class="sc">~</span> waist, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span><span class="st">"logit"</span>), <span class="at">data =</span> data_diabetes)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print model summary</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(logmodel_1))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = obese ~ waist, family = binomial(link = "logit"), </span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="do">##     data = data_diabetes)</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  -17.357      2.973  -5.837 5.30e-09 ***</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="do">## waist         17.174      2.974   5.775 7.71e-09 ***</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 178.71  on 129  degrees of freedom</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 102.79  on 128  degrees of freedom</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 106.79</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 5</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="fu">ggPredict</span>(logmodel_1) <span class="sc">+</span> </span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>  my.ggtheme</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lm-reg-cls_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Fitted logistic model to diabetes data given the 130 study participants and using waist as explantatory variable to model obesity status.</figcaption>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to get predictions use predict() functions</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># if no new observations is specified predictions are returned for the values of exploratory variables used</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># we specify response to return prediction on the probability scale</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>obese_predicted <span class="ot">&lt;-</span> <span class="fu">predict</span>(logmodel_1, <span class="at">type=</span><span class="st">"response"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(obese_predicted))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="do">##          3          7          9         11         16         21 </span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.98231495 0.93752915 0.07405337 0.41460606 0.22839680 0.72385967</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>The regression equation for the fitted model is: <span class="math display">\[log(\frac{\hat{p_i}}{1-\hat{p_i}})=-17.357  +  17.174\cdot x_i\]</span></li>
<li>We see from the output that <span class="math inline">\(\hat{\beta_0} = -17.357\)</span> and <span class="math inline">\(\hat{\beta_1} = 17.174\)</span>.</li>
<li>These estimates are arrived at via <strong>maximum likelihood estimation</strong>, something that is out of scope here.</li>
</ul>
<section id="hypothesis-testing" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="hypothesis-testing">Hypothesis testing</h3>
<ul>
<li>Similarly to linear models, we can determine whether each variable is related to the outcome of interest by testing the null hypothesis that the relevant logistic regression coefficient is zero.</li>
<li>This can be performed by <strong>Wald test</strong> which is equals to estimated logistic regression coefficient divided by its standard error and follows the Standard Normal distribution: <span class="math display">\[W = \frac{\hat\beta-\beta}{e.s.e.(\hat\beta)} \sim N(0,1)\]</span>.</li>
<li>Alternatively, its square approximates the Chi-squared distribution with 1 degree of freedom: <span class="math display">\[W^2 = \frac{(\hat\beta-\beta)^2}{\hat {var}(\hat\beta)} \sim \chi_1^2\]</span>.</li>
</ul>
<p>In our example, we can check whether <code>waist</code> is associated with <code>obesity status</code> by testing null hypothesis: <span class="math inline">\(H_0:\beta_1=0\)</span>. We calculate Wald statistics as <span class="math inline">\(W^2 = \frac{(\hat\beta-0)}{\hat {e.s.e}(\hat\beta)} = \frac{17.174}{2.974} = 5.774714\)</span> and we can find the corresponding p-value using standard normal distribution:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pnorm</span>(<span class="fl">5.774714</span>, <span class="at">lower.tail =</span> F)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 7.70839e-09</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>which confirms the summary output shown previously above and shows that there is enough evidence to reject the null hypothesis at 5% significance level <span class="math inline">\(p-value &lt;&lt; 0.05\)</span> and conclude that there is a significant association between <code>waist</code> and <code>obesity status</code>.</p>
</section>
<section id="deviance" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="deviance">Deviance</h3>
<ul>
<li>Deviance is the number that measures the goodness of fit of a logistic regression model.</li>
<li>We use saturated and residual deviance to assess model, instead of <span class="math inline">\(R^2\)</span> or <span class="math inline">\(R^2(adj)\)</span>.</li>
<li>We can also use deviance to check the association between explanatory variable and the outcome, an alternative and slightly more powerful test than Wald test (although these two test give similar results when sample size is large).</li>
<li>In the <strong>likelihood ratio test</strong> the test statistics is the <strong>deviance</strong> for the full model minus the deviance for the full model excluding the relevant explanatory variable. This test statistics follow a Chi-squared distribution with 1 degree of freedom.</li>
</ul>
<p><br></p>
<p>For instance, given our model we have null deviance of 274.4 and residual deviance of 268.7. The difference 5.7 is larger than than 95th percentile of <span class="math inline">\(\chi^2(129-128)\)</span> = 3.841459, where 129 is degrees of freedom for null model and 128 is degrees of freedom for null model excluding the <code>waist</code> explanatory variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="at">df=</span><span class="dv">1</span>, <span class="at">p=</span><span class="fl">0.95</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3.841459</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Again <span class="math inline">\(5.7 &gt; 3.84\)</span> and we can conclude that <code>waist</code> is a significant term in the model.</li>
</ul>
</section>
<section id="odds-ratios" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="odds-ratios">Odds ratios</h3>
<ul>
<li>In logistic regression we often interpret the model coefficients by taking <span class="math inline">\(e^{\hat{\beta}}\)</span></li>
<li>and we talk about <strong>odd ratios</strong>.</li>
<li>For instance we can say, given our above model, <span class="math inline">\(e^{17.174} = 28745736\)</span> that for each unit increase in <code>waist</code> the odds of suffering from obesity get multiplied by 28745736.</li>
</ul>
<p>These odds ratios are very high as here we are modeling obesity status with waist measurements, and increasing one unit in waist would mean adding up additional 1m to waist measurements. Typically:</p>
<ul>
<li>Odd ratios of 1.0 (or close to 1.0) indicates that exposure is not associated with the outcome.</li>
<li>Odds ratios <span class="math inline">\(&gt; 1.0\)</span> indicates that the odds of exposure among cases are greater than the odds of exposure among controls.</li>
<li>Odds raitos <span class="math inline">\(&lt; 1.0\)</span> indicates that the odds of exposure among cases are lower than the odds of exposure among controls.</li>
<li>The magnitude of the odds ratio is called the <strong>strength of the association</strong>. The further away an odds ratio is from 1.0, the more likely it is that the relationship between the exposure and the disease is causal. For example, an odds ratio of 1.2 is above 1.0, but is not a strong association. An odds ratio of 10 suggests a stronger association.</li>
</ul>
</section>
<section id="other-covariates" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="other-covariates">Other covariates</h3>
<ul>
<li>We can use the same logic as in multiple regression to expand by models by additional variables, numerical, binary or categorical.</li>
<li>For instance, we can test whether there is a gender effect when suffering from obesity:</li>
</ul>
<div class="cell" data-fig-cap-location="margin">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit logistic regression including age and gender</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>logmodel_2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(obese <span class="sc">~</span> waist <span class="sc">+</span> gender, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span><span class="st">"logit"</span>), <span class="at">data =</span> data_diabetes)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print model summary</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(logmodel_2))</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = obese ~ waist + gender, family = binomial(link = "logit"), </span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="do">##     data = data_diabetes)</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  -18.2756     3.1077  -5.881 4.08e-09 ***</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="do">## waist         17.4401     3.0523   5.714 1.11e-08 ***</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="do">## genderfemale   1.2335     0.5228   2.359   0.0183 *  </span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 178.708  on 129  degrees of freedom</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance:  96.877  on 127  degrees of freedom</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 102.88</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 6</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co"># plot model</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="fu">ggPredict</span>(logmodel_2) <span class="sc">+</span> </span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>  my.ggtheme <span class="sc">+</span> </span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">"Set2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lm-reg-cls_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Obesity status model with logistic regression given waist and gender exploratory variables. There is some seperation between the fitted lines for men and women and the model summary shows that there is enough evidence to reject a null hypothesis o no association between gender and obesity status.</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="poisson-regression" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="poisson-regression"><span class="header-section-number">2.9</span> Poisson regression</h2>
<ul>
<li>GLMs can be also applied to count data</li>
<li>For instance to model hospital admissions due to respiratory disease or number of bird nests in a certain habitat.</li>
<li>Here, we commonly assume that data follow the Poisson distribution <span class="math inline">\(Y_i \sim Pois(\mu_i)\)</span></li>
<li>and the corresponding model is: <span class="math display">\[E(Y_i)=\mu_i = \eta_ie^{\mathbf{x_i}^T\boldsymbol\beta}\]</span> with a log link <span class="math inline">\(\ln\mu_i = \ln \eta_i + \mathbf{x_i}^T\boldsymbol\beta\)</span></li>
<li>Hypothesis testing and assessing model fit follows the same logic as in logistic regression.</li>
</ul>
<div id="exm-poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Number of cancer cases) </strong></span>Suppose we wish to model <span class="math inline">\(Y_i\)</span> the number of cancer cases in the i-th intermediate geographical location (IG) in Glasgow. We have collected data for 271 small regions with between 2500 and 6000 people living in them. Together with cancer occurrence with have the following data:</p>
<ul>
<li>Y_all: number of cases of all types of cancer in the IG in 2013</li>
<li>E_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013</li>
<li>pm10: air pollution</li>
<li>smoke: percentage of people in an area that smoke</li>
<li>ethnic: percentage of people who are non-white</li>
<li>log.price: natural log of average house price</li>
<li>easting and northing: co-ordinates of the central point of the IG divided by 10000</li>
</ul>
<p>We can model the <strong>rate of occurrence of cancer</strong> using the very same <code>glm</code> function:¨ - now we use <strong>poisson family distribution</strong> to model counts - and we will include an <strong>offset term</strong> to the model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions.</p>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in and preview data</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>cancer <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"data/lm/cancer.csv"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(cancer)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="do">##          IG Y_all     E_all pm10 smoke ethnic log.price  easting northing</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 S02000260   133 106.17907 17.8  21.9   5.58  11.59910 26.16245 66.96574</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 S02000261    38  62.43131 18.6  21.8   7.91  11.84940 26.29271 67.00278</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 S02000262    97 120.00694 18.6  20.8   9.58  11.74106 26.21429 67.04280</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 S02000263    80 109.10245 17.0  14.0  10.39  12.30138 25.45705 67.05938</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 5 S02000264   181 149.77821 18.6  15.2   5.67  11.88449 26.12484 67.09280</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 6 S02000265    77  82.31156 17.0  14.6   5.61  11.82004 25.37644 67.09826</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># fit Poisson regression</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>epid1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y_all <span class="sc">~</span> pm10 <span class="sc">+</span> smoke <span class="sc">+</span> ethnic <span class="sc">+</span> log.price <span class="sc">+</span> easting <span class="sc">+</span> northing <span class="sc">+</span> <span class="fu">offset</span>(<span class="fu">log</span>(E_all)), </span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>             <span class="at">family =</span> poisson, </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> cancer)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(epid1))</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + </span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="do">##     northing + offset(log(E_all)), family = poisson, data = cancer)</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) -0.8592657  0.8029040  -1.070 0.284531    </span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="do">## pm10         0.0500269  0.0066724   7.498 6.50e-14 ***</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="do">## smoke        0.0033516  0.0009463   3.542 0.000397 ***</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="do">## ethnic      -0.0049388  0.0006354  -7.773 7.66e-15 ***</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="do">## log.price   -0.1034461  0.0169943  -6.087 1.15e-09 ***</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="do">## easting     -0.0331305  0.0103698  -3.195 0.001399 ** </span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="do">## northing     0.0300213  0.0111013   2.704 0.006845 ** </span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for poisson family taken to be 1)</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 972.94  on 270  degrees of freedom</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 565.18  on 264  degrees of freedom</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 2356.2</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Rate ratio</strong></p>
<ul>
<li>Similarly to logistic regression, it is common to look at the <span class="math inline">\(e^\beta\)</span>.</li>
<li>For instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient.</li>
<li>The ppm10 coefficient is positive, 0.0500269, indicating that cancer incidence rate increases with increased air pollution.</li>
<li>The rate ratio allows us to quantify by how much, here by a factor of <span class="math inline">\(e^{0.0500269} = 1.05\)</span>.</li>
</ul>
</section>
<section id="logistic-lasso" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="logistic-lasso"><span class="header-section-number">2.10</span> Logistic Lasso</h2>
<ul>
<li>Logistic Lasso combines logistic regression with Lasso regularization to analyze binary outcome data while simultaneously performing variable selection and regularization.</li>
<li>The equation for Logistic Lasso combines logistic regression with Lasso regularization. We estimate set of coefficients <span class="math inline">\(\hat \beta\)</span> that minimize the combined logistic loss function and the Lasso penalty:</li>
</ul>
<p><span class="math display">\[
\left( \sum_{i=1}^n [y_i \log(p_i) + (1-y_i) \log(1-p_i)] \right) + \lambda \sum_{j=1}^p |\beta_j|
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> represents the binary outcome (0 or 1) for the ( i )-th observation.</li>
<li><span class="math inline">\(p_i\)</span> is the predicted probability of <span class="math inline">\(y_i = 1\)</span> given by the logistic model <span class="math inline">\(p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip})}}\)</span>.</li>
<li><span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> are the coefficients of the model, including the intercept <span class="math inline">\(\beta_0\)</span>.</li>
<li><span class="math inline">\(x_{i1}, \ldots, x_{ip}\)</span> are the predictor variables for the <span class="math inline">\(i\)</span>-th observation.</li>
<li><span class="math inline">\(\lambda\)</span> is the regularization parameter that controls the strength of the Lasso penalty <span class="math inline">\(\lambda \sum_{j=1}^p |\beta_j|\)</span>, which encourages sparsity in the coefficients <span class="math inline">\(\beta_j\)</span> by shrinking some of them to zero.</li>
<li><span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(p\)</span> is the number of predictors.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./lm-intro.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to linear models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./lm-coeff.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Common cases</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>