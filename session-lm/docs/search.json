[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to linear models",
    "section": "",
    "text": "Preface\nLinear models allows us to answer questions such as:\n\nis there a relationship between exposure and outcome, e.g. height and weight?\nhow strong is the relationship between the two variables?\nwhat will be a predicted value of the outcome given a new set of exposure values?\nhow accurately can we predict outcome?\nwhich variables are associated with the response, e.g. is it only height that explains weight or could it be height and age that are both associated with the response?\n\nLearning outcomes\n\nto understand what a linear model is and be familiar with the terminology\nto be able to state linear model in the general vector-matrix notation\nto be able to use the general vector-matrix notation to numerically estimate model parameters\nto be able to use lm() function for model fitting, parameter estimation, hypothesis testing and prediction\nto be able to evaluate model fit by interpreting \\(R^2\\) and \\(R^2(adj)\\) values\nto be able to check model assumptions\nto be able to use glm() for extending linear models into generalized linear models\n\nDo you see a mistake or a typo? I would be grateful if you let me know via olga.dethlefsen@nbis.se\nThis repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/"
  },
  {
    "objectID": "lm-intro.html#why-linear-models",
    "href": "lm-intro.html#why-linear-models",
    "title": "1  Introduction to linear models",
    "section": "1.1 Why linear models?",
    "text": "1.1 Why linear models?\nWith linear models we can answer questions such as:\n\nis there a relationship between exposure and outcome, e.g. height and weight?\nhow strong is the relationship between the two variables?\nwhat will be a predicted value of the outcome given a new set of exposure values?\nhow accurately can we predict outcome?\nwhich variables are associated with the response, e.g. is it only height that explains weight or could it be height and age that are both associated with the response?\n\n\n\nCode\ndata_diabetes %&gt;%\n  ggplot(aes(x = height, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  my.ggtheme + \n  xlab(\"height [m]\") + \n  ylab(\"weight [kg]\")\n\n\n\n\n\nFigure 1.1: Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA."
  },
  {
    "objectID": "lm-intro.html#statistical-vs.-deterministic-relationship",
    "href": "lm-intro.html#statistical-vs.-deterministic-relationship",
    "title": "1  Introduction to linear models",
    "section": "1.2 Statistical vs. deterministic relationship",
    "text": "1.2 Statistical vs. deterministic relationship\nRelationships in probability and statistics can generally be one of three things: deterministic, random, or statistical:\n\na deterministic relationship involves an exact relationship between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation \\(Fahrenheit=\\frac{9}{5}\\cdot Celcius+32\\)\nthere is no relationship between variables in the random relationship, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year\na statistical relationship is a mixture of deterministic and random relationship, e.g. the savings that Olga has left in the bank account depend on Olga’s monthly salary income (deterministic part) and the money spent on buying succulents (random part)\n\n\n\n\n\n\nFigure 1.2: Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between \\(x\\) and \\(y\\) is not perfect (increasing relationship), c) statistical relationship between \\(x\\) and \\(y\\) is not perfect (decreasing relationship), d) random signal"
  },
  {
    "objectID": "lm-intro.html#what-linear-models-are-and-are-not",
    "href": "lm-intro.html#what-linear-models-are-and-are-not",
    "title": "1  Introduction to linear models",
    "section": "1.3 What linear models are and are not",
    "text": "1.3 What linear models are and are not\n\nIn an linear model we model (explain) the relationship between a single continuous variable \\(Y\\) and one or more variables \\(X\\). The \\(X\\) variables can be numerical, categorical or a mixture of both.\nOne very general form for the model would be: \\[Y = f(X_1, X_2, \\dots X_p) + \\epsilon\\] where \\(f\\) is some unknown function and \\(\\epsilon\\) is the error in this representation.\nFor instance a simple linear regression through the origin is a simple linear model of the form \\[Y_i = \\beta \\cdot x + \\epsilon\\] often used to express a relationship of one numerical variable to another, e.g. the calories burnt and the kilometers cycled.\nLinear models can become quite advanced by including many variables, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the transformation of the variables, e.g. a function of kilometers cycled squared\nFormally, linear models are a way of describing a response variable in terms of linear combination of predictor variables, i.e. expression constructed from a a set of terms by multiplying each term by a constant and/or adding the results.\nFor instance these are all models that can be constructed using linear combinations of predictors:\n\n\n\n\n\n\nFigure 1.3: Examples of a linear models: A) \\(y_i = x_1 + e_i\\), B) \\(x_1 + I_{x_i} + e_i\\) C) \\(y_i = x_i^2 + e_i\\), D) \\(y_i = x + x_i^3 + e_i\\) showing that linear models can get more complex and/or capture more than a straight line relationship.\n\n\n\n\n\n\\(Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i\\)\n\\(Y_i = \\alpha + \\beta x_i^2 \\epsilon\\)\n\\(Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon\\)\n\\(Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon\\)\n\nvs. an example of a non-linear model where parameter \\(\\beta\\) appears in the exponent of \\(x_i\\)\n\n\\(Y_i = \\alpha + x_i^\\beta + \\epsilon\\)"
  },
  {
    "objectID": "lm-intro.html#terminology",
    "href": "lm-intro.html#terminology",
    "title": "1  Introduction to linear models",
    "section": "1.4 Terminology",
    "text": "1.4 Terminology\nThere are many terms and notations used interchangeably:\n\n\\(y\\) is being called:\n\nresponse\noutcome\ndependent variable\n\n\\(x\\) is being called:\n\nexposure\nexplanatory variable\ndependent variable\npredictor\ncovariate"
  },
  {
    "objectID": "lm-intro.html#simple-linear-regression",
    "href": "lm-intro.html#simple-linear-regression",
    "title": "1  Introduction to linear models",
    "section": "1.5 Simple linear regression",
    "text": "1.5 Simple linear regression\n\nIt is used to check the association between the numerical outcome and one numerical explanatory variable\nIn practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n\n\nExample 1.1 (Weight and plasma volume) Let’s look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\nExample data:\n\nweight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n\n\n\n\n\n\nFigure 1.4: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca.\n\n\n\n\n\n\n\n\n\nFigure 1.5: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable\n\n\n\n\nThe equation for the red line is: \\[Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8\\] and in general: \\[Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n\\]\n\nIn other words, by finding the best-fitting straight line we are building a statistical model to represent the relationship between plasma volume (\\(Y\\)) and explanatory body weight variable (\\(x\\))\nIf we were to use our model \\(Y_i=0.086 + 0.044 \\cdot x_i\\) to find plasma volume given a weight of 58 kg (our first observation, \\(i=1\\)), we would notice that we would get \\(Y=0.086 + 0.044 \\cdot 58 = 2.638\\), not exactly \\(2.75\\) as we have for our first man in our dataset that we started with, i.e. \\(2.75 - 2.638 = 0.112 \\neq 0\\).\nWe thus add to the above equation an error term to account for this and now we can write our simple regression model more formally as:\n\n\\[Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i \\tag{1.1}\\] where:\n\n\\(x\\): is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n\\(y\\): is called: response, outcome, dependent variable\n\\(\\alpha\\) and \\(\\beta\\) are model coefficients\nand \\(\\epsilon_i\\) is an error terms"
  },
  {
    "objectID": "lm-intro.html#least-squares",
    "href": "lm-intro.html#least-squares",
    "title": "1  Introduction to linear models",
    "section": "1.6 Least squares",
    "text": "1.6 Least squares\n\nin the above “body weight - plasma volume” example, the values of \\(\\alpha\\) and \\(\\beta\\) have just appeared\nin practice, \\(\\alpha\\) and \\(\\beta\\) values are unknown and we use data to estimate these coefficients, noting the estimates with a hat, \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\nleast squares is one of the methods of parameters estimation, i.e. finding \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\n\n\n\n\n\n\nFigure 1.6: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line\n\n\n\n\n\nLet \\(\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i\\) be the prediction \\(y_i\\) based on the \\(i\\)-th value of \\(x\\):\n\nThen \\(\\epsilon_i = y_i - \\hat{y_i}\\) represents the \\(i\\)-th residual, i.e. the difference between the \\(i\\)-th observed response value and the \\(i\\)-th response value that is predicted by the linear model\nRSS, the residual sum of squares is defined as: \\[RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2\\] or equivalently as: \\[RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2\\]\nthe least squares approach chooses \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) to minimize the RSS. With some calculus, a good video explanation for the interested ones is here, we get Theorem 1.1\n\n\nTheorem 1.1 (Least squares estimates for a simple linear regression) \\[\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}\\] \\[\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}\\]\nwhere:\n\n\\(\\bar{x}\\): mean value of \\(x\\)\n\\(\\bar{y}\\): mean value of \\(y\\)\n\\(S_{xx}\\): sum of squares of \\(X\\) defined as \\(S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2\\)\n\\(S_{yy}\\): sum of squares of \\(Y\\) defined as \\(S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2\\)\n\\(S_{xy}\\): sum of products of \\(X\\) and \\(Y\\) defined as \\(S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})\\)\n\n\n\n\n\n\n\nExample 1.2 (Least squares) Let’s try least squares method to find coefficient estimates in the “body weight and plasma volume example”\n\n# initial data\nweight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# rename variables for convenience\nx &lt;- weight\ny &lt;- plasma\n\n# mean values of x and y\nx.bar &lt;- mean(x)\ny.bar &lt;- mean(y)\n\n# Sum of squares\nSxx &lt;-  sum((x - x.bar)^2)\nSxy &lt;- sum((x-x.bar)*(y-y.bar))\n\n# Coefficient estimates\nbeta.hat &lt;- Sxy / Sxx\nalpha.hat &lt;- y.bar - Sxy/Sxx*x.bar\n\n# Print estimated coefficients alpha and beta\nprint(alpha.hat)\n\n[1] 0.08572428\n\nprint(beta.hat)\n\n[1] 0.04361534\n\n\n\nIn R we can use lm(), the built-in function, to fit a linear regression model and we can replace the above code with one line\n\nlm(plasma ~ weight)\n\n\nCall:\nlm(formula = plasma ~ weight)\n\nCoefficients:\n(Intercept)       weight  \n    0.08572      0.04362"
  },
  {
    "objectID": "lm-intro.html#intercept-and-slope",
    "href": "lm-intro.html#intercept-and-slope",
    "title": "1  Introduction to linear models",
    "section": "1.7 Intercept and Slope",
    "text": "1.7 Intercept and Slope\n\nLinear regression gives us estimates of model coefficient \\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\)\n\\(\\alpha\\) is known as the intercept\n\\(\\beta\\) is known as the slope\n\n\n\n\n\n\nFigure 1.7: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)"
  },
  {
    "objectID": "lm-intro.html#hypothesis-testing",
    "href": "lm-intro.html#hypothesis-testing",
    "title": "1  Introduction to linear models",
    "section": "1.8 Hypothesis testing",
    "text": "1.8 Hypothesis testing\nIs there a relationship between the response and the predictor?\n\nthe calculated \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are estimates of the population values of the intercept and slope and are therefore subject to sampling variation\ntheir precision is measured by their estimated standard errors, e.s.e(\\(\\hat{\\alpha}\\)) and e.s.e(\\(\\hat{\\beta}\\))\nthese estimated standard errors are used in hypothesis testing and in constructing confidence and prediction intervals\n\nThe most common hypothesis test involves testing the null hypothesis of:\n\n\\(H_0:\\) There is no relationship between \\(X\\) and \\(Y\\)\nversus the alternative hypothesis \\(H_a:\\) there is some relationship between \\(X\\) and \\(Y\\)\n\nMathematically, this corresponds to testing:\n\n\\(H_0: \\beta=0\\)\nversus \\(H_a: \\beta\\neq0\\)\nsince if \\(\\beta=0\\) then the model \\(Y_i=\\alpha+\\beta x_i + \\epsilon_i\\) reduces to \\(Y=\\alpha + \\epsilon_i\\)\n\nUnder the null hypothesis \\(H_0: \\beta = 0\\)  \n\n\\(n\\) is number of observations\n\\(p\\) is number of model parameters\n\\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}\\) is the ratio of the departure of the estimated value of a parameter, \\(\\hat\\beta\\), from its hypothesized value, \\(\\beta\\), to its standard error, called t-statistics\nthe t-statistics follows Student’s t distribution with \\(n-p\\) degrees of freedom\n\n\nExample 1.3 (Hypothesis testing) Let’s look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the R summary of the model\n\nweight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nmodel &lt;- lm(plasma ~ weight)\nprint(summary(model))\n\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,    Adjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n\n\n\n\nUnder Estimate we see estimates of our model coefficients, \\(\\hat{\\alpha}\\) (intercept) and \\(\\hat{\\beta}\\) (slope, here weight), followed by their estimated standard errors, Std. Errors\nIf we were to test if there is an association between weight and plasma volume we would write under the null hypothesis \\(H_0: \\beta = 0\\) \\[\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582\\]\nand we would compare t-statistics to Student's t distribution with \\(n-p = 8 - 2 = 6\\) degrees of freedom (as we have 8 observations and two model parameters, \\(\\alpha\\) and \\(\\beta\\))\nwe can use Student’s t distribution table or R code to obtain the associated P-value\n\n\n2*pt(2.856582, df=6, lower=F)\n\n[1] 0.02893095\n\n\n\nhere the observed t-statistics is large and therefore yields a small P-value, meaning that there is sufficient evidence to reject null hypothesis in favor of the alternative and conclude that there is a significant association between weight and plasma volume"
  },
  {
    "objectID": "lm-intro.html#vector-matrix-notations",
    "href": "lm-intro.html#vector-matrix-notations",
    "title": "1  Introduction to linear models",
    "section": "1.9 Vector-matrix notations",
    "text": "1.9 Vector-matrix notations\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of multiple regression, with more than one explanatory variable in the model, it is more efficient to use vectors and matrices to define the regression model.\nLet’s rewrite our simple linear regression model \\(Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n\\) into vector-matrix notation in 6 steps.\n\nFirst we rename our \\(\\alpha\\) to \\(\\beta_0\\) and \\(\\beta\\) to \\(\\beta_1\\) as it is easier to keep tracking the number of model parameters this way\nThen we notice that we actually have \\(n\\) equations such as: \\[y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1\\] \\[y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2\\] \\[y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3\\] \\[\\dots\\] \\[y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n\\]\nWe group all \\(Y_i\\) and \\(\\epsilon_i\\) into column vectors: \\(\\mathbf{Y}=\\begin{bmatrix}  y_1 \\\\  y_2 \\\\  \\vdots \\\\  y_{n} \\end{bmatrix}\\) and \\(\\boldsymbol\\epsilon=\\begin{bmatrix}  \\epsilon_1 \\\\  \\epsilon_2 \\\\  \\vdots \\\\  \\epsilon_{n} \\end{bmatrix}\\)\nWe stack two parameters \\(\\beta_0\\) and \\(\\beta_1\\) into another column vector:\\[\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}\\]\nWe append a vector of ones with the single predictor for each \\(i\\) and create a matrix with two columns called design matrix \\[\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}\\]\nWe write our linear model in a vector-matrix notations as: \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\]\n\n\nDefinition 1.1 (vector matrix form of the linear model) The vector-matrix representation of a linear model with \\(p-1\\) predictors can be written as \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\]\nwhere:\n\n\\(\\mathbf{Y}\\) is \\(n \\times1\\) vector of observations\n\\(\\mathbf{X}\\) is \\(n \\times p\\) design matrix\n\\(\\boldsymbol\\beta\\) is \\(p \\times1\\) vector of parameters\n\\(\\boldsymbol\\epsilon\\) is \\(n \\times1\\) vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, \\(\\sigma^2\\))\n\nIn full, the above vectors and matrix have the form:\n\\(\\mathbf{Y}=\\begin{bmatrix}  y_1 \\\\  y_2 \\\\  \\vdots \\\\  y_{n} \\end{bmatrix}\\) \\(\\boldsymbol\\beta=\\begin{bmatrix}  \\beta_0 \\\\  \\beta_1 \\\\  \\vdots \\\\  \\beta_{p} \\end{bmatrix}\\) \\(\\boldsymbol\\epsilon=\\begin{bmatrix}  \\epsilon_1 \\\\  \\epsilon_2 \\\\  \\vdots \\\\  \\epsilon_{n} \\end{bmatrix}\\) \\(\\mathbf{X}=\\begin{bmatrix}  1 & x_{1,1} & \\dots & x_{1,p-1} \\\\  1 & x_{2,1} & \\dots & x_{2,p-1} \\\\  \\vdots & \\vdots & \\vdots & \\vdots \\\\  1 & x_{n,1} & \\dots & x_{n,p-1} \\end{bmatrix}\\)\n\n\nTheorem 1.2 (Least squares in vector-matrix notation) The least squares estimates for a linear regression of the form: \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\]\nis given by: \\[\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\]\n\n\nExample 1.4 (vector-matrix-notation) Following the above definition we can write the “weight - plasma volume model” as: \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\] where:\n\\(\\mathbf{Y}=\\begin{bmatrix}  2.75 \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12 \\end{bmatrix}\\)\n\\(\\boldsymbol\\beta=\\begin{bmatrix}  \\beta_0 \\\\  \\beta_1 \\end{bmatrix}\\) \\(\\boldsymbol\\epsilon=\\begin{bmatrix}  \\epsilon_1 \\\\  \\epsilon_2 \\\\  \\vdots \\\\  \\epsilon_{8} \\end{bmatrix}\\) \\(\\mathbf{X}=\\begin{bmatrix}  1 & 58.0 \\\\  1 & 70.0 \\\\  1 & 74.0 \\\\  1 & 63.5 \\\\  1 & 62.0 \\\\  1 & 70.5 \\\\  1 & 71.0 \\\\  1 & 66.0 \\\\ \\end{bmatrix}\\)\nand we can estimate model parameters using \\(\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\).\nWe can do it by hand or in R as follows:\n\nn &lt;- length(plasma) # no. of observation\nY &lt;- as.matrix(plasma, ncol=1)\nX &lt;- cbind(rep(1, length=n), weight)\nX &lt;- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\n\n     [,1]\n[1,] 2.75\n[2,] 2.86\n[3,] 3.37\n[4,] 2.76\n[5,] 2.62\n[6,] 3.49\n[7,] 3.05\n[8,] 3.12\n\nprint(X)\n\n       weight\n[1,] 1   58.0\n[2,] 1   70.0\n[3,] 1   74.0\n[4,] 1   63.5\n[5,] 1   62.0\n[6,] 1   70.5\n[7,] 1   71.0\n[8,] 1   66.0\n\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n\n             [,1]\n       0.08572428\nweight 0.04361534"
  },
  {
    "objectID": "lm-intro.html#confidence-intervals-and-prediction-intervals",
    "href": "lm-intro.html#confidence-intervals-and-prediction-intervals",
    "title": "1  Introduction to linear models",
    "section": "1.10 Confidence intervals and prediction intervals",
    "text": "1.10 Confidence intervals and prediction intervals\n\nwhen we estimate coefficients we can also find their confidence intervals, typically 95% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter\nwe can also use linear regression models to predict the response value given a new observation and find prediction intervals. Here, we look at any specific value of \\(x_i\\), and find an interval around the predicted value \\(y_i'\\) for \\(x_i\\) such that there is a 95% probability that the real value of y (in the population) corresponding to \\(x_i\\) is within this interval\n\n\nExample 1.5 (Prediction and intervals) Let’s:\n\nfind confidence intervals for our coefficient estimates\npredict plasma volume for a men weighting 60 kg\nfind prediction interval\nplot original data, fitted regression model, predicted observation and prediction interval\n\n\n# fit regression model\nmodel &lt;- lm(plasma ~ weight)\nprint(summary(model))\n\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,    Adjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n\n# find confidence intervals for the model coefficients\nconfint(model)\n\n                   2.5 %     97.5 %\n(Intercept) -2.419908594 2.59135716\nweight       0.006255005 0.08097567\n\n# predict plasma volume for a new observation of 60 kg\n# we have to create data frame with a variable name matching the one used to build the model\nnew.obs &lt;- data.frame(weight = 60)\npredict(model, newdata = new.obs)\n\n       1 \n2.702645 \n\n# find prediction intervals\nprediction.interval &lt;- predict(model, newdata = new.obs,  interval = \"prediction\")\nprint(prediction.interval)\n\n       fit      lwr      upr\n1 2.702645 2.079373 3.325916\n\n# plot the original data, fitted regression and predicted value\nplot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\", ylim=c(2,4))\nlines(weight, model$fitted.values, col=\"red\") # fitted model in red\npoints(new.obs, predict(model, newdata = new.obs), pch=19, col=\"blue\") # predicted value at 60kg\nsegments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval"
  },
  {
    "objectID": "lm-intro.html#assessing-model-fit",
    "href": "lm-intro.html#assessing-model-fit",
    "title": "1  Introduction to linear models",
    "section": "1.11 Assessing model fit",
    "text": "1.11 Assessing model fit\n\nEarlier we learned how to estimate parameters in a liner model using least squares estimation.\nNow we will consider how to assess the goodness of fit of a model, i.e. how well does the model explain our data.\nWe do that by calculating the amount of variability in the response that is explained by the model.\n\n\n\\(R^2\\): summary of the fitted model\n\nconsidering a simple linear regression, the simplest model, Model 0, we could consider fitting is \\[Y_i = \\beta_0+ \\epsilon_i\\] that corresponds to a line that run through the data but lies parallel to the horizontal axis\nin our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple)\n\n\n\n\n\n\n\n\n\n\n\nTSS, denoted Total corrected sum-of-squares is the residual sum-of-squares for Model 0 \\[S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}\\] corresponding the to the sum of squared distances to the purple line\n\n\n\n\n\n\n\n\n\n\n\nFitting Model 1 of the form \\[Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\] we have earlier defined\nRSS, the residual sum-of-squares as: \\[RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\]\nthat corresponds to the squared distances between the observed values \\(y_i, \\dots,y_n\\) to fitted values \\(\\hat{y_1}, \\dots \\hat{y_n}\\), i.e. distances to the red fitted line\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (\\(R^2\\)) A simple but useful measure of model fit is given by \\[R^2 = 1 - \\frac{RSS}{TSS}\\] where:\n\nRSS is the residual sum-of-squares for Model 1, the fitted model of interest\nTSS is the sum of squares of the null model\n\n\n\n\\(R^2\\) quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model\n\\(R^2\\) is also referred as coefficient of determination\nIt is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data\nValues of \\(R^2\\) approaching 1 indicate the model to be a good fit\nValues of \\(R^2\\) less than 0.5 suggest that the model gives rather a poor fit to the data\n\n\n\n\\(R^2\\) and correlation coefficient\n\nTheorem 1.3 (\\(R^2\\)) In the case of simple linear regression:\nModel 1: \\(Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\) \\[R^2 = r^2\\] where:\n\n\\(R^2\\) is the coefficient of determination\n\\(r^2\\) is the sample correlation coefficient\n\n\n\n\n\\(R^2(adj)\\)\n\nin the case of multiple linear regression, where there is more than one explanatory variable in the model\nwe are using the adjusted version of \\(R^2\\) to assess the model fit\nas the number of explanatory variables increase, \\(R^2\\) also increases\n\\(R^2(adj)\\) takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model\n\n\nTheorem 1.4 (\\(R^2(adj)\\)) For any multiple linear regression \\[Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i\\] \\(R^2(adj)\\) is defined as \\[R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}\\] where\n\n\\(p\\) is the number of independent predictors, i.e. the number of variables in the model, excluding the constant\n\n\\(R^2(adj)\\) can also be calculated from \\(R^2\\): \\[R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}\\]"
  },
  {
    "objectID": "lm-intro.html#the-assumptions-of-a-linear-model",
    "href": "lm-intro.html#the-assumptions-of-a-linear-model",
    "title": "1  Introduction to linear models",
    "section": "1.12 The assumptions of a linear model",
    "text": "1.12 The assumptions of a linear model\nUp until now we were fitting models and discussed how to assess the model fit. Before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data. Informally we have been using box plots and scatter plots to look at the data. There are however formal definitions of the assumptions.\nAssumption A: The deterministic part of the model captures all the non-random structure in the data\n\nThis implies that the mean of the errors \\(\\epsilon_i\\) is zero.\nTt applies only over the range of explanatory variables.\n\nAssumption B: the scale of variability of the errors is constant at all values of the explanatory variables\n\nPractically we are looking at whether the observations are equally spread on both side of the regression line.\n\nAssumption C: the errors are independent\n\nBroadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another.\n\nAssumptions D: the errors are normally distributed\n\nThis will allow us to describe the variation in the model’s parameters estimates and therefore make inferences about the population from which our sample was taken.\n\nAssumption E: the values of the explanatory variables are recorded without error\n\nThis one is not possible to check via examining the data, instead we have to consider the nature of the experiment.\n\n\n1.12.1 Checking assumptions\nResiduals, \\(\\hat{\\epsilon_i} = y_i - \\hat{y_i}\\) are the main ingredient to check model assumptions. We use plots such as:\n\nHistograms or normal probability plots of \\(\\hat{\\epsilon_i}\\)\n\n\nuseful to check the assumption of normality\n\n\nPlots of \\(\\hat{\\epsilon_i}\\) versus the fitted values \\(\\hat{y_i}\\)\n\n\nused to detect changes in error variance\nused to check if the mean of the errors is zero\n\n\nPlots of \\(\\hat{\\epsilon_i}\\) vs. an explanatory variable \\(x_{ij}\\)\n\n\nthis helps to check that the variable \\(x_j\\) has a linear relationship with the response variable\n\n\nPlots of \\(\\hat{\\epsilon_i}\\) vs. an explanatory variable \\(x_{kj}\\) that is not in the model\n\n\nthis helps to check whether the additional variable \\(x_k\\) might have a relationship with the response variable\n\n\nPlots of \\(\\hat{\\epsilon_i}\\) in the order of the observations were collected\n\n\nthis is useful to check whether errors might be correlated over time\n\nLet’s fit a simple model to predict BMI given waist for the diabetes study and see if the model meets the assumptions of linear models.\n\n# fit simple linear regression model\nmodel &lt;- lm(BMI ~ waist, data = data_diabetes)\n\n# plot diagnostic plots of the linear model\n# by default plot(model) calls four diagnostics plots\n# par() divides plot window in 2 x 2 grid\npar(mfrow=c(2,2))\nplot(model)\n\n\n\n\nDefault diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Simple regression to model BMI with waist explanatory variable.\n\n\n\n\n\nThe residual plots provides examples of a situation where the assumptions appear to be met.\nThe linear regression appears to describe data quite well.\nThere is no obvious trend of any kind in the residuals vs. fitted values (the shape is scattered) with potential few outliers that we may want to decided to exclude later.\nPoints lie reasonably well along the line in the normal probability plot, hence normality appears to be met.\n\nExamples of assumptions not being met\n\n\n\n\n\nExample of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case\n\n\n\n\n\n\n\n\n\nExample of non-constant variance\n\n\n\n\n\n\n\n\n\nExample of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected\n\n\n\n\n\n\n1.12.2 Influential observations\n\nSometimes individual observations can exert a great deal of influence on the fitted model.\nOne routine way of checking for this is to fit the model \\(n\\) times, missing out each observation in turn.\nIf we removed i-th observation and compared the fitted value from the full model, say \\(\\hat{y_j}\\) to those obtained by removing this point, denoted \\(\\hat{y_{j(i)}}\\) then\nobservations with a high Cook’s distance (measuring the effect of deleting a given observation) could be influential.\n\nLet’s remove some observation with higher Cook’s distance from protein data set, re-fit our model and compare the diagnostics plots\n\n# observations to be removed (based on Residuals vs. Leverage plot)\nobs &lt;- c(13, 78, 83, 84)\n\n# fit models removing observations\ndata_diabetes_flr &lt;- data_diabetes[-obs, ]\n\nmodel_flr &lt;- lm(BMI ~ waist, data = data_diabetes_flr)\n\n# plot diagnostics plot\npar(mfrow=c(2,2))\nplot(model_flr)"
  },
  {
    "objectID": "lm-reg-cls.html#linear-models-in-ml-context",
    "href": "lm-reg-cls.html#linear-models-in-ml-context",
    "title": "2  Linear models: regression and classification",
    "section": "2.1 Linear models in ML context",
    "text": "2.1 Linear models in ML context\nWe can think of linear models in machine learning context, as linear models are often used for both building both regression and classification machine learning models and used for predictions It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response, and there are feature selection approaches that enable us to exclude irrelevant variables and as a consequence improve prediction results.\n\nIndeed, it is known that including irrelevant variables leads to unnecessary complexity in the resulting model, and often worse prediction results.\nThere are few approaches to perform feature selection or variable selection, that is for excluding irrelevant variables from a multiple regression model.\nHere, we can group the feature selection methods by differences classes such as: subset selection, Shrinkage methods and dimension reduction. Another classification is by filter methods, wrapper methods and embedded methods.\nBefore we can divide more into these methods, we need to discuss how to evaluate regression results. We will need a way of comparing the models and evaluating the predictions outcomes."
  },
  {
    "objectID": "lm-reg-cls.html#evaluating-regression",
    "href": "lm-reg-cls.html#evaluating-regression",
    "title": "2  Linear models: regression and classification",
    "section": "2.2 Evaluating regression",
    "text": "2.2 Evaluating regression\n\nRegression models can be evaluated by assessing a model fit, something that we have seen previously with adjusted \\(R^2\\). Other metrics than can also be expressed in terms of RSS include Akaike information criterion (AIC) and Bayesian information criterion (BIC).\nAlternatively, by using data splitting strategies such as validation and cross-validation we can directly evaluate the prediction error. Here, we use metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) or Mean Absolute Percentage Error (MAPE)-\n\n\nmodel fit\nAdjusted R-squared (as seen before) \\[\nR_{adj}^2=1-\\frac{RSS}{TSS}\\frac{n-1}{n-p-1} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\frac{n-1}{n-p-1}\n\\]\nAIC and BIC\nAIC is grounded in information theory and BIC is derived from a Bayesian point of view. Both are formally defined in likelihood functions. For regression models they can be expressed in terms of RSS because the likelihood of a model in the context of normal errors is directly related to the RSS.\n\\[\\text{AIC} = n \\ln(\\text{RSS}/n) + 2p\\]\nwhere:\n\n\\(n\\) is the number of observations.\n\\(\\text{RSS}\\) is the residual sum of squares.\n\\(p\\) is the number of parameters in the model (including the intercept).\n\n\\[\\text{BIC} = n \\ln(\\text{RSS}/n) + p \\ln(n)\\]\nwhere:\n\n\\(n\\) is the number of observations.\n\\(\\text{RSS}\\) is the residual sum of squares.\n\\(p\\) is the number of parameters in the model.\n\nBoth criteria, AIC and BIC, introduce penalties for the number of parameters to avoid overfitting. BIC introduces a stronger penalty based on the sample size, making it more conservative than AIC. When comparing models using AIC or BIC that incorporate RSS, the objective remains the same: select the model that provides the best balance between goodness of fit and model simplicity. The model with the lower AIC or BIC value is generally preferred, as it indicates either a more parsimonious model or a model that better fits the data (or both).\n\n\npredictions\n\nMean Squared Error (MSE): average squared difference between the predicted values and the actual values. \\[MSE = \\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2\\]\nRoot Mean Squared Error (RMSE): square root of the MSE \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2}\\]\nMAE: average absolute difference between the predicted values and the actual values \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y}_i|\\]\nMean Absolute Percentage Error (MAPE): average percentage difference between the predicted values and the actual values.\n\nThe smaller the difference between the predicted values vs. the validation or cross-validation predicted values, the better the model."
  },
  {
    "objectID": "lm-reg-cls.html#feature-selection",
    "href": "lm-reg-cls.html#feature-selection",
    "title": "2  Linear models: regression and classification",
    "section": "2.3 Feature selection",
    "text": "2.3 Feature selection\nFeature selection is the process of selecting the most relevant and informative subset of features from a larger set of potential features in order to improve the performance and interpretability of a machine learning model. There are generally three main groups of feature selection methods:\n\nFilter methods use statistical measures to score the features and select the most relevant ones, e.g. based on correlation coefficient or \\(\\chi^2\\) test. They tend to be computationally efficient but may overlook complex interactions between features and can be sensitive to the choice of metric used to evaluate the feature importance.\nWrapper methods use a machine learning algorithm to evaluate the performance of different subsets of features, e.g. forward/backward feature selection. They tend to be computationally heavy.\nEmbedded methods incorporate feature selection as part of the machine learning algorithm itself, e.g. regularized regression or Random Forest. These methods are computationally efficient and can be more accurate than filter methods."
  },
  {
    "objectID": "lm-reg-cls.html#regularized-regression",
    "href": "lm-reg-cls.html#regularized-regression",
    "title": "2  Linear models: regression and classification",
    "section": "2.4 Regularized regression",
    "text": "2.4 Regularized regression\nRegularized regression expands on the regression by adding a penalty term or terms to shrink the model coefficients of less important features towards zero. This can help to prevent overfitting and improve the accuracy of the predictive model. Depending on the penalty added, we talk about Ridge, Lasso or Elastic Nets regression.\nPreviously when talking about regression, we saw that the least squares fitting procedure estimates model coefficients \\(\\beta_0, \\beta_1, \\cdots, \\beta_p\\) using the values that minimize the residual sum of squares: \\[RSS = \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 \\tag{2.1}\\]\nIn regularized regression the coefficients are estimated by minimizing slightly different quantity. In Ridge regression we estimate \\(\\hat\\beta^{L}\\) that minimizes \\[\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2 \\tag{2.2}\\]\nwhere:\n\\(\\lambda \\ge 0\\) is a tuning parameter to be determined separately e.g. via cross-validation\nEquation 2.2 trades two different criteria:\n\nas with least squares, lasso regression seeks coefficient estimates that fit the data well, by making RSS small\nhowever, the second term \\(\\lambda \\sum_{j=1}^{p}\\beta_j^2\\), called shrinkage penalty is small when \\(\\beta_1, \\cdots, \\beta_p\\) are close to zero, so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero.\nthe tuning parameter \\(\\lambda\\) controls the relative impact of these two terms on the regression coefficient estimates\n\nwhen \\(\\lambda = 0\\), the penalty term has no effect\nas \\(\\lambda \\rightarrow \\infty\\) the impact of the shrinkage penalty grows and the ridge regression coefficient estimates approach zero\n\n\n\n\nCode\nlibrary(glmnet)\nlibrary(latex2exp)\n\n# select subset data\n# and scale: since regression puts constraints on the size of the coefficient\ndata_input &lt;- data_diabetes %&gt;%\n  dplyr::select(BMI, chol, hdl, age, stab.glu) %&gt;% \n  na.omit() \n  \n# fit ridge regression for a series of lambda values \n# note: lambda values were chosen by experimenting to show lambda effect on beta coefficient estimates\nx &lt;- model.matrix(BMI ~., data = data_input)\ny &lt;- data_input %&gt;% pull(BMI)\nmodel &lt;- glmnet(x, y, alpha=0, lambda = seq(0, 100, 1))\n\n# plot beta estimates vs. lambda\nbetas &lt;- model$beta %&gt;% as.matrix() %&gt;% t()\ndata_plot &lt;- tibble(data.frame(lambda = model$lambda, betas)) %&gt;%\n  dplyr::select(-\"X.Intercept.\") %&gt;%\n  pivot_longer(-lambda, names_to = \"variable\", values_to = \"beta\")\n\ndata_plot %&gt;%\n  ggplot(aes(x = lambda, y = beta, color = variable)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_classic() +\n  xlab(TeX(\"$\\\\lambda$\")) + \n  ylab(TeX(\"Standardized coefficients\")) + \n  scale_color_brewer(palette = \"Set1\") + \n  theme(legend.title = element_blank(), legend.position = \"top\", legend.text = element_text(size=12)) + \n  theme(axis.title = element_text(size = 12), axis.text = element_text(size = 10))\n\n\n\n\n\nExample of Ridge regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero for larger lambda values."
  },
  {
    "objectID": "lm-reg-cls.html#bias-variance-trade-off",
    "href": "lm-reg-cls.html#bias-variance-trade-off",
    "title": "2  Linear models: regression and classification",
    "section": "2.5 Bias-variance trade-off",
    "text": "2.5 Bias-variance trade-off\nRidge regression’s advantages over least squares estimates stems from bias-variance trade-off, another fundamental concept in machine learning.\n\nThe bias-variance trade-off describes the relationship between model complexity, prediction accuracy, and the ability of the model to generalize to new data.\nBias refers to the error that is introduced by approximating a real-life problem with a simplified model. A high bias model is one that makes overly simplistic assumptions about the underlying data, resulting in under-fitting and poor accuracy.\nVariance refers to the sensitivity of a model to fluctuations in the training data. A high variance model is one that is overly complex and captures noise in the training data, resulting in overfitting and poor generalization to new data.\nThe goal of machine learning is to find a model with the right balance between bias and variance, which can generalize well to new data.\nThe bias-variance trade-off can be visualized in terms of MSE, means squared error of the model. The MSE can be decomposed into: \\[MSE(\\hat\\beta) := bias^2(\\hat\\beta) + Var(\\hat\\beta) + noise\\]\nThe irreducible error is the inherent noise in the data that cannot be reduced by any model, while the bias and variance terms can be reduced by choosing an appropriate model complexity. The trade-off lies in finding the right balance between bias and variance that minimizes the total MSE.\nIn practice, this trade-off can be addressed by regularizing the model, selecting an appropriate model complexity, or by using ensemble methods that combine multiple models to reduce the variance (e.g. Random Forest). Ultimately, the goal is to find a model that is both accurate and generalizing.\n\n\n\n\n\n\nFigure 2.1: Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. al, A Introduction to statistical learning"
  },
  {
    "objectID": "lm-reg-cls.html#ridge-lasso-and-elastic-nets",
    "href": "lm-reg-cls.html#ridge-lasso-and-elastic-nets",
    "title": "2  Linear models: regression and classification",
    "section": "2.6 Ridge, Lasso and Elastic Nets",
    "text": "2.6 Ridge, Lasso and Elastic Nets\nIn Ridge regression we minimize: \\[\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2 \\tag{2.3}\\] where \\(\\lambda \\sum_{j=1}^{p}\\beta_j^2\\) is also known as L2 regularization element or \\(l_2\\) penalty\nIn Lasso regression, that is Least Absolute Shrinkage and Selection Operator regression we change penalty term to absolute value of the regression coefficients: \\[\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j| = RSS + \\lambda \\sum_{j=1}^{p}|\\beta_j| \\tag{2.4}\\] where \\(\\lambda \\sum_{j=1}^{p}|\\beta_j|\\) is also known as L1 regularization element or \\(l_1\\) penalty\nLasso regression was introduced to help model interpretation. With Ridge regression we improve model performance but unless \\(\\lambda = \\infty\\) all beta coefficients are non-zero, hence all variables remain in the model. By using \\(l_1\\) penalty we can force some of the coefficients estimates to be exactly equal to 0, hence perform variable selection\n\n\nCode\nlibrary(glmnet)\nlibrary(latex2exp)\n\n# select subset data\n# and scale: since regression puts constraints on the size of the coefficient\ndata_input &lt;- data_diabetes %&gt;%\n  dplyr::select(BMI, chol, hdl, age, stab.glu) %&gt;% \n  na.omit() \n  \n# fit ridge regression for a series of lambda values \n# note: lambda values were chosen by experimenting to show lambda effect on beta coefficient estimates\nx &lt;- model.matrix(BMI ~., data = data_input)\ny &lt;- data_input %&gt;% pull(BMI)\nmodel &lt;- glmnet(x, y, alpha=1, lambda = seq(0, 2, 0.1))\n\n# plot beta estimates vs. lambda\nbetas &lt;- model$beta %&gt;% as.matrix() %&gt;% t()\ndata_plot &lt;- tibble(data.frame(lambda = model$lambda, betas)) %&gt;%\n  dplyr::select(-\"X.Intercept.\") %&gt;%\n  pivot_longer(-lambda, names_to = \"variable\", values_to = \"beta\")\n\ndata_plot %&gt;%\n  ggplot(aes(x = lambda, y = beta, color = variable)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_classic() +\n  xlab(TeX(\"$\\\\lambda$\")) + \n  ylab(TeX(\"Standardized coefficients\")) + \n  scale_color_brewer(palette = \"Set1\") + \n  theme(legend.title = element_blank(), legend.position = \"top\", legend.text = element_text(size=12)) + \n  theme(axis.title = element_text(size = 12), axis.text = element_text(size = 10))\n\n\n\n\n\nExample of Lasso regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero and are also set to zero for larger lambda values.\n\n\n\n\nElastic Net use both L1 and L2 penalties to try to find middle grounds by performing parameter shrinkage and variable selection. \\[\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j| + \\lambda \\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda \\sum_{j=1}^{p}|\\beta_j| + \\lambda \\sum_{j=1}^{p}\\beta_j^2  \\tag{2.5}\\]\nIn the glmnet library we can fit Elastic Net by setting parameters \\(\\alpha\\). Actually, under the hood glmnet minimizes a cost function: \\[\\sum_{i_=1}^{n}(y_i-\\hat y_i)^2 + \\lambda \\left ( (1-\\alpha) \\sum_{j=1}^{p}\\beta_j^2 + \\alpha \\sum_{j=1}^{p}|\\beta_j|\\right )\\] where:\n\n\\(n\\) is the number of samples\n\\(p\\) is the number of parameters\n\\(\\lambda\\), \\(\\alpha\\) hyperparameters control the shrinkage\n\nWhen \\(\\alpha = 0\\) this corresponds to Ridge regression and when \\(\\alpha=1\\) this corresponds to Lasso regression. A value of \\(0 &lt; \\alpha &lt; 1\\) gives us Elastic Net regularization, combining both L1 and L2 regularization terms.\n\n\nCode\nlibrary(glmnet)\nlibrary(latex2exp)\n\n# select subset data\n# and scale: since regression puts constraints on the size of the coefficient\ndata_input &lt;- data_diabetes %&gt;%\n  dplyr::select(BMI, chol, hdl, age, stab.glu) %&gt;% \n  na.omit() \n  \n# fit ridge regression for a series of lambda values \n# note: lambda values were chosen by experimenting to show lambda effect on beta coefficient estimates\nx &lt;- model.matrix(BMI ~., data = data_input)\ny &lt;- data_input %&gt;% pull(BMI)\nmodel &lt;- glmnet(x, y, alpha=0.1, lambda = seq(0, 3, 0.05))\n\n# plot beta estimates vs. lambda\nbetas &lt;- model$beta %&gt;% as.matrix() %&gt;% t()\ndata_plot &lt;- tibble(data.frame(lambda = model$lambda, betas)) %&gt;%\n  dplyr::select(-\"X.Intercept.\") %&gt;%\n  pivot_longer(-lambda, names_to = \"variable\", values_to = \"beta\")\n\ndata_plot %&gt;%\n  ggplot(aes(x = lambda, y = beta, color = variable)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_classic() +\n  xlab(TeX(\"$\\\\lambda$\")) + \n  ylab(TeX(\"Standardized coefficients\")) + \n  scale_color_brewer(palette = \"Set1\") + \n  theme(legend.title = element_blank(), legend.position = \"top\", legend.text = element_text(size=12)) + \n  theme(axis.title = element_text(size = 12), axis.text = element_text(size = 10))\n\n\n\n\n\nExample of Elastic Net regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values and alpha value 0.1, showing the changes of model coefficients as a function of lambda being somewhere between those for Ridge and Lasso regression."
  },
  {
    "objectID": "lm-reg-cls.html#generalized-linear-models",
    "href": "lm-reg-cls.html#generalized-linear-models",
    "title": "2  Linear models: regression and classification",
    "section": "2.7 Generalized linear models",
    "text": "2.7 Generalized linear models\n\nGLMs extend linear model framework to outcome variables that do not follow normal distribution.\nThey are most frequently used to model binary, categorical or count data.\nFor instance, fitting a regression line to binary data yields predicted values that could take any value, including \\(&lt;0\\),\nnot to mention that it is hard to argue that the values of 0 and 1s are normally distributed.\n\n\n\nCode\ndata_diabetes %&gt;%\n  mutate(obese = as.numeric(obese) - 1) %&gt;%\n  ggplot(aes(y=obese, x=waist)) +\n  geom_jitter(width=0, height = 0) +\n  geom_smooth(method=\"lm\", se=FALSE, color=col.blue.dark) + \n  my.ggtheme\n\n\n\n\n\nFigure 2.2: Example of fitting linear model to binary data, to model obesity status coded as 1 (Yes) and 0 (No) with waist variable. Linear model does not fit the data well in this case and the predicted values can take any value, not only 0 and 1."
  },
  {
    "objectID": "lm-reg-cls.html#logistic-regression",
    "href": "lm-reg-cls.html#logistic-regression",
    "title": "2  Linear models: regression and classification",
    "section": "2.8 Logistic regression",
    "text": "2.8 Logistic regression\nLet’s look again at the binary obesity status data and try to fit logistic regression model using waist as explanatory variable instead of fitting inappropriate here simple linear model.\n\n\nCode\np1 &lt;- data_diabetes %&gt;%\n  mutate(obese = as.numeric(obese) - 1) %&gt;%\n  ggplot(aes(y=obese, x=waist)) +\n  geom_jitter(width=0, height = 0.05, alpha = 0.7) +\n  my.ggtheme\n\np2 &lt;- data_diabetes %&gt;%\n  ggplot(aes(x = obese, y = waist, fill = obese)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \"Set2\") + \n  my.ggtheme\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nFigure 2.3: Obesity status data: jittered plot (left) and box plot of waist stratified by obesity status for the 130 study participants.\n\n\n\n\n\nSince the response variable takes only two values (Yes/No) we use GLM model\nto fit logistic regression model for the probability of suffering from obesity (Yes).\nWe let \\(p_i=P(Y_i=1)\\) denote the probability of suffering from obesity (success)\nand we assume that the response follows binomial distribution: \\(Y_i \\sim Bi(1, p_i)\\) distribution.\nWe can then write the regression model now as: \\[log(\\frac{p_i}{1-p_i})=\\beta_0 + \\beta_1x_i\\] and given the properties of logarithms this is also equivalent to: \\[p_i = \\frac{exp(\\beta_0 + \\beta_1x_i)}{1 + exp(\\beta_0 + \\beta_1x_i)}\\]\n\n\n\nIn essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function.\nHere, the link function \\(log(\\frac{p_i}{1-p_i})\\) provides the link between the binomial distribution of \\(Y_i\\) (suffering from obesity) and the linear predictor (waist)\nThus the GLM model can be written as \\[g(\\mu_i)=\\mathbf{X}\\boldsymbol\\beta\\] where g() is the link function.\n\n\nIn R we can use glm() function to fit GLM models:\n\n# re-code obese status from Yes/No to 1/0\ndata_diabetes &lt;- \n  data_diabetes %&gt;%\n  mutate(obese = as.numeric(obese) - 1)\n\n# fit logistic regression model\nlogmodel_1 &lt;- glm(obese ~ waist, family = binomial(link=\"logit\"), data = data_diabetes)\n\n# print model summary\nprint(summary(logmodel_1))\n## \n## Call:\n## glm(formula = obese ~ waist, family = binomial(link = \"logit\"), \n##     data = data_diabetes)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -17.357      2.973  -5.837 5.30e-09 ***\n## waist         17.174      2.974   5.775 7.71e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 102.79  on 128  degrees of freedom\n## AIC: 106.79\n## \n## Number of Fisher Scoring iterations: 5\n\n# plot\nggPredict(logmodel_1) + \n  my.ggtheme\n\n\n\n\nFitted logistic model to diabetes data given the 130 study participants and using waist as explantatory variable to model obesity status.\n\n\n\n# to get predictions use predict() functions\n# if no new observations is specified predictions are returned for the values of exploratory variables used\n# we specify response to return prediction on the probability scale\nobese_predicted &lt;- predict(logmodel_1, type=\"response\")\nprint(head(obese_predicted))\n##          3          7          9         11         16         21 \n## 0.98231495 0.93752915 0.07405337 0.41460606 0.22839680 0.72385967\n\n\nThe regression equation for the fitted model is: \\[log(\\frac{\\hat{p_i}}{1-\\hat{p_i}})=-17.357  +  17.174\\cdot x_i\\]\nWe see from the output that \\(\\hat{\\beta_0} = -17.357\\) and \\(\\hat{\\beta_1} = 17.174\\).\nThese estimates are arrived at via maximum likelihood estimation, something that is out of scope here.\n\n\nHypothesis testing\n\nSimilarly to linear models, we can determine whether each variable is related to the outcome of interest by testing the null hypothesis that the relevant logistic regression coefficient is zero.\nThis can be performed by Wald test which is equals to estimated logistic regression coefficient divided by its standard error and follows the Standard Normal distribution: \\[W = \\frac{\\hat\\beta-\\beta}{e.s.e.(\\hat\\beta)} \\sim N(0,1)\\].\nAlternatively, its square approximates the Chi-squared distribution with 1 degree of freedom: \\[W^2 = \\frac{(\\hat\\beta-\\beta)^2}{\\hat {var}(\\hat\\beta)} \\sim \\chi_1^2\\].\n\nIn our example, we can check whether waist is associated with obesity status by testing null hypothesis: \\(H_0:\\beta_1=0\\). We calculate Wald statistics as \\(W^2 = \\frac{(\\hat\\beta-0)}{\\hat {e.s.e}(\\hat\\beta)} = \\frac{17.174}{2.974} = 5.774714\\) and we can find the corresponding p-value using standard normal distribution:\n\n2*pnorm(5.774714, lower.tail = F)\n## [1] 7.70839e-09\n\nwhich confirms the summary output shown previously above and shows that there is enough evidence to reject the null hypothesis at 5% significance level \\(p-value &lt;&lt; 0.05\\) and conclude that there is a significant association between waist and obesity status.\n\n\nDeviance\n\nDeviance is the number that measures the goodness of fit of a logistic regression model.\nWe use saturated and residual deviance to assess model, instead of \\(R^2\\) or \\(R^2(adj)\\).\nWe can also use deviance to check the association between explanatory variable and the outcome, an alternative and slightly more powerful test than Wald test (although these two test give similar results when sample size is large).\nIn the likelihood ratio test the test statistics is the deviance for the full model minus the deviance for the full model excluding the relevant explanatory variable. This test statistics follow a Chi-squared distribution with 1 degree of freedom.\n\n\nFor instance, given our model we have null deviance of 274.4 and residual deviance of 268.7. The difference 5.7 is larger than than 95th percentile of \\(\\chi^2(129-128)\\) = 3.841459, where 129 is degrees of freedom for null model and 128 is degrees of freedom for null model excluding the waist explanatory variable.\n\nqchisq(df=1, p=0.95)\n## [1] 3.841459\n\n\nAgain \\(5.7 &gt; 3.84\\) and we can conclude that waist is a significant term in the model.\n\n\n\nOdds ratios\n\nIn logistic regression we often interpret the model coefficients by taking \\(e^{\\hat{\\beta}}\\)\nand we talk about odd ratios.\nFor instance we can say, given our above model, \\(e^{17.174} = 28745736\\) that for each unit increase in waist the odds of suffering from obesity get multiplied by 28745736.\n\nThese odds ratios are very high as here we are modeling obesity status with waist measurements, and increasing one unit in waist would mean adding up additional 1m to waist measurements. Typically:\n\nOdd ratios of 1.0 (or close to 1.0) indicates that exposure is not associated with the outcome.\nOdds ratios \\(&gt; 1.0\\) indicates that the odds of exposure among cases are greater than the odds of exposure among controls.\nOdds raitos \\(&lt; 1.0\\) indicates that the odds of exposure among cases are lower than the odds of exposure among controls.\nThe magnitude of the odds ratio is called the strength of the association. The further away an odds ratio is from 1.0, the more likely it is that the relationship between the exposure and the disease is causal. For example, an odds ratio of 1.2 is above 1.0, but is not a strong association. An odds ratio of 10 suggests a stronger association.\n\n\n\nOther covariates\n\nWe can use the same logic as in multiple regression to expand by models by additional variables, numerical, binary or categorical.\nFor instance, we can test whether there is a gender effect when suffering from obesity:\n\n\n# fit logistic regression including age and gender\nlogmodel_2 &lt;- glm(obese ~ waist + gender, family = binomial(link=\"logit\"), data = data_diabetes)\n\n# print model summary\nprint(summary(logmodel_2))\n## \n## Call:\n## glm(formula = obese ~ waist + gender, family = binomial(link = \"logit\"), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -18.2756     3.1077  -5.881 4.08e-09 ***\n## waist         17.4401     3.0523   5.714 1.11e-08 ***\n## genderfemale   1.2335     0.5228   2.359   0.0183 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.708  on 129  degrees of freedom\n## Residual deviance:  96.877  on 127  degrees of freedom\n## AIC: 102.88\n## \n## Number of Fisher Scoring iterations: 6\n\n# plot model\nggPredict(logmodel_2) + \n  my.ggtheme + \n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nObesity status model with logistic regression given waist and gender exploratory variables. There is some seperation between the fitted lines for men and women and the model summary shows that there is enough evidence to reject a null hypothesis o no association between gender and obesity status."
  },
  {
    "objectID": "lm-reg-cls.html#poisson-regression",
    "href": "lm-reg-cls.html#poisson-regression",
    "title": "2  Linear models: regression and classification",
    "section": "2.9 Poisson regression",
    "text": "2.9 Poisson regression\n\nGLMs can be also applied to count data\nFor instance to model hospital admissions due to respiratory disease or number of bird nests in a certain habitat.\nHere, we commonly assume that data follow the Poisson distribution \\(Y_i \\sim Pois(\\mu_i)\\)\nand the corresponding model is: \\[E(Y_i)=\\mu_i = \\eta_ie^{\\mathbf{x_i}^T\\boldsymbol\\beta}\\] with a log link \\(\\ln\\mu_i = \\ln \\eta_i + \\mathbf{x_i}^T\\boldsymbol\\beta\\)\nHypothesis testing and assessing model fit follows the same logic as in logistic regression.\n\n\nExample 2.1 (Number of cancer cases) Suppose we wish to model \\(Y_i\\) the number of cancer cases in the i-th intermediate geographical location (IG) in Glasgow. We have collected data for 271 small regions with between 2500 and 6000 people living in them. Together with cancer occurrence with have the following data:\n\nY_all: number of cases of all types of cancer in the IG in 2013\nE_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013\npm10: air pollution\nsmoke: percentage of people in an area that smoke\nethnic: percentage of people who are non-white\nlog.price: natural log of average house price\neasting and northing: co-ordinates of the central point of the IG divided by 10000\n\nWe can model the rate of occurrence of cancer using the very same glm function:¨ - now we use poisson family distribution to model counts - and we will include an offset term to the model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions.\n\n\n# Read in and preview data\ncancer &lt;- read.csv(\"data/lm/cancer.csv\")\nhead(cancer)\n##          IG Y_all     E_all pm10 smoke ethnic log.price  easting northing\n## 1 S02000260   133 106.17907 17.8  21.9   5.58  11.59910 26.16245 66.96574\n## 2 S02000261    38  62.43131 18.6  21.8   7.91  11.84940 26.29271 67.00278\n## 3 S02000262    97 120.00694 18.6  20.8   9.58  11.74106 26.21429 67.04280\n## 4 S02000263    80 109.10245 17.0  14.0  10.39  12.30138 25.45705 67.05938\n## 5 S02000264   181 149.77821 18.6  15.2   5.67  11.88449 26.12484 67.09280\n## 6 S02000265    77  82.31156 17.0  14.6   5.61  11.82004 25.37644 67.09826\n\n# fit Poisson regression\nepid1 &lt;- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), \n             family = poisson, \n             data = cancer)\n\nprint(summary(epid1))\n## \n## Call:\n## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + \n##     northing + offset(log(E_all)), family = poisson, data = cancer)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -0.8592657  0.8029040  -1.070 0.284531    \n## pm10         0.0500269  0.0066724   7.498 6.50e-14 ***\n## smoke        0.0033516  0.0009463   3.542 0.000397 ***\n## ethnic      -0.0049388  0.0006354  -7.773 7.66e-15 ***\n## log.price   -0.1034461  0.0169943  -6.087 1.15e-09 ***\n## easting     -0.0331305  0.0103698  -3.195 0.001399 ** \n## northing     0.0300213  0.0111013   2.704 0.006845 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 972.94  on 270  degrees of freedom\n## Residual deviance: 565.18  on 264  degrees of freedom\n## AIC: 2356.2\n## \n## Number of Fisher Scoring iterations: 4\n\nRate ratio\n\nSimilarly to logistic regression, it is common to look at the \\(e^\\beta\\).\nFor instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient.\nThe ppm10 coefficient is positive, 0.0500269, indicating that cancer incidence rate increases with increased air pollution.\nThe rate ratio allows us to quantify by how much, here by a factor of \\(e^{0.0500269} = 1.05\\)."
  },
  {
    "objectID": "lm-reg-cls.html#logistic-lasso",
    "href": "lm-reg-cls.html#logistic-lasso",
    "title": "2  Linear models: regression and classification",
    "section": "2.10 Logistic Lasso",
    "text": "2.10 Logistic Lasso\n\nLogistic Lasso combines logistic regression with Lasso regularization to analyze binary outcome data while simultaneously performing variable selection and regularization.\nThe equation for Logistic Lasso combines logistic regression with Lasso regularization. We estimate set of coefficients \\(\\hat \\beta\\) that minimize the combined logistic loss function and the Lasso penalty:\n\n\\[\n\\left( \\sum_{i=1}^n [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)] \\right) + \\lambda \\sum_{j=1}^p |\\beta_j|\n\\]\nwhere:\n\n\\(y_i\\) represents the binary outcome (0 or 1) for the ( i )-th observation.\n\\(p_i\\) is the predicted probability of \\(y_i = 1\\) given by the logistic model \\(p_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip})}}\\).\n\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are the coefficients of the model, including the intercept \\(\\beta_0\\).\n\\(x_{i1}, \\ldots, x_{ip}\\) are the predictor variables for the \\(i\\)-th observation.\n\\(\\lambda\\) is the regularization parameter that controls the strength of the Lasso penalty \\(\\lambda \\sum_{j=1}^p |\\beta_j|\\), which encourages sparsity in the coefficients \\(\\beta_j\\) by shrinking some of them to zero.\n\\(n\\) is the number of observations, and \\(p\\) is the number of predictors."
  },
  {
    "objectID": "lm-coeff.html#example-simple-linear-regression",
    "href": "lm-coeff.html#example-simple-linear-regression",
    "title": "3  Common cases",
    "section": "3.1 Example: simple linear regression",
    "text": "3.1 Example: simple linear regression\n\nm1 &lt;- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n## \n## Call:\n## lm(formula = BMI ~ waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.2374  -2.7689  -0.4532   2.4065  19.3549 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -5.12445    2.73538  -1.873   0.0633 .  \n## waist        0.35298    0.02723  12.965   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.426 on 128 degrees of freedom\n## Multiple R-squared:  0.5677, Adjusted R-squared:  0.5643 \n## F-statistic: 168.1 on 1 and 128 DF,  p-value: &lt; 2.2e-16\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\"waist [cm]\")\n\n\n\n\n\nFigure 3.1: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n\n\n\nModel (generic)\n\n\\(Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i\\)\n\nModel (fitted)\n\n\\(BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i\\)\n\nSlope\n\nThe value of slope tells us how and by much the outcome changes with a unit change in \\(x\\)\nIf the waist increases by 1 unit, here in cm, what would be our expected change in BMI1$?\nAnd if the waist increases by 10 units what would be our expected change in BMI2$?\n\nIntercept\n\nThe intercept, often labeled the constant, is the value of Y when \\(x_i=0\\).\nIn models where \\(x_i\\) can be equal 0, the intercept is simply the expected mean value of response.\nIn models where \\(x_i\\) cannot be equal 0, like in our BMI example where it is not possible to have BMI equal to zero, the intercept has no intrinsic meaning.\nThe intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\nHypothesis testing\n\nWe’ve seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject \\(H_0: \\beta=0\\) in favor of the alternative \\(H_a: \\beta\\neq0\\).\nHere, for the \\(\\beta\\) coefficient we have \\(t-statistics = 0.35298 / 0.02723 = 12.965\\) and a corresponding \\(p-value = 12.96291\\), as \\(t-statistics \\sim t(130-2) &lt;&lt; 0.05\\). Such large t-statsitics or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI.\nWe can double-check R output by calculating p-value ourselves using the Student t distribution:\n\n\n2*pt(12.96291, df=128, lower=F)\n## [1] 4.605102e-25\n\n\nIs there enough evidence to reject the null hypothesis of \\(H_0: \\alpha=0\\) in favor of the alternative \\(H_a: \\alpha\\neq0\\) assuming 5% significance level?3.\nIs there enough evidence to reject the null hypothesis of \\(H_0: \\alpha=0\\) in favor of the alternative \\(H_a: \\alpha\\neq0\\) assuming 10% significance level?4.\n\nPredictions\n\nUsing the model we can predict the BMI value for a new observation of waist.\nFor instance, we can find expected BMI value for someone who measures 100 cm in waist by:\n\\(BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355\\)\nIn R can use predict() function:\n\n\n# predict BMI for a new value of 100\nnew_data &lt;- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n##        1 \n## 30.17348\n\n\nWhat would be BMI for someone with waist measurements of 75?5\nWhat would be BMI for someone with waist measurements of 200?6\n\nModel fit\n\nIn simple regression we can use \\(R^2\\) to assess model fit, here \\(R^2 = 0.5677\\).\nDo you think that the model fits the data well?7\n\nModel assumptions\nWe should also not forget to look at the residual plots to check model assumptions:\n\n\nCode\npar(mfrow = c(2,2))\nplot(m1)\n\n\n\n\n\n\nGiven the diagnostic plots can we comment about the assumptions of linear models being met?8"
  },
  {
    "objectID": "lm-coeff.html#example-multiple-regression",
    "href": "lm-coeff.html#example-multiple-regression",
    "title": "3  Common cases",
    "section": "3.2 Example: multiple regression",
    "text": "3.2 Example: multiple regression\nLet’s try to model BMI using more variables\nModel (generic)\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\epsilon_i\\)\n\n\n# fit multiple linear regression and print model summary\nm2 &lt;- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 35.456968   3.149661  11.257  &lt; 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,    Adjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n\nCoefficient interpretations\nUsing the model answer the questions:\n\nwhat would happen to BMI if hdl levels increase by 10?9\nwhat would happen to BMI if age increases by 1 year?10\n\nHypothesis testing\n\noverall, is there a relationship between the response \\(Y\\) (BMI) and predictors?11\n\n Not so easy: alternative model\nLet’s consider another multiple regression model:\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot waist_i + \\epsilon_i\\)\n\nWe fit the model in R and look at the model summary:\n\nm2_alt &lt;- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864, Adjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: &lt; 2.2e-16\n\n\nWhat happens to BMI if hdl increases by 10?12\nWhat happens to BMI if hdl increases by 10 using the first model again?[decreases by ca. 0.9]\nHow do you explain the difference in BMI changes given these two models?\n\nSpecific interpretation\n\nObviously there is difference between decrease of 0.9 BMI and decrease of 0.9 in BMI (alternative model).\nOur interpretations need to be more specific and we say that a unit increase in \\(x\\) with other predictors held constant will produce a change equal to \\(\\hat{\\beta}\\) in the response \\(y\\)\nOften it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in hdl would also imply a change in total cholesterol chol.\nFurther, our explanation contains no notation of causation.\nWe will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model."
  },
  {
    "objectID": "lm-coeff.html#example-categorical-variable",
    "href": "lm-coeff.html#example-categorical-variable",
    "title": "3  Common cases",
    "section": "3.3 Example: categorical variable",
    "text": "3.3 Example: categorical variable\n\nWe want to compare the average BMI of men and women.\nWe can do that using linear regression and including gender as binary variable\n\n\n\nCode\nfont.size &lt;- 20\ncol.blue.light &lt;- \"#a6cee3\"\ncol.blue.dark &lt;- \"#1f78b4\"\nmy.ggtheme &lt;- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \"top\")\n      \n\n# visualize the data with box plot\ndata_diabetes %&gt;%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \"Set2\") + \n  my.ggtheme\n\n\n\n\n\nModel\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\] for some coding, e.g. we choose to set “Female=1” and “Male=0” or vice versa.\nIn R we write:\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n## [1] \"factor\"\n\n# fit linear regression and print model summary\nm3 &lt;- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n## \n## Call:\n## lm(formula = BMI ~ gender, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -14.167  -4.117  -0.327   3.160  19.273 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   27.7674     0.8527  32.566  &lt; 2e-16 ***\n## genderfemale   3.9396     1.1379   3.462 0.000729 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.437 on 128 degrees of freedom\n## Multiple R-squared:  0.08563,    Adjusted R-squared:  0.07849 \n## F-statistic: 11.99 on 1 and 128 DF,  p-value: 0.0007286\n\nEstimates \\[\\hat{\\alpha} = 27.7674\\] \\[\\hat{\\beta} = 3.9396\\]\n\nThe lm() function chooses automatically one of the category as baseline, here females.\nModel summary prints the output of the model with the baseline category “hidden”.\nNotice that the only label we have is “genderfemale”.\nMeaning that we ended-up having a model coded as below: \\[\\begin{equation}\n  I_{x_i} =\n  \\left\\{\n      \\begin{array}{cc}\n              1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n              0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n      \\end{array}\n  \\right.\n\\end{equation}\\]\nConsequently, if observation \\(i\\) is female then the expected value of BMI is: \\[E(BMI_i|female) = 27.7674 + 3.9396 = 31.707\\]\nand if observation \\(i\\) is male then the expected value of BMI is: \\[E(BMI_i|male) = 27.7674\\] We can plot the model in R:\n\n\nggPredict(m3) + \n  my.ggtheme"
  },
  {
    "objectID": "lm-coeff.html#example-categorical-numerical-variables",
    "href": "lm-coeff.html#example-categorical-numerical-variables",
    "title": "3  Common cases",
    "section": "3.4 Example: categorical & numerical variables",
    "text": "3.4 Example: categorical & numerical variables\n\nAbove we observed a signficant difference in average BMI between men and women among the study participants.\nCan we also observe a significant relationship between BMI and height?\nAnd if so, does this relationship depend on gender?\n\n\n\nCode\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %&gt;%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \"Set2\") + \n  my.ggtheme\n\n\n\n\n\n\nFrom the plot we can see that BMI decreases slightly with height.\nOn average, men are taller than women.\nOn average, women have higher BMI than men.\nThe relationship between height and BMI appears to be the same for males and females, i.e. BMI decreases with height for both men and women.\n\nTo assess the relationship we use a model containing height and gender.\nModel\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the heigth of person \\(i\\).\nIn R we write:\n\n# fit linear model and print model summary\nm4 &lt;- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,    Adjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n\nModel together with estimates\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the weight of person \\(i\\)\nEstimates\n\\[\\hat{\\alpha} = 37.743 \\] \\[\\hat{\\beta} = 3.163\\] \\[\\hat{\\gamma} = -5.719\\]\n\nFor instance, using our estimates, for a female who happens to 1.7 m tall we would predict BMI of: \\[E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837\\]\nand for a male of height 1.7 m tall we would predict BMI of \\[E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207\\]\n\nIn R we can plot our data and the fitted model to verify our calculations:\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \"Set2\") + \n  my.ggtheme"
  },
  {
    "objectID": "lm-coeff.html#example-interactions",
    "href": "lm-coeff.html#example-interactions",
    "title": "3  Common cases",
    "section": "3.5 Example: interactions",
    "text": "3.5 Example: interactions\n\nThe fitted lines in the above example are parallel, the slope is modeled to be the same for men and women, and the intercept denotes the group differences.\nIt is also possible to allow for both intercept and slope being fitted separately for each group.\nThis is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\nAnd we then talk about including interaction effect since the two lines may interact (cross).\n\nModel\n\\[Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}\\] where:\n\n\\(Y_{i,j}\\) is the BMI of person \\(j\\) of gender \\(i\\)\n\\(x_{ij}\\) is the height of person \\(j\\) of gender \\(i\\)\n\\(i=1\\) corresponds to women in our example (keeping the same coding as above)\n\\(i=2\\) corresponds to men\n\nIn R we define the interaction term with *:\n\n# fit linear model with interaction\nm5 &lt;- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n## \n## Call:\n## lm(formula = BMI ~ gender * height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.5564  -4.1137  -0.3072   3.1057  19.2005 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)           31.222     20.318   1.537    0.127\n## genderfemale          14.219     26.032   0.546    0.586\n## height                -1.981     11.638  -0.170    0.865\n## genderfemale:height   -6.558     15.414  -0.425    0.671\n## \n## Residual standard error: 6.469 on 126 degrees of freedom\n## Multiple R-squared:  0.09099,    Adjusted R-squared:  0.06935 \n## F-statistic: 4.204 on 3 and 126 DF,  p-value: 0.007155\n\nNow, based on the regression output we would expect:\n\nfor a woman of height \\(x\\), a BMI value of: \\[E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x\\]\nfor a man of height \\(x\\), a BMI value of \\[E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x\\]\n\nEstimates \\[\\hat{\\alpha_1} = 45.441\\] \\[\\hat{\\beta_1} = 31.222\\]\n\\[\\hat{\\alpha_2} = 47.34778\\] \\[\\hat{\\beta_2} = -1.981\\]\n\nWe can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term “Genderfemale:height” is equal to zero.\nOr therefore conclude that the relationship between BMI and height is different for men and women.\nWe can plot the fitted model and see that the lines are no longer parallel.\n\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \"Set2\") + \n  my.ggtheme"
  },
  {
    "objectID": "lm-coeff.html#example-logistic-regression-with-categorical-variable",
    "href": "lm-coeff.html#example-logistic-regression-with-categorical-variable",
    "title": "3  Common cases",
    "section": "3.6 Example: logistic regression with categorical variable",
    "text": "3.6 Example: logistic regression with categorical variable\n\n# recode diabetic status to 1 and 0\ndata_diabetes &lt;- data_diabetes %&gt;%\n  mutate(obese = ifelse(obese == \"Yes\", 1, 0))\n\n# fit logistic regression using age and gender\nm6 &lt;- glm(obese ~  hdl + gender, family = binomial(link=\"logit\"), data = data_diabetes)\nsummary(m6)\n## \n## Call:\n## glm(formula = obese ~ hdl + gender, family = binomial(link = \"logit\"), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept)   0.55047    0.58718   0.937   0.3485   \n## hdl          -0.02997    0.01197  -2.504   0.0123 * \n## genderfemale  1.26586    0.40120   3.155   0.0016 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 164.39  on 127  degrees of freedom\n## AIC: 170.39\n## \n## Number of Fisher Scoring iterations: 4\n\n\nBy how much change odds of suffering from obesity when hdl increases by 1?13\nWhat are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?14\n\n\nggPredict(m6) + \n  scale_color_brewer(palette = \"Set2\") + \n  my.ggtheme\n\n\n\n\nWe can predict obesity status in R for a man with hdl values of 50:\n\n# define new observation\ndf &lt;- data.frame(hdl = 50, gender = as.factor(\"male\"))\n\n# predict probability of suffering from obesity\nprob_obese &lt;- predict(m6, newdata = df, type = \"response\")\nprint(prob_obese)\n##         1 \n## 0.2792396"
  },
  {
    "objectID": "lm-coeff.html#footnotes",
    "href": "lm-coeff.html#footnotes",
    "title": "3  Common cases",
    "section": "",
    "text": "If the waist increases by 1 cm we would expect our BMI to increase by \\(\\approx 0.35\\) since \\(\\hat{\\beta} = 0.35298\\)↩︎\nIf the waist increases by 10 cm we would expect BMI to increase by \\(0.35298 \\cdot 10 \\approx 3.53\\)↩︎\nNo, as \\(p-value = 0.0633 \\nless 0.05\\)↩︎\nYes, as \\(p-value = 0.0633 &lt; 0.1\\)↩︎\nBMI = -5.12445 + 0.35298 = 21.349↩︎\nBMI = -5.12445 + 0.35298 = 65.47141, however here we have to be careful in predicting outside the model range.↩︎\nIn simple linear regression \\(R^2\\) is the same as \\(r^2\\) and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.↩︎\nThe diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.↩︎\ndecreases by \\(-0.090023 \\cdot 10 = 0.90023\\)↩︎\ndecrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.↩︎\nwe have seen before that in the case of simple linear regression it was enough to test the null hypothesis of \\(H_0: \\beta=0\\) versus \\(H_0: \\beta\\neq0\\) to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of \\[H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\] versus the alternative \\[H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero\\] This hypothesis test is performed by computing F-statistics reported in the model summary and calculated as \\(F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\\) where \\(TSS = \\sum(y_i - \\bar{y})^2\\) and \\(RSS = \\sum(y_i - \\hat{y_i})^2\\). Here, the \\(F-statsitics = 3.046\\) and the associated \\(p-value &lt; 0.05\\) so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.↩︎\ndecreases by ca. 0.06↩︎\nthe odds increase by e^{-0.02997} = 0.97↩︎\nThe odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.↩︎"
  },
  {
    "objectID": "lm-intro-exercises.html",
    "href": "lm-intro-exercises.html",
    "title": "Exercises (introduction to linear models)",
    "section": "",
    "text": "Exercise 1 (Fitting linear model) Going back to the diabetes data, fit linear regression models using vector-matrix notations to model BMI based on age [years] and waist [m] measurements. In particular, define design matrix \\(\\mathbf{X}\\), vector of observations \\(\\mathbf{Y}\\) and vector of parameters \\(\\boldsymbol\\beta\\) and use \\[\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\] to find beta values estimates.\nCheck your calculations by fitting the model using lm() function.\nThe below code can get you started.\n\n\ninch2m &lt;- 2.54/100\npound2kg &lt;- 0.45\ndata_diabetes &lt;- diabetes %&gt;%\n  mutate(height  = height * inch2m, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2m) %&gt;%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %&gt;% \n  mutate(diabetic = ifelse(glyhb &gt; 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %&gt;%\n  na.omit()\n\n\nExercise 2 (Hypothesis testing) Your colleague Anna is interested in association between BMI and cholesterol, both total cholesterol (chol) and high density lipoprotein fraction (hdl). She has correctly fitted linear model using lm() function but her computer broke and she only has the below output:\n\n# Coefficients:\n#               Estimate Std. Error t value Pr(&gt;|t|)\n# (Intercept)  3.471e+01  2.940e+00  11.808  &lt; 2e-16 ***\n# chol         1.965e-05  1.231e-02\n# hdl         -9.371e-02  3.220e-02\nCan you help Anna finding out whether chol and hdl are significantly associated with BMI? What are the t-value statistics and the corresponding p-values? Calculate these values without fitting the model and then fit the model to double check your calculations.\n\nExercise 3 (Evaluate model fit) After helping Anna you got interested in whether your initial model containing age and waist is a better fit to the data than Anna’s model containing chol and hdl. Evaluate model fit by calculating \\(R^2(adj)\\) based on the equation:\n\\[R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}\\] where\n\n\\(p\\) is the number of independent predictors, i.e. the number of variables in the model, excluding the constant and \\(n\\) is the number of observations.\n\nCheck your calculations using lm() function.\nHint: If reg holds a fitted linear regression model, you can assess the estimated BMI values by accessing reg$fitted.values and residuals via reg$residuals.\n\n\nSolution. Exercise 1\n\ninch2m &lt;- 2.54/100\npound2kg &lt;- 0.45\ndata_diabetes &lt;- diabetes %&gt;%\n  mutate(height  = height * inch2m, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2m) %&gt;%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %&gt;% \n  mutate(diabetic = ifelse(glyhb &gt; 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %&gt;%\n  na.omit()\n\n# define Y\nY &lt;- data_diabetes %&gt;% select(\"BMI\") %&gt;% as.matrix()\n\n# define X\nX &lt;- cbind(rep(1, nrow(data_diabetes)), data_diabetes$age, data_diabetes$waist)\nX &lt;- as.matrix(X)\n\n# alternatively we case use model.matrix() to define X\nX &lt;- model.matrix(~ age + waist, data = data_diabetes)\n\n# least square estimate\nbeta.hat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n##                     BMI\n## (Intercept) -2.39192911\n## age         -0.05716479\n## waist       35.46856117\n\n# check calculations using lm() function\nreg_1 &lt;- lm(BMI ~ age + waist, data = data_diabetes)\nsummary(reg_1)\n## \n## Call:\n## lm(formula = BMI ~ age + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.6952  -2.8803  -0.5864   2.2229  18.7443 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -2.39193    2.95646  -0.809   0.4200    \n## age         -0.05716    0.02551  -2.241   0.0267 *  \n## waist       35.46856    2.68192  13.225   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.358 on 127 degrees of freedom\n## Multiple R-squared:  0.5841, Adjusted R-squared:  0.5776 \n## F-statistic: 89.19 on 2 and 127 DF,  p-value: &lt; 2.2e-16\n\n\n\nSolution. Exercise 2\nUnder the null hypothesis \\(H_0: \\beta = 0\\) \n\n\\(n\\) is number of observations\n\\(p\\) is number of model parameters\n\\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}\\) is the ratio of the departure of the estimated value of a parameter, \\(\\hat\\beta\\), from its hypothesized value, \\(\\beta\\), to its standard error, called t-statistics\nthe t-statistics follows Student’s t distribution with \\(n-p\\) degrees of freedom\n\nThis means that to check if the there is an association between chol and BMI we check if there is enough evidence to reject the null hypothesis of \\(H_0: \\beta = 0\\). Here, t-statistics equals to \\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{1.965\\times 10^{05} - 0}{1.231\\times 10^{05}} = 0.001596263\\) and a corresponding p-values can be found:\n\n2*pt(0.001596263, df=130 - 3, lower.tail = F)\n## [1] 0.9987289\n\nAssuming 5% significance level, we do not have enough evidence to reject the null hypothesis in favor of the alternative as p-value is large \\(p = 0.99873 &gt; 0.05\\). This means we do not observe association between chol and BMI.\nAnalogously, for age we have t-statistics equal to \\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{-9.371\\times 10^{02} - 0}{3.220\\times 10^{02}} = -2.910248\\) and a corresponding p-value equals to:\n\n2*pt(-2.910248, df=130 - 3, lower.tail = T)\n## [1] 0.004265831\n\nHere, p-value is small and we can thus reject the null hypothesis in favour of the alternative and conclude that there is an association between hdl and BMI.\nTo check our calculations we can re-fit the linear model and print the entire summary.\n\nreg_2 &lt;- lm(BMI ~ chol + hdl, data = data_diabetes)\nsummary(reg_2)\n## \n## Call:\n## lm(formula = BMI ~ chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.8952  -4.8988  -0.9225   3.0629  21.7951 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.471e+01  2.940e+00  11.808  &lt; 2e-16 ***\n## chol         1.965e-05  1.231e-02   0.002  0.99873    \n## hdl         -9.371e-02  3.220e-02  -2.910  0.00426 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.538 on 127 degrees of freedom\n## Multiple R-squared:  0.0643, Adjusted R-squared:  0.04956 \n## F-statistic: 4.363 on 2 and 127 DF,  p-value: 0.0147\n\n\n\nSolution. Exercise 3\n\nn &lt;- nrow(data_diabetes)\np &lt;- 2 # beta for age and beta for waist (model 1)\np &lt;- 2 # beta for chol and beta for hdl (model 2)\n\n# calculate TSS\nTSS &lt;- sum((data_diabetes$BMI - mean(data_diabetes$BMI))^2)\n\n# calculate RSS and R2_adj (model 1)\n# model BMI ~ age + waist \nreg_1 &lt;- lm(BMI ~ age + waist, data = data_diabetes)\nreg &lt;- reg_1\nRSS &lt;- sum((reg$residuals)^2)\nR2_adj &lt;- 1 - (RSS/(n-p-1))/(TSS/(n-1))\nprint(R2_adj)\n## [1] 0.5775851\n\n# calculate RSS and R2_adj (model 2)\n# model BMI ~ chol + hdl\nreg_2 &lt;- lm(BMI ~ chol + hdl, data = data_diabetes)\nreg &lt;- reg_2\nRSS &lt;- sum((reg$residuals)^2)\nR2_adj &lt;- 1 - (RSS/(n-p-1))/(TSS/(n-1))\nprint(R2_adj)\n## [1] 0.04956166\n\n\n# check calculations\nsummary(reg_1)\n## \n## Call:\n## lm(formula = BMI ~ age + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.6952  -2.8803  -0.5864   2.2229  18.7443 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -2.39193    2.95646  -0.809   0.4200    \n## age         -0.05716    0.02551  -2.241   0.0267 *  \n## waist       35.46856    2.68192  13.225   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.358 on 127 degrees of freedom\n## Multiple R-squared:  0.5841, Adjusted R-squared:  0.5776 \n## F-statistic: 89.19 on 2 and 127 DF,  p-value: &lt; 2.2e-16\nsummary(reg_2)\n## \n## Call:\n## lm(formula = BMI ~ chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.8952  -4.8988  -0.9225   3.0629  21.7951 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.471e+01  2.940e+00  11.808  &lt; 2e-16 ***\n## chol         1.965e-05  1.231e-02   0.002  0.99873    \n## hdl         -9.371e-02  3.220e-02  -2.910  0.00426 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.538 on 127 degrees of freedom\n## Multiple R-squared:  0.0643, Adjusted R-squared:  0.04956 \n## F-statistic: 4.363 on 2 and 127 DF,  p-value: 0.0147\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "lm-diagn-exercises.html#answers-to-selected-exercises",
    "href": "lm-diagn-exercises.html#answers-to-selected-exercises",
    "title": "Exercises (model diagnostics)",
    "section": "Answers to selected exercises",
    "text": "Answers to selected exercises\n\nSolution. Exercise 1\n\n\n# access and preview data\ndata(fat, package = \"faraway\")\nhead(fat)\n##   brozek siri density age weight height adipos  free neck chest abdom   hip\n## 1   12.6 12.3  1.0708  23 154.25  67.75   23.7 134.9 36.2  93.1  85.2  94.5\n## 2    6.9  6.1  1.0853  22 173.25  72.25   23.4 161.3 38.5  93.6  83.0  98.7\n## 3   24.6 25.3  1.0414  22 154.00  66.25   24.7 116.0 34.0  95.8  87.9  99.2\n## 4   10.9 10.4  1.0751  26 184.75  72.25   24.9 164.7 37.4 101.8  86.4 101.2\n## 5   27.8 28.7  1.0340  24 184.25  71.25   25.6 133.1 34.4  97.3 100.0 101.9\n## 6   20.6 20.9  1.0502  24 210.25  74.75   26.5 167.0 39.0 104.5  94.4 107.8\n##   thigh knee ankle biceps forearm wrist\n## 1  59.0 37.3  21.9   32.0    27.4  17.1\n## 2  58.7 37.3  23.4   30.5    28.9  18.2\n## 3  59.6 38.9  24.0   28.8    25.2  16.6\n## 4  60.1 37.3  22.8   32.4    29.4  18.2\n## 5  63.2 42.2  24.0   32.2    27.7  17.7\n## 6  66.0 42.0  25.6   35.7    30.6  18.8\n\n# fit linear regression model\nmodel.all &lt;- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n\n# print model summary\nprint(summary(model.all))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  &lt; 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: &lt; 2.2e-16\n\n# diagnostics plots\npar(mfrow=c(2,2))\nplot(model.all)\n\n# remove potentially influential observations\nobs &lt;- c(86)\nfat2 &lt;- fat[-obs, ]\n\n# re-fit the model\nmodel.clean &lt;- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n\n# diagnostics plots\npar(mfrow=c(2,2))\nplot(model.clean)\n\n\n\n\n\n\n\n\n# model summary\nprint(summary(model.clean))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  &lt; 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no height)\nmodel.red1 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\nprint(summary(model.red1))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2830  -2.6162  -0.1017   2.8789   9.3713 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -22.66569   11.97691  -1.892  0.05963 .  \n## age           0.05948    0.02954   2.013  0.04521 *  \n## weight       -0.09829    0.04114  -2.389  0.01765 *  \n## neck         -0.43444    0.21445  -2.026  0.04389 *  \n## abdom         0.88762    0.06839  12.979  &lt; 2e-16 ***\n## hip          -0.17180    0.12919  -1.330  0.18483    \n## thigh         0.25327    0.12960   1.954  0.05183 .  \n## knee         -0.02318    0.22128  -0.105  0.91665    \n## ankle         0.17300    0.20411   0.848  0.39752    \n## biceps        0.15695    0.15715   0.999  0.31894    \n## forearm       0.43091    0.18239   2.363  0.01895 *  \n## wrist        -1.51011    0.49120  -3.074  0.00235 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.976 on 240 degrees of freedom\n## Multiple R-squared:  0.7484, Adjusted R-squared:  0.7369 \n## F-statistic:  64.9 on 11 and 240 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no knee)\nmodel.red2 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + ankle + biceps + forearm + wrist, data = fat)\nprint(summary(model.red2))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2552  -2.5979  -0.1133   2.8693   9.3584 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -23.08716   11.25781  -2.051  0.04137 *  \n## age           0.05875    0.02864   2.051  0.04134 *  \n## weight       -0.09965    0.03897  -2.557  0.01117 *  \n## neck         -0.43088    0.21131  -2.039  0.04253 *  \n## abdom         0.88875    0.06740  13.186  &lt; 2e-16 ***\n## hip          -0.17231    0.12884  -1.337  0.18234    \n## thigh         0.24942    0.12403   2.011  0.04544 *  \n## ankle         0.16946    0.20089   0.844  0.39974    \n## biceps        0.15847    0.15616   1.015  0.31123    \n## forearm       0.42946    0.18150   2.366  0.01876 *  \n## wrist        -1.51470    0.48823  -3.102  0.00215 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.968 on 241 degrees of freedom\n## Multiple R-squared:  0.7484, Adjusted R-squared:  0.738 \n## F-statistic: 71.69 on 10 and 241 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no ankle)\nmodel.red3 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh  + biceps + forearm + wrist, data = fat)\nprint(summary(model.red3))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0740  -2.5615  -0.1021   2.7999   9.3199 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -20.61247   10.86240  -1.898   0.0589 .  \n## age           0.05727    0.02857   2.004   0.0461 *  \n## weight       -0.09141    0.03770  -2.424   0.0161 *  \n## neck         -0.45458    0.20931  -2.172   0.0308 *  \n## abdom         0.88098    0.06673  13.203   &lt;2e-16 ***\n## hip          -0.17575    0.12870  -1.366   0.1733    \n## thigh         0.25504    0.12378   2.061   0.0404 *  \n## biceps        0.15178    0.15587   0.974   0.3311    \n## forearm       0.42805    0.18138   2.360   0.0191 *  \n## wrist        -1.40948    0.47175  -2.988   0.0031 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.965 on 242 degrees of freedom\n## Multiple R-squared:  0.7477, Adjusted R-squared:  0.7383 \n## F-statistic: 79.67 on 9 and 242 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no biceps)\nmodel.red4 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh  + forearm + wrist, data = fat)\nprint(summary(model.red4))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0574  -2.7411  -0.1912   2.6929   9.4977 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -20.06213   10.84654  -1.850  0.06558 .  \n## age           0.05922    0.02850   2.078  0.03876 *  \n## weight       -0.08414    0.03695  -2.277  0.02366 *  \n## neck         -0.43189    0.20799  -2.077  0.03889 *  \n## abdom         0.87721    0.06661  13.170  &lt; 2e-16 ***\n## hip          -0.18641    0.12821  -1.454  0.14727    \n## thigh         0.28644    0.11949   2.397  0.01727 *  \n## forearm       0.48255    0.17251   2.797  0.00557 ** \n## wrist        -1.40487    0.47167  -2.978  0.00319 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.965 on 243 degrees of freedom\n## Multiple R-squared:  0.7467, Adjusted R-squared:  0.7383 \n## F-statistic: 89.53 on 8 and 243 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no hip)\nmodel.red5 &lt;- lm(brozek ~ age + weight + neck + abdom  + thigh  + forearm + wrist, data = fat)\nprint(summary(model.red5))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + \n##     wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0193  -2.8016  -0.1234   2.9387   9.0019 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -30.17420    8.34200  -3.617 0.000362 ***\n## age           0.06149    0.02852   2.156 0.032047 *  \n## weight       -0.11236    0.03151  -3.565 0.000437 ***\n## neck         -0.37203    0.20434  -1.821 0.069876 .  \n## abdom         0.85152    0.06437  13.229  &lt; 2e-16 ***\n## thigh         0.20973    0.10745   1.952 0.052099 .  \n## forearm       0.51824    0.17115   3.028 0.002726 ** \n## wrist        -1.40081    0.47274  -2.963 0.003346 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.974 on 244 degrees of freedom\n## Multiple R-squared:  0.7445, Adjusted R-squared:  0.7371 \n## F-statistic: 101.6 on 7 and 244 DF,  p-value: &lt; 2.2e-16\n\n# compare model.clean and final model\nprint(summary(model.clean))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  &lt; 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: &lt; 2.2e-16\nprint(summary(model.red5))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + \n##     wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0193  -2.8016  -0.1234   2.9387   9.0019 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -30.17420    8.34200  -3.617 0.000362 ***\n## age           0.06149    0.02852   2.156 0.032047 *  \n## weight       -0.11236    0.03151  -3.565 0.000437 ***\n## neck         -0.37203    0.20434  -1.821 0.069876 .  \n## abdom         0.85152    0.06437  13.229  &lt; 2e-16 ***\n## thigh         0.20973    0.10745   1.952 0.052099 .  \n## forearm       0.51824    0.17115   3.028 0.002726 ** \n## wrist        -1.40081    0.47274  -2.963 0.003346 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.974 on 244 degrees of freedom\n## Multiple R-squared:  0.7445, Adjusted R-squared:  0.7371 \n## F-statistic: 101.6 on 7 and 244 DF,  p-value: &lt; 2.2e-16\n\nNote: we have just run a very simple feature selection using stepwise regression. In this method, using backward elimination, we build a model containing all the variables and remove them one by one based on defined criteria (here we have used p-values) and we stop when we have a justifiable model or when removing a predictor does not change the chosen criterion significantly."
  },
  {
    "objectID": "lm-coeff-exercises.html#answers-to-selected-exercises",
    "href": "lm-coeff-exercises.html#answers-to-selected-exercises",
    "title": "Exercises (regression coefficients)",
    "section": "Answers to selected exercises",
    "text": "Answers to selected exercises\n\nSolution. Exercise 1\n\n\n\n\n\nhtwtgen &lt;- read.csv(\"data/lm/heights_weights_genders.csv\")\nhead(htwtgen)\n\n  Gender   Height   Weight\n1   Male 73.84702 241.8936\n2   Male 68.78190 162.3105\n3   Male 74.11011 212.7409\n4   Male 71.73098 220.0425\n5   Male 69.88180 206.3498\n6   Male 67.25302 152.2122\n\n# a)\nmodel1 &lt;- lm(Height ~ Gender, data = htwtgen)\nmodel2 &lt;- lm(Height ~ Gender + Weight, data = htwtgen)\nmodel3 &lt;- lm(Height ~ Gender * Weight, data = htwtgen)\n\n# print(summary(model1))\n# print(summary(model2))\n# print(summary(model3))\n\n\nuse equations to find the height for men and women respectively: \\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\] \\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\]\n\n\n\n# for men\nnew.obs &lt;- data.frame(Weight=120, Gender=\"Male\")\npredict(model3, newdata = new.obs)\n\n       1 \n60.65427 \n\n# for female\nnew.obs &lt;- data.frame(Weight=120, Gender=\"Female\")\npredict(model3, newdata = new.obs)\n\n       1 \n61.79882 \n\n\n\nSolution. Exercise 2\n\n\n# read in data and show preview\ntrout &lt;- read.csv(\"data/lm/trout.csv\")\n\n# recode the Group variable and treat like categories (factor)\ntrout$Group &lt;- factor(trout$Group, labels=c(\"Dominant\", \"Subordinate\"))\nhead(trout)\n##   Energy Ration    Group\n## 1  44.26  81.35 Dominant\n## 2  67.16  91.68 Dominant\n## 3  48.15  58.00 Dominant\n## 4  34.53  58.63 Dominant\n## 5  67.93  91.93 Dominant\n## 6  72.45  96.56 Dominant\n\n# plot data\n# boxplots of Energy and Ration per group\nboxplot(trout$Energy ~ trout$Group, xlab=\"\", ylab=\"Energy\")\n\n\n\nboxplot(trout$Ration ~ trout$Group, xlab=\"\", ylab=\"Ration\")\n\n\n\n\n# scatter plot of Ration vs. Energy\nplot(trout$Ration, trout$Energy, pch=19, xlab=\"Ration\", ylab=\"Energy\")\n\n\n\n\n\nFrom the exploratory plots we see that there is some sort of relationship between ratio and energy, i.e. energy increase while ration obtained increases\nFrom box plots we see that the ration obtained may be different in two groups\n\n\n# Is there a relationship between ration obtained and energy expenditure\nmodel1 &lt;- lm(Energy ~ Ration, data = trout)\nprint(summary(model1))\n## \n## Call:\n## lm(formula = Energy ~ Ration, data = trout)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.704  -4.703  -0.578   2.432  33.506 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   4.3037    12.5156   0.344 0.734930    \n## Ration        0.7211     0.1716   4.203 0.000535 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.05 on 18 degrees of freedom\n## Multiple R-squared:  0.4953, Adjusted R-squared:  0.4673 \n## F-statistic: 17.66 on 1 and 18 DF,  p-value: 0.0005348\n# from the regression output we can see that yes, a unit increase in ratio increase energy expenditure by 0.72\n\n# Is there a relationship between ration obtained and energy expenditure different for each type of fish?\n# we first check if there is a group effect\nmodel2 &lt;- lm(Energy ~ Ration + Group, data = trout)\nprint(summary(model2))\n## \n## Call:\n## lm(formula = Energy ~ Ration + Group, data = trout)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.130  -5.139  -0.870   2.199  25.622 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      -24.8506    13.3031  -1.868  0.07910 .  \n## Ration             1.0109     0.1626   6.218 9.36e-06 ***\n## GroupSubordinate  17.0120     5.1075   3.331  0.00396 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.647 on 17 degrees of freedom\n## Multiple R-squared:  0.6946, Adjusted R-squared:  0.6587 \n## F-statistic: 19.33 on 2 and 17 DF,  p-value: 4.182e-05\nggPredict(model2) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))\n\n\n\n\n# and whether there is an interaction effect\nmodel3 &lt;- lm(Energy ~ Ration * Group, data = trout)\nprint(summary(model3))\n## \n## Call:\n## lm(formula = Energy ~ Ration * Group, data = trout)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.7951  -6.0981  -0.1554   3.9612  23.5946 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)              -9.2330    15.9394  -0.579 0.570483    \n## Ration                    0.8149     0.1968   4.141 0.000767 ***\n## GroupSubordinate        -18.9558    22.6934  -0.835 0.415848    \n## Ration:GroupSubordinate   0.5200     0.3204   1.623 0.124148    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.214 on 16 degrees of freedom\n## Multiple R-squared:  0.7378, Adjusted R-squared:  0.6886 \n## F-statistic:    15 on 3 and 16 DF,  p-value: 6.537e-05\nggPredict(model3) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))\n\n\n\n\nBased on the regression output and plots we can say:\n\nthere is a relationship between ration obtained and energy expenditure\nthat this relationship is the same in the two groups although the energy expenditure is higher in the dominant fish\n\n\nSolution. Exercise 3\n\n\n\n\nYes. The redn and initial were significantly associated (p-value = 0.00312, linear regression).\n\nmodel1 &lt;- lm(redn ~ initial, data = blooddrug)\nsummary(model1)\n## \n## Call:\n## lm(formula = redn ~ initial, data = blooddrug)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -23.476 -11.705   1.558   9.197  24.392 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) -72.7302    29.1879  -2.492  0.02036 * \n## initial       0.5902     0.1788   3.301  0.00312 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.79 on 23 degrees of freedom\n## Multiple R-squared:  0.3214, Adjusted R-squared:  0.2919 \n## F-statistic: 10.89 on 1 and 23 DF,  p-value: 0.003125\n\n\n\n\nNo. The drug2 and drug3 were not significantly different from drug1 (p-value = 0.714 and p-value = 0.628, respectively). The patients of the drug 1 group had 2.750 higher blood pressure drop (redn) than those of the drug 2 group. However, the difference was relatively small comparing to the standard error of the estimate, which was 7.402. The difference between drug 1 and 3 was relatively small, too.\n\nmodel2 &lt;- lm(redn ~ drug, data = blooddrug)\nsummary(model2)\n## \n## Call:\n## lm(formula = redn ~ drug, data = blooddrug)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -32.000  -9.286   0.000  12.714  26.000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   23.250      5.517   4.214 0.000358 ***\n## drug2          2.750      7.402   0.372 0.713796    \n## drug3         -3.964      8.076  -0.491 0.628379    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 15.6 on 22 degrees of freedom\n## Multiple R-squared:  0.03349,    Adjusted R-squared:  -0.05437 \n## F-statistic: 0.3812 on 2 and 22 DF,  p-value: 0.6875\n\n\n\n\nYes. The redn of the drug2 group was significantly higher than that of the drug1 group after adjustment for the effects of the initial (P = 0.018). The reduction of the patients who got the drug 2 was much higher (13.6906) than the drug 1, comparing to the standard error of the difference (5.3534) after accounting for initial blood pressure.\n\nmodel3 &lt;- lm(redn ~ drug + initial, data = blooddrug)\nsummary(model3)\n## \n## Call:\n## lm(formula = redn ~ drug + initial, data = blooddrug)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.8114 -10.5842  -0.4959   6.2834  16.4265 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -124.8488    28.0674  -4.448 0.000223 ***\n## drug2         13.6906     5.3534   2.557 0.018346 *  \n## drug3         -7.2045     5.4275  -1.327 0.198625    \n## initial        0.8895     0.1671   5.323 2.81e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.42 on 21 degrees of freedom\n## Multiple R-squared:  0.5886, Adjusted R-squared:  0.5298 \n## F-statistic: 10.01 on 3 and 21 DF,  p-value: 0.0002666"
  }
]