[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to linear models",
    "section": "",
    "text": "Linear models allows us to answer questions such as:\n\nis there a relationship between exposure and outcome, e.g. body weight and plasma volume?\nhow strong is the relationship between the two variables?\nwhat will be a predicted value of the outcome given a new set of exposure values?\nhow accurately can we predict outcome?\nwhich variables are associated with the response, e.g. is it body weight and height that can explain the plasma volume or is it just the body weight?\n\nLearning outcomes\n\nto understand what a linear model is and be familiar with the terminology\nto be able to state linear model in the general vector-matrix notation\nto be able to use the general vector-matrix notation to numerically estimate model parameters\nto be able to use lm() function for model fitting, parameter estimation, hypothesis testing and prediction\n\nDo you see a mistake or a typo? I would be grateful if you let me know via olga.dethlefsen@nbis.se\nThis repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/"
  },
  {
    "objectID": "lm-intro.html#why-linear-models",
    "href": "lm-intro.html#why-linear-models",
    "title": "1  Introduction to linear models",
    "section": "1.1 Why linear models?",
    "text": "1.1 Why linear models?\nWith linear models we can answer questions such as:\n\nis there a relationship between exposure and outcome, e.g. body weight and plasma volume?\nhow strong is the relationship between the two variables?\nwhat will be a predicted value of the outcome given a new set of exposure values?\nhow accurately can we predict outcome?\nwhich variables are associated with the response, e.g. is it body weight and height that can explain the plasma volume or is it just the body weight?"
  },
  {
    "objectID": "lm-intro.html#statistical-vs.-deterministic-relationship",
    "href": "lm-intro.html#statistical-vs.-deterministic-relationship",
    "title": "1  Introduction to linear models",
    "section": "1.2 Statistical vs. deterministic relationship",
    "text": "1.2 Statistical vs. deterministic relationship\nRelationships in probability and statistics can generally be one of three things: deterministic, random, or statistical:\n\na deterministic relationship involves an exact relationship between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation \\(Fahrenheit=\\frac{9}{5}\\cdot Celcius+32\\)\nthere is no relationship between variables in the random relationship, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year\na statistical relationship is a mixture of deterministic and random relationship, e.g. the savings that Olga has left in the bank account depend on Olga’s monthly salary income (deterministic part) and the money spent on buying succulents (random part)\n\n\n\n\n\n\nFigure 1.1: Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between \\(x\\) and \\(y\\) is not perfect (increasing relationship), c) statistical relationship between \\(x\\) and \\(y\\) is not perfect (decreasing relationship), d) random signal"
  },
  {
    "objectID": "lm-intro.html#what-linear-models-are-and-are-not",
    "href": "lm-intro.html#what-linear-models-are-and-are-not",
    "title": "1  Introduction to linear models",
    "section": "1.3 What linear models are and are not",
    "text": "1.3 What linear models are and are not\n\nA linear model is one in which the parameters appear linearly in the deterministic part of the model\ne.g. simple linear regression through the origin is a simple linear model of the form \\[Y_i = \\beta x + \\epsilon\\] often used to express a relationship of one numerical variable to another, e.g. the calories burnt and the kilometers cycled\nlinear models can become quite advanced by including many variables, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the transformation of the variables, e.g. a function of kilometers cycled squared\n\nMore examples where model parameters appear linearly:\n\n\n\n\n\n\nFigure 1.2: Example of a linear model: \\(y_i = x_i^2 + e_i\\) showing that linear models can capture more than a straight line relationship\n\n\n\n\n\\(Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i\\)\n\\(Y_i = \\alpha + \\beta x_i^2 \\epsilon\\)\n\\(Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon\\)\n\\(Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon\\)\n\nand an example on a non-linear model where parameter \\(\\beta\\) appears in the exponent of \\(x_i\\)\n\n\\(Y_i = \\alpha + x_i^\\beta + \\epsilon\\)"
  },
  {
    "objectID": "lm-intro.html#terminology",
    "href": "lm-intro.html#terminology",
    "title": "1  Introduction to linear models",
    "section": "1.4 Terminology",
    "text": "1.4 Terminology\nThere are many terms and notations used interchangeably:\n\n\\(y\\) is being called:\n\nresponse\noutcome\ndependent variable\n\n\\(x\\) is being called:\n\nexposure\nexplanatory variable\ndependent variable\npredictor\ncovariate"
  },
  {
    "objectID": "lm-intro.html#simple-linear-regression",
    "href": "lm-intro.html#simple-linear-regression",
    "title": "1  Introduction to linear models",
    "section": "1.5 Simple linear regression",
    "text": "1.5 Simple linear regression\n\nIt is used to check the association between the numerical outcome and one numerical explanatory variable\nIn practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n\n\nExample 1.1 (Weight and plasma volume) Let’s look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\nExample data:\n\n\nCode\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n\n\n\n\n\n\n\nFigure 1.3: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca.\n\n\n\n\n\n\n\n\n\nFigure 1.4: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable\n\n\n\n\nThe equation for the red line is:\n\\[Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8\\]\nand in general:\n\\[Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n\\]\n\nIn other words, by finding the best-fitting straight line we are building a statistical model to represent the relationship between plasma volume (\\(Y\\)) and explanatory body weight variable (\\(x\\))\nIf we were to use our model \\(Y_i=0.086 + 0.044 \\cdot x_i\\) to find plasma volume given a weight of 58 kg (our first observation, \\(i=1\\)), we would notice that we would get \\(Y=0.086 + 0.044 \\cdot 58 = 2.638\\), not exactly \\(2.75\\) as we have for our first man in our dataset that we started with, i.e. \\(2.75 - 2.638 = 0.112 \\neq 0\\).\nWe thus add to the above equation an error term to account for this and now we can write our simple regression model more formally as:\n\n\\[Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i \\tag{1.1}\\] where:\n\n\\(x\\): is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n\\(y\\): is called: response, outcome, dependent variable\n\\(\\alpha\\) and \\(\\beta\\) are model coefficients\nand \\(\\epsilon_i\\) is an error terms"
  },
  {
    "objectID": "lm-intro.html#least-squares",
    "href": "lm-intro.html#least-squares",
    "title": "1  Introduction to linear models",
    "section": "1.6 Least squares",
    "text": "1.6 Least squares\n\nin the above “body weight - plasma volume” example, the values of \\(\\alpha\\) and \\(\\beta\\) have just appeared\nin practice, \\(\\alpha\\) and \\(\\beta\\) values are unknown and we use data to estimate these coefficients, noting the estimates with a hat, \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\nleast squares is one of the methods of parameters estimation, i.e. finding \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\n\n\n\n\n\n\nFigure 1.5: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line\n\n\n\n\n\nLet \\(\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i\\) be the prediction \\(y_i\\) based on the \\(i\\)-th value of \\(x\\):\n\nThen \\(\\epsilon_i = y_i - \\hat{y_i}\\) represents the \\(i\\)-th residual, i.e. the difference between the \\(i\\)-th observed response value and the \\(i\\)-th response value that is predicted by the linear model\nRSS, the residual sum of squares is defined as: \\[RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2\\] or equivalently as: \\[RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2\\]\nthe least squares approach chooses \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) to minimize the RSS. With some calculus, a good video explanation for the interested ones is here, we get Theorem 1.1\n\n\nTheorem 1.1 (Least squares estimates for a simple linear regression) \\[\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}\\]\n\\[\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}\\]\nwhere:\n\n\\(\\bar{x}\\): mean value of \\(x\\)\n\\(\\bar{y}\\): mean value of \\(y\\)\n\\(S_{xx}\\): sum of squares of \\(X\\) defined as \\(S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2\\)\n\\(S_{yy}\\): sum of squares of \\(Y\\) defined as \\(S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2\\)\n\\(S_{xy}\\): sum of products of \\(X\\) and \\(Y\\) defined as \\(S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})\\)\n\n\nWe can further re-write the above sum of squares to obtain\n\nsum of squares of \\(X\\), \\[S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})\\]\nsum of products of \\(X\\) and \\(Y\\)\n\n\\[S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}\\]\n\nExample 1.2 (Least squares) Let’s try least squares method to find coefficient estimates in the “body weight and plasma volume example”\n\n# initial data\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# rename variables for convenience\nx <- weight\ny <- plasma\n\n# mean values of x and y\nx.bar <- mean(x)\ny.bar <- mean(y)\n\n# Sum of squares\nSxx <-  sum((x - x.bar)^2)\nSxy <- sum((x-x.bar)*(y-y.bar))\n\n# Coefficient estimates\nbeta.hat <- Sxy / Sxx\nalpha.hat <- y.bar - Sxy/Sxx*x.bar\n\n# Print estimated coefficients alpha and beta\nprint(alpha.hat)\n\n[1] 0.08572428\n\nprint(beta.hat)\n\n[1] 0.04361534\n\n\n\nIn R we can use lm(), the built-in function, to fit a linear regression model and we can replace the above code with one line\n\nlm(plasma ~ weight)\n\n\nCall:\nlm(formula = plasma ~ weight)\n\nCoefficients:\n(Intercept)       weight  \n    0.08572      0.04362"
  },
  {
    "objectID": "lm-intro.html#intercept-and-slope",
    "href": "lm-intro.html#intercept-and-slope",
    "title": "1  Introduction to linear models",
    "section": "1.7 Intercept and Slope",
    "text": "1.7 Intercept and Slope\n\nLinear regression gives us estimates of model coefficient \\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\)\n\\(\\alpha\\) is known as the intercept\n\\(\\beta\\) is known as the slope\n\n\n\n\n\n\nFigure 1.6: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)"
  },
  {
    "objectID": "lm-intro.html#hypothesis-testing",
    "href": "lm-intro.html#hypothesis-testing",
    "title": "1  Introduction to linear models",
    "section": "1.8 Hypothesis testing",
    "text": "1.8 Hypothesis testing\nIs there a relationship between the response and the predictor?\n\nthe calculated \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are estimates of the population values of the intercept and slope and are therefore subject to sampling variation\ntheir precision is measured by their estimated standard errors, e.s.e(\\(\\hat{\\alpha}\\)) and e.s.e(\\(\\hat{\\beta}\\))\nthese estimated standard errors are used in hypothesis testing and in constructing confidence and prediction intervals\n\nThe most common hypothesis test involves testing the null hypothesis of:\n\n\\(H_0:\\) There is no relationship between \\(X\\) and \\(Y\\)\nversus the alternative hypothesis \\(H_a:\\) there is some relationship between \\(X\\) and \\(Y\\)\n\nMathematically, this corresponds to testing:\n\n\\(H_0: \\beta=0\\)\nversus \\(H_a: \\beta\\neq0\\)\nsince if \\(\\beta=0\\) then the model \\(Y_i=\\alpha+\\beta x_i + \\epsilon_i\\) reduces to \\(Y=\\alpha + \\epsilon_i\\)\n\nUnder the null hypothesis \\(H_0: \\beta = 0\\)  \n\n\\(n\\) is number of observations\n\\(p\\) is number of model parameters\n\\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}\\) is the ratio of the departure of the estimated value of a parameter, \\(\\hat\\beta\\), from its hypothesized value, \\(\\beta\\), to its standard error, called t-statistics\nthe t-statistics follows Student’s t distribution with \\(n-p\\) degrees of freedom\n\n\nExample 1.3 (Hypothesis testing) Let’s look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the R summary of the model\n\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,    Adjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n\n\n\n\nUnder Estimate we see estimates of our model coefficients, \\(\\hat{\\alpha}\\) (intercept) and \\(\\hat{\\beta}\\) (slope, here weight), followed by their estimated standard errors, Std. Errors\nIf we were to test if there is an association between weight and plasma volume we would write under the null hypothesis \\(H_0: \\beta = 0\\) \\[\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582\\]\nand we would compare t-statistics to Student's t distribution with \\(n-p = 8 - 2 = 6\\) degrees of freedom (as we have 8 observations and two model parameters, \\(\\alpha\\) and \\(\\beta\\))\nwe can use Student’s t distribution table or R code to obtain the associated P-value\n\n\n2*pt(2.856582, df=6, lower=F)\n\n[1] 0.02893095\n\n\n\nhere the observed t-statistics is large and therefore yields a small P-value, meaning that there is sufficient evidence to reject null hypothesis in favor of the alternative and conclude that there is a significant association between weight and plasma volume"
  },
  {
    "objectID": "lm-intro.html#vector-matrix-notations",
    "href": "lm-intro.html#vector-matrix-notations",
    "title": "1  Introduction to linear models",
    "section": "1.9 Vector-matrix notations",
    "text": "1.9 Vector-matrix notations\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of multiple regression, with more than one explanatory variable in the model, it is more efficient to use vectors and matrices to define the regression model.\nLet’s rewrite our simple linear regression model \\(Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n\\) into vector-matrix notation in 6 steps.\n\nFirst we rename our \\(\\alpha\\) to \\(\\beta_0\\) and \\(\\beta\\) to \\(\\beta_1\\) as it is easier to keep tracking the number of model parameters this way\nThen we notice that we actually have \\(n\\) equations such as:\n\n\\[y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1\\]\n\\[y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2\\]\n\\[y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3\\]\n\\[\\dots\\]\n\\[y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n\\]\n\nWe group all \\(Y_i\\) and \\(\\epsilon_i\\) into column vectors: \\(\\mathbf{Y}=\\begin{bmatrix}  y_1 \\\\  y_2 \\\\  \\vdots \\\\  y_{n} \\end{bmatrix}\\) and \\(\\boldsymbol\\epsilon=\\begin{bmatrix}  \\epsilon_1 \\\\  \\epsilon_2 \\\\  \\vdots \\\\  \\epsilon_{n} \\end{bmatrix}\\)\nWe stack two parameters \\(\\beta_0\\) and \\(\\beta_1\\) into another column vector:\\[\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}\\]\nWe append a vector of ones with the single predictor for each \\(i\\) and create a matrix with two columns called design matrix \\[\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}\\]\nWe write our linear model in a vector-matrix notations as:\n\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\]\n\nDefinition 1.1 (vector matrix form of the linear model) The vector-matrix representation of a linear model with \\(p-1\\) predictors can be written as\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\]\nwhere:\n\n\\(\\mathbf{Y}\\) is \\(n \\times1\\) vector of observations\n\\(\\mathbf{X}\\) is \\(n \\times p\\) design matrix\n\\(\\boldsymbol\\beta\\) is \\(p \\times1\\) vector of parameters\n\\(\\boldsymbol\\epsilon\\) is \\(n \\times1\\) vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, \\(\\sigma^2\\))\n\nIn full, the above vectors and matrix have the form:\n\\(\\mathbf{Y}=\\begin{bmatrix}  y_1 \\\\  y_2 \\\\  \\vdots \\\\  y_{n} \\end{bmatrix}\\) \\(\\boldsymbol\\beta=\\begin{bmatrix}  \\beta_0 \\\\  \\beta_1 \\\\  \\vdots \\\\  \\beta_{p} \\end{bmatrix}\\) \\(\\boldsymbol\\epsilon=\\begin{bmatrix}  \\epsilon_1 \\\\  \\epsilon_2 \\\\  \\vdots \\\\  \\epsilon_{n} \\end{bmatrix}\\) \\(\\mathbf{X}=\\begin{bmatrix}  1 & x_{1,1} & \\dots & x_{1,p-1} \\\\  1 & x_{2,1} & \\dots & x_{2,p-1} \\\\  \\vdots & \\vdots & \\vdots & \\vdots \\\\  1 & x_{n,1} & \\dots & x_{n,p-1} \\end{bmatrix}\\)\n\n\nTheorem 1.2 (Least squares in vector-matrix notation) The least squares estimates for a linear regression of the form:\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\]\nis given by:\n\\[\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\]\n\n\nExample 1.4 (vector-matrix-notation) Following the above definition we can write the “weight - plasma volume model” as:\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\]\nwhere:\n\\(\\mathbf{Y}=\\begin{bmatrix}  2.75 \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12 \\end{bmatrix}\\)\n\\(\\boldsymbol\\beta=\\begin{bmatrix}  \\beta_0 \\\\  \\beta_1 \\end{bmatrix}\\) \\(\\boldsymbol\\epsilon=\\begin{bmatrix}  \\epsilon_1 \\\\  \\epsilon_2 \\\\  \\vdots \\\\  \\epsilon_{8} \\end{bmatrix}\\) \\(\\mathbf{X}=\\begin{bmatrix}  1 & 58.0 \\\\  1 & 70.0 \\\\  1 & 74.0 \\\\  1 & 63.5 \\\\  1 & 62.0 \\\\  1 & 70.5 \\\\  1 & 71.0 \\\\  1 & 66.0 \\\\ \\end{bmatrix}\\)\nand we can estimate model parameters using \\(\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\).\nWe can do it by hand or in R as follows:\n\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\n\n     [,1]\n[1,] 2.75\n[2,] 2.86\n[3,] 3.37\n[4,] 2.76\n[5,] 2.62\n[6,] 3.49\n[7,] 3.05\n[8,] 3.12\n\nprint(X)\n\n       weight\n[1,] 1   58.0\n[2,] 1   70.0\n[3,] 1   74.0\n[4,] 1   63.5\n[5,] 1   62.0\n[6,] 1   70.5\n[7,] 1   71.0\n[8,] 1   66.0\n\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n\n             [,1]\n       0.08572428\nweight 0.04361534"
  },
  {
    "objectID": "lm-intro.html#confidence-intervals-and-prediction-intervals",
    "href": "lm-intro.html#confidence-intervals-and-prediction-intervals",
    "title": "1  Introduction to linear models",
    "section": "1.10 Confidence intervals and prediction intervals",
    "text": "1.10 Confidence intervals and prediction intervals\n\nwhen we estimate coefficients we can also find their confidence intervals, typically 95% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter\nwe can also use linear regression models to predict the response value given a new observation and find prediction intervals. Here, we look at any specific value of \\(x_i\\), and find an interval around the predicted value \\(y_i'\\) for \\(x_i\\) such that there is a 95% probability that the real value of y (in the population) corresponding to \\(x_i\\) is within this interval\n\n::: {exm-prediction-and-intervals} ## prediction and intervals\nLet’s:\n\nfind confidence intervals for our coefficient estimates\npredict plasma volume for a men weighting 60 kg\nfind prediction interval\nplot original data, fitted regression model, predicted observation and prediction interval\n\n\n# fit regression model\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,    Adjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n\n# find confidence intervals for the model coefficients\nconfint(model)\n\n                   2.5 %     97.5 %\n(Intercept) -2.419908594 2.59135716\nweight       0.006255005 0.08097567\n\n# predict plasma volume for a new observation of 60 kg\n# we have to create data frame with a variable name matching the one used to build the model\nnew.obs <- data.frame(weight = 60)\npredict(model, newdata = new.obs)\n\n       1 \n2.702645 \n\n# find prediction intervals\nprediction.interval <- predict(model, newdata = new.obs,  interval = \"prediction\")\nprint(prediction.interval)\n\n       fit      lwr      upr\n1 2.702645 2.079373 3.325916\n\n# plot the original data, fitted regression and predicted value\nplot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\", ylim=c(2,4))\nlines(weight, model$fitted.values, col=\"red\") # fitted model in red\npoints(new.obs, predict(model, newdata = new.obs), pch=19, col=\"blue\") # predicted value at 60kg\nsegments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval"
  },
  {
    "objectID": "lm-intro-exercises.html",
    "href": "lm-intro-exercises.html",
    "title": "Exercises (introduction to linear models)",
    "section": "",
    "text": "Data for exercises are on Canvas under Files -> data_exercises –> linear-models"
  },
  {
    "objectID": "lm-intro-exercises.html#answers-to-selected-exercises",
    "href": "lm-intro-exercises.html#answers-to-selected-exercises",
    "title": "Exercises (introduction to linear models)",
    "section": "Answers to selected exercises",
    "text": "Answers to selected exercises\n\nSolution. Exercise 1\n\n\n\n\n\n\\(S_{xx} = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n} = 12164 - \\frac{456^2}{19} = 1220\\)\n\\(S_{xy} = \\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n} = 369.87 - \\frac{(456 \\cdot 14.25)}{19} = 27.87\\)\n\\(\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}} = 27.87 / 1220 = 0.02284\\)\n\\(\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x} = \\frac{14.25}{19}-\\frac{27.87}{1220}\\cdot \\frac{456}{19} = 0.20174\\)\n\n\n\n\n\n\nWe can calculate test statistics following:\n\n\\(\\frac{\\hat{\\beta} - \\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p) = \\frac{0.02284 - 0}{0.003295} = 6.934\\) where the value follows Student’s t distribution with \\(n-p = 19 - 2 = 17\\) degrees of freedom. We can now estimate the a p-value using Student’s t distribution table or use R function\n\n\n2*pt(6.934, df=17, lower=F)\n\n[1] 2.414315e-06\n\n\nAs p-value << 0.001 there is sufficient evidence to reject \\(H_0\\) in favor of \\(H_1\\), thus we can conclude that there is a significant relationship between protein levels and gestation\n\n\n\n\n\nSimilarly, we can test \\(H_0:\\beta = 0.02\\), i.e. \\(\\frac{\\hat{\\beta} - \\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p) = \\frac{0.02284 - 0.02}{0.20174} = 0.01407753\\). Now the test statistics is small\n\n2*pt(0.01407753, df=17, lower=F)\n\n[1] 0.988932\n\n\np-value is large and hence there is no sufficient evidence to reject \\(H_0\\) and we can conclude that \\(\\beta = 0.02\\)\n\nWe can rewrite the linear model in vector-matrix formation as \\(\\mathbf{Y}= \\mathbf{\\beta}\\mathbf{X} + \\mathbf{\\epsilon}\\) where:\n\nresponse \\(\\mathbf{Y}=\\begin{bmatrix}  y_1 \\\\  y_2 \\\\  \\vdots \\\\  y_{19} \\end{bmatrix}\\)\nparameters \\(\\boldsymbol\\beta=\\begin{bmatrix}  \\alpha \\\\  \\beta \\end{bmatrix}\\)\ndesign matrix \\(\\mathbf{X}=\\begin{bmatrix}  1 & x_1 \\\\  1 & x_2 \\\\  \\vdots & \\vdots \\\\  1 & x_{19} \\end{bmatrix}\\)\nerrors \\(\\boldsymbol\\epsilon=\\begin{bmatrix}  \\epsilon_1 \\\\  \\epsilon_2 \\\\  \\vdots \\\\  \\epsilon_{19} \\end{bmatrix}\\)\n\nThe least squares estimates in vector-matrix notation is \\(\\hat{\\boldsymbol\\beta}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and we can calculate this in R\n\n\n# read in data\ndata.protein <- read.csv(\"data/lm/protein.csv\")\n\n# print out top observations\nhead(data.protein)\n\n  Protein Gestation\n1    0.38        11\n2    0.58        12\n3    0.51        13\n4    0.38        15\n5    0.58        17\n6    0.67        18\n\n# define Y and X matrices given the data\nn <- nrow(data.protein) # nu. of observations\nY <-  as.matrix(data.protein$Protein, ncol=1) # response\nX <-  as.matrix(cbind(rep(1, length=n), data.protein$Gestation)) # design matrix\nhead(X) # double check that the design matrix looks like it should\n\n     [,1] [,2]\n[1,]    1   11\n[2,]    1   12\n[3,]    1   13\n[4,]    1   15\n[5,]    1   17\n[6,]    1   18\n\n# least squares estimate\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y # beta.hat is a matrix that contains our alpha and beta in the model\nprint(beta.hat)\n\n           [,1]\n[1,] 0.20173770\n[2,] 0.02284426\n\n\n\nWe use lm() function to check our calculations\n\n\n# fit linear regression model and print model summary\nprotein <- data.protein$Protein # our Y\ngestation <- data.protein$Gestation # our X\n\nmodel <- lm(protein ~ gestation)\nprint(summary(model))\n\n\nCall:\nlm(formula = protein ~ gestation)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.16853 -0.08720 -0.01009  0.08578  0.20422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.201738   0.083363   2.420    0.027 *  \ngestation   0.022844   0.003295   6.934 2.42e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1151 on 17 degrees of freedom\nMultiple R-squared:  0.7388,    Adjusted R-squared:  0.7234 \nF-statistic: 48.08 on 1 and 17 DF,  p-value: 2.416e-06\n\n\n\n\n\n\nnew.obs <- data.frame(gestation = 20)\ny.pred <- predict(model, newdata = new.obs)\n\n# we can visualize the data, fitted linear model (red), and the predicted value (blue)\nplot(gestation, protein, pch=19, xlab=\"gestation [weeks]\", ylab=\"protein levels [mgml-1]\")\nlines(gestation, model$fitted.values, col=\"red\")\npoints(new.obs, y.pred, col=\"blue\", pch=19, cex = 1)\n\n\n\n\n\nSolution. Exercise 2\n\n\nWe can rewrite the linear model in vector-matrix formation as \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\]\n\nwhere: response \\(\\mathbf{Y}=\\begin{bmatrix}  y_1 \\\\  y_2 \\\\  \\vdots \\\\  y_{14} \\end{bmatrix}\\)\nparameters \\(\\boldsymbol\\beta=\\begin{bmatrix}  \\alpha \\\\  \\beta \\\\  \\gamma \\end{bmatrix}\\)\ndesign matrix \\(\\mathbf{X}=\\begin{bmatrix}  1 & x_1 & x_1^2\\\\  1 & x_2 & x_2^2\\\\  \\vdots & \\vdots & \\vdots \\\\  1 & x_{14} & x_{14}^2 \\end{bmatrix}\\)\nerrors \\(\\boldsymbol\\epsilon=\\begin{bmatrix}  \\epsilon_1 \\\\  \\epsilon_2 \\\\  \\vdots \\\\  \\epsilon_{14} \\end{bmatrix}\\)\n\nload data to from “potatoes.csv” and use least squares estimates for obtain estimates of model coefficients\n\n\ndata.potatoes <- read.csv(\"data/lm/potatoes.csv\")\n\n# define matrices\nn <- nrow(data.potatoes)\nY <-  data.potatoes$Glucose\nX1 <- data.potatoes$Weeks\nX2 <- (data.potatoes$Weeks)^2\nX <- cbind(rep(1, length(n)), X1, X2)\nX <- as.matrix(X)\n\n# least squares estimate\n# beta here refers to the matrix of model coefficients incl. alpha, beta and gamma\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n\n         [,1]\n   200.169312\nX1 -19.443122\nX2   1.030423\n\n\n\nwe use lm() function to verify our calculations:\n\n\nmodel <- lm(Y ~ X1 + X2)\nprint(summary(model))\n\n\nCall:\nlm(formula = Y ~ X1 + X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.405 -11.250  -8.071  12.911  29.286 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 200.1693    15.0527  13.298 4.02e-08 ***\nX1          -19.4431     3.1780  -6.118 7.54e-05 ***\nX2            1.0304     0.1406   7.329 1.49e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.4 on 11 degrees of freedom\nMultiple R-squared:  0.8694,    Adjusted R-squared:  0.8457 \nF-statistic: 36.61 on 2 and 11 DF,  p-value: 1.373e-05\n\n\n\nperform a hypothesis test to test \\(H_0:\\gamma=0\\); and comment whether we there is a significant quadratic term\n\n\n\\(\\frac{\\hat{\\gamma} - \\gamma}{e.s.e(\\hat{\\gamma})} \\sim t(n-p) = \\frac{1.030423 - 0}{0.1406} = 7.328755\\) where the value follows Student’s t distribution with \\(n-p = 19 - 2 = 17\\) degrees of freedom. We can now estimate the a p-value using Student’s t distribution table or use a function in R\n\n\n2*pt(7.328755, df=14-3, lower=F)\n\n[1] 1.487682e-05\n\n\nAs p-value << 0.001 there is sufficient evidence to reject \\(H_0\\) in favor of \\(H_1\\), thus we can conclude that there is a significant quadratic relationship between glucose and storage time\n\npredict glucose concentration at storage time 4 and 16 weeks\n\n\nnew.obs <- data.frame(X1 = c(4, 16), X2 = c(4^2, 16^2))\npred.y <- predict(model, newdata = new.obs)\n\nplot(data.potatoes$Weeks, data.potatoes$Glucose, xlab=\"Storage time [weeks]\", ylab=\"Glucose [g/kg]\", pch=19)\nlines(data.potatoes$Weeks, model$fitted.values, col=\"red\")\npoints(new.obs[,1], pred.y, pch=19, col=\"blue\")"
  },
  {
    "objectID": "lm-coeff.html",
    "href": "lm-coeff.html",
    "title": "2  Regression coefficients",
    "section": "",
    "text": "Aims\nLearning outcomes"
  },
  {
    "objectID": "lm-coeff.html#interpreting-and-using-linear-regression-models",
    "href": "lm-coeff.html#interpreting-and-using-linear-regression-models",
    "title": "2  Regression coefficients",
    "section": "2.1 Interpreting and using linear regression models",
    "text": "2.1 Interpreting and using linear regression models\n\nIn previous section we have seen how to find estimates of model coefficients, using theorems and vector-matrix notations.\nNow, we will focus on what model coefficient values tell us and how to interpret them\nAnd we will look at the common cases of using linear regression models\nWe will do this via analyzing some examples"
  },
  {
    "objectID": "lm-coeff.html#example-plasma-volume",
    "href": "lm-coeff.html#example-plasma-volume",
    "title": "2  Regression coefficients",
    "section": "2.2 Example: Plasma volume",
    "text": "2.2 Example: Plasma volume\n\n# data\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# fit regression model\nmodel <- lm(plasma ~ weight)\n\n# plot the original data and fitted regression line\nplot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\")\nlines(weight, model$fitted.values, col=\"red\") # fitted model in red\ngrid()\n\n# print model summary\nprint(summary(model))\n## \n## Call:\n## lm(formula = plasma ~ weight)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.27880 -0.14178 -0.01928  0.13986  0.32939 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)  0.08572    1.02400   0.084   0.9360  \n## weight       0.04362    0.01527   2.857   0.0289 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2188 on 6 degrees of freedom\n## Multiple R-squared:  0.5763, Adjusted R-squared:  0.5057 \n## F-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n\n\n\n\n\nFigure 2.1: Scatter plot showing plasma volume for each weight\n\n\n\nModel:\n\n\\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\) where \\(x_i\\) corresponds to \\(weight_i\\)\n\nSlope\n\nThe value of slope tells us how and by much the outcome changes with a unit change in \\(x\\)\nIf we go up in weight 1 kg what would be our expected change in plasma volume\\(^1\\)?\nAnd if we go up in weight 10 kg what would be our expected change in plasma volume\\(^2\\)?\n\nIntercept\n\nthe intercept, often labeled the constant, is the value of Y when \\(x_i=0\\)\nin models where \\(x_i\\) can be equal 0, the intercept is simply the expected mean value of response\nin models where \\(x_i\\) cannot be equal 0, like in our plasma example no weight makes no sense for healthy men, the intercept has no intrinsic meaning\nthe intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome\n\n\n\n\\(^1\\): If we go up in weight 1 kg we would expect our plasma volume to increase by 0.04 liter since \\(\\hat{\\beta} = 0.04\\)\n\\(^2\\) If we go up in weight 10 kg we would expect our plasma volume to increase by \\(0.04 \\cdot 10 = 0.4\\) liter"
  },
  {
    "objectID": "lm-coeff.html#example-galapagos-islands",
    "href": "lm-coeff.html#example-galapagos-islands",
    "title": "2  Regression coefficients",
    "section": "2.3 Example: Galapagos Islands",
    "text": "2.3 Example: Galapagos Islands\nResearchers were interested in biological diversity on the Galapagos islands. They have collected data on number of plant species (Species) and number of endemic species on 30 islands as well as some descriptors of the islands such as area [\\(\\mathrm{km^2}\\)], elevation [m], distance to nearest island [km], distance to Santa Cruz [km] and the area of the adjacent island [\\(\\mathrm{km^2}\\)].\nThe preview of data is here:\n\n\nCode\n# data is available via faraway package\nif(!require(faraway)){\n    install.packages(\"faraway\")\n    library(faraway)\n}\n\nhead(gala, 10) %>%\n  kable() %>%\n  kable_paper(\"hover\")\n\n\n\n\n\n \n  \n      \n    Species \n    Endemics \n    Area \n    Elevation \n    Nearest \n    Scruz \n    Adjacent \n  \n \n\n  \n    Baltra \n    58 \n    23 \n    25.09 \n    346 \n    0.6 \n    0.6 \n    1.84 \n  \n  \n    Bartolome \n    31 \n    21 \n    1.24 \n    109 \n    0.6 \n    26.3 \n    572.33 \n  \n  \n    Caldwell \n    3 \n    3 \n    0.21 \n    114 \n    2.8 \n    58.7 \n    0.78 \n  \n  \n    Champion \n    25 \n    9 \n    0.10 \n    46 \n    1.9 \n    47.4 \n    0.18 \n  \n  \n    Coamano \n    2 \n    1 \n    0.05 \n    77 \n    1.9 \n    1.9 \n    903.82 \n  \n  \n    Daphne.Major \n    18 \n    11 \n    0.34 \n    119 \n    8.0 \n    8.0 \n    1.84 \n  \n  \n    Daphne.Minor \n    24 \n    0 \n    0.08 \n    93 \n    6.0 \n    12.0 \n    0.34 \n  \n  \n    Darwin \n    10 \n    7 \n    2.33 \n    168 \n    34.1 \n    290.2 \n    2.85 \n  \n  \n    Eden \n    8 \n    4 \n    0.03 \n    71 \n    0.4 \n    0.4 \n    17.95 \n  \n  \n    Enderby \n    2 \n    2 \n    0.18 \n    112 \n    2.6 \n    50.2 \n    0.10 \n  \n\n\n\nTable 2.1:  Preview of the Galapagos Islands data \n\n\nAnd we can fit a linear regression model to model number of Species given the remaining variables. Let’s keep aside for now that number of Species is actually a count variable, not a continuous numerical variable, we just want to estimate the number of Species for now.\nFitted Model\n\n\\(Y_i = \\beta_0 + \\beta_1 Area_i + \\beta_2 Elevation_i + \\beta_3 Nearest_i + \\beta_4 Scruz_i + \\beta_5 Adjacent_i + \\epsilon_i\\)\n\n\n# fit multiple linear regression and print model summary\nmodel1 <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala)\nprint(summary(model1))\n## \n## Call:\n## lm(formula = Species ~ Area + Elevation + Nearest + Scruz + Adjacent, \n##     data = gala)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -111.679  -34.898   -7.862   33.460  182.584 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  7.068221  19.154198   0.369 0.715351    \n## Area        -0.023938   0.022422  -1.068 0.296318    \n## Elevation    0.319465   0.053663   5.953 3.82e-06 ***\n## Nearest      0.009144   1.054136   0.009 0.993151    \n## Scruz       -0.240524   0.215402  -1.117 0.275208    \n## Adjacent    -0.074805   0.017700  -4.226 0.000297 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 60.98 on 24 degrees of freedom\n## Multiple R-squared:  0.7658, Adjusted R-squared:  0.7171 \n## F-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07\n\nUsing the model compare two islands in terms of number of species\n\nif the second island has an elevation 1 m higher than the first one?\\(^1\\)\nif the second island has an elevation 100 m higher than the first one?\\(^2\\)\nif the second island is 100 km closer to Santa Cruz?\\(^3\\)\noverall, is there a relationship between the response \\(Y\\) (Species) and predictors?\\(^4\\)\n\n\n\n\\(^1\\) the second island will have 0.32 species more than the first one, \\(\\hat{\\beta_2} = 0.319465 \\approx 0.32\\)\n\\(^2\\) the second island will have \\(0.32 \\cdot 100 = 32\\) more species than the first one\n\\(^3\\) the second island would have \\(-0.24 \\cdot 100 = -24\\) less species than the first if there was enough evidence to reject the null hypothesis of \\(\\beta_4 = 0\\); It is not appropriate to try to interpret non-significant coefficients.\n\\(^4\\) we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of \\(H_0: \\beta=0\\) versus \\(H_0: \\beta\\neq0\\) to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of \\[H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\] versus the alternative \\[H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero\\] This hypothesis test is performed by computing F-statistics reported in the model summary and calculated as \\(F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\\) where \\(TSS = \\sum(y_i - \\bar{y})^2\\) and \\(RSS = \\sum(y_i - \\hat{y_i})^2\\). Here, the \\(F-statsitics = 15.7\\) and the associated \\(p-value < 0.05\\) so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (Species) and predictors.\n\n Not so easy: alternative model\nConsider an alternative model where we only use elevation to model the number of species\n\\[Y_i = \\beta_0 + \\beta_1 Elevation_i + \\epsilon_i\\]\nWe fit the model in R and look at the model summary\n\nmodel2 <- lm(Species ~ Elevation, data = gala)\nprint(summary(model2))\n## \n## Call:\n## lm(formula = Species ~ Elevation, data = gala)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -218.319  -30.721  -14.690    4.634  259.180 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 11.33511   19.20529   0.590     0.56    \n## Elevation    0.20079    0.03465   5.795 3.18e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 78.66 on 28 degrees of freedom\n## Multiple R-squared:  0.5454, Adjusted R-squared:  0.5291 \n## F-statistic: 33.59 on 1 and 28 DF,  p-value: 3.177e-06\n\nUsing the alternative model compare again two islands in terms of number of species\n\nif the second island has an elevation 1 m higher than the first one?\\(^1\\)\nif the second island has an elevation 100 m higher than the first one?\\(^2\\)\n\n\n\n\\(^1\\) the second island will have 0.20 species more than the first one\n\\(^2\\) the second island will have \\(0.20 \\cdot 100 = 20\\) more species\n\nSpecific interpretation\n\nObviously there is difference between 32 and 20 times more species given the same elevation difference as obtained by the multiple regression (first model) and simple regression (alternative model).\nOur interpretations need to be more specific and we say that a unit increase in \\(x\\) with other predictors held constant will produce a change equal to \\(\\hat{\\beta}\\) in the response \\(y\\)\nIt is of course often quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in evaluation is most likely associated with other variables, even though they are not included in the model.\nFurther, our explanation contains no notation of causation, even though the two models are showing a strong association between elevation and number of species.\nWe will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model."
  },
  {
    "objectID": "lm-coeff.html#example-height-and-gender",
    "href": "lm-coeff.html#example-height-and-gender",
    "title": "2  Regression coefficients",
    "section": "2.4 Example: Height and gender",
    "text": "2.4 Example: Height and gender\nData are available containing the weight [lbs] and height [inches] of 10000 men and women\n\n# read in data\nhtwtgen <- read.csv(\"data/lm/heights_weights_genders.csv\")\nhead(htwtgen)\n##   Gender   Height   Weight\n## 1   Male 73.84702 241.8936\n## 2   Male 68.78190 162.3105\n## 3   Male 74.11011 212.7409\n## 4   Male 71.73098 220.0425\n## 5   Male 69.88180 206.3498\n## 6   Male 67.25302 152.2122\n\n# boxplot for females and males\nboxplot(htwtgen$Height ~ htwtgen$Gender, \n        xlab=\"\", ylab=\"Height\", col=\"lightblue\")\n\n\n\n\nFigure 2.2: Box plot for 10 000 heigth measurments stratifed by gender\n\n\n\n\n\nWe want to compare the average height of men and women.\nWe can do that using linear regression and including gender as binary variable\n\nModel\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i\\]\nwhere \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\] for some coding, e.g. we choose to set “Female=1” and “Male=0” or vice versa.\nIn R we write:\n\n# Note: check that Gender is indeed non-numeric\nprint(class(htwtgen$Gender))\n## [1] \"character\"\n\n# fit linear regression and print model summary\nmodel1 <- lm(Height ~ Gender, data = htwtgen)\nprint(summary(model1))\n## \n## Call:\n## lm(formula = Height ~ Gender, data = htwtgen)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.6194  -1.8374   0.0088   1.9185   9.9724 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 63.70877    0.03933  1619.8   <2e-16 ***\n## GenderMale   5.31757    0.05562    95.6   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.781 on 9998 degrees of freedom\n## Multiple R-squared:  0.4776, Adjusted R-squared:  0.4775 \n## F-statistic:  9140 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nEstimates\n\\[\\hat{\\alpha} = 63.71\\]\n\\[\\hat{\\beta} = 5.32\\]\n\nthe lm() function chooses automatically one of the category as baseline, here Females\nmodel summary prints the output of the model with the baseline category “hidden”\ni.e. notice the only label we have is “GenderMale”\nmeaning that we ended-up having a model coded as below: \\[\\begin{equation}\n  I_{x_i} =\n  \\left\\{\n      \\begin{array}{cc}\n              1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n              0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n      \\end{array}\n  \\right.\n\\end{equation}\\]\nConsequently, if observation \\(i\\) is male then the expected value of height is:\n\n\\[E(Height_i|Male) = 63.71 + 5.32 = 69.03\\]\n\nand if observation \\(i\\) is female then the expected value of height is:\n\n\\[E(Height_i|Male) = 63.71\\]"
  },
  {
    "objectID": "lm-coeff.html#example-height-weight-and-gender-i",
    "href": "lm-coeff.html#example-height-weight-and-gender-i",
    "title": "2  Regression coefficients",
    "section": "2.5 Example: Height, weight and gender I",
    "text": "2.5 Example: Height, weight and gender I\n\nSo as expected, there is a difference in average height between men and women.\nCan we also observe a significant relationship between weight and height?\nAnd if so, does this relationship depend on gender?\n\n\n\nCode\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing height measurments given weight, stratified by gender\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\nggplot(data=htwtgen, aes(x = Weight, y=Height, col = Gender)) +\n  geom_point(alpha = 0.5) +\n  theme_light() \n\n\n\n\n\n\nFrom the plot we can see that height increases with weight.\nOn average, men are taller than women.\nOn average, men weight more than women.\nThe relationship between height and weight appears to be the same for males and females, i.e. height increases with weight for both men and women.\n\nTo assess the relationship we use a model containing height and gender.\nModel\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\]\nwhere \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the weight of person \\(i\\)\nIn R we write:\n\n# fit linear model and print model summary\nmodel2 <- lm(Height ~ Gender + Weight, data = htwtgen)\nprint(summary(model2))\n## \n## Call:\n## lm(formula = Height ~ Gender + Weight, data = htwtgen)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.4956 -0.9583  0.0126  0.9867  5.8358 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 47.0306678  0.1025161  458.76   <2e-16 ***\n## GenderMale  -0.9628643  0.0474947  -20.27   <2e-16 ***\n## Weight       0.1227594  0.0007396  165.97   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.435 on 9997 degrees of freedom\n## Multiple R-squared:  0.8609, Adjusted R-squared:  0.8609 \n## F-statistic: 3.093e+04 on 2 and 9997 DF,  p-value: < 2.2e-16\n\nModel together with estimates\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\]\nwhere \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the weight of person \\(i\\)\nEstimates\n\\[\\hat{\\alpha} = 47.031\\]\n\\[\\hat{\\beta} = -0.963\\]\n\\[\\hat{\\gamma} = 0.123\\]\n\nUsing our estimates, for a male of with an example weight of 161.4 we would predict a height of:\n\n\\[E(Height_i|Male, Weight = 161.4) = 47.031 - 0.963 + (0.123 \\cdot 161.4) = 65.9\\]\n\nand for a female of weight 161.4 we would predict a height of\n\n\\[E(Height_i|Female, Weight = 161.4) = 47.031 + (0.123 \\cdot 161.4) = 66.9\\]\nIn R we can plot our data and the fitted moded to verify our calculations:\n\n\nCode\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggplot(data=htwtgen, aes(x = Weight, y=Height, col = Gender)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method=lm) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))"
  },
  {
    "objectID": "lm-coeff.html#example-heigth-weight-and-gender-ii",
    "href": "lm-coeff.html#example-heigth-weight-and-gender-ii",
    "title": "2  Regression coefficients",
    "section": "2.6 Example: Heigth, weight and gender II",
    "text": "2.6 Example: Heigth, weight and gender II\n\nThe fitted lines in the above example are parallel, the slope is modeled to be the same for men and women, and the intercept denotes the group differences\nIt is also possible to allow both intercept and slope being fitted separately for each group\nThis is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\nAnd we then talk about including interaction effect, as the two lines may interact (cross).\n\nModel\n\\[Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}\\]\nwhere:\n\n\\(Y_{i,j}\\) is the height of person \\(j\\) of gender \\(i\\)\n\\(x_{ij}\\) is the weight of person \\(j\\) of gender \\(i\\)\n\\(i=1\\) corresponds to men in our example (keeping the same coding as above)\n\\(i=2\\) corresponds to women\n\nIn R we define the interaction term with *:\n\n# fit linear model with interaction\nmodel3 <- lm(Height ~ Gender * Weight, data = htwtgen)\nprint(summary(model3))\n## \n## Call:\n## lm(formula = Height ~ Gender * Weight, data = htwtgen)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.4698 -0.9568  0.0092  0.9818  5.7544 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       47.347783   0.146325 323.579  < 2e-16 ***\n## GenderMale        -1.683668   0.242119  -6.954 3.78e-12 ***\n## Weight             0.120425   0.001067 112.903  < 2e-16 ***\n## GenderMale:Weight  0.004493   0.001480   3.036   0.0024 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.435 on 9996 degrees of freedom\n## Multiple R-squared:  0.861,  Adjusted R-squared:  0.861 \n## F-statistic: 2.064e+04 on 3 and 9996 DF,  p-value: < 2.2e-16\n\nNow, based on the regression output we would expect:\n\nfor a men of weight \\(x\\), a height of:\n\n\\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\]\n\nfor a women of weight \\(x\\), a height of \\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\]\n\nEstimates\n\\[\\hat{\\alpha_1} = 45.7\\]\n\\[\\hat{\\beta_1} = 0.125\\]\n\\[\\hat{\\alpha_2} = 47.34778\\]\n\\[\\hat{\\beta_2} = 0.12043\\]\n\nWe can see from the regression output that the interaction term, “GenderMale:Weight, is significant\nand therefore the relationship between weight and height is different for men and women.\nWe can plot the fitted model and see that the lines are no longer parallel.\nWe will see clearer example of the interactions in the exercises.\n\n\n\nCode\n# ggiraphExtra makes it easy to visualize fitted models\nif(!require(ggiraphExtra)){\n    install.packages(\"ggiraphExtra\")\n    library(ggiraphExtra)\n}\n\nggPredict(model3) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))"
  },
  {
    "objectID": "lm-coeff-exercises.html",
    "href": "lm-coeff-exercises.html",
    "title": "3  Exercises (Regression coefficients)",
    "section": "",
    "text": "Data for exercises are on Canvas under Files -> data_exercises –> linear-models"
  },
  {
    "objectID": "lm-coeff-exercises.html#answers-to-selected-exercises",
    "href": "lm-coeff-exercises.html#answers-to-selected-exercises",
    "title": "3  Exercises (Regression coefficients)",
    "section": "3.1 Answers to selected exercises",
    "text": "3.1 Answers to selected exercises\n\nSolution. Exercise 3.1\n\n\n\n\n\nhtwtgen <- read.csv(\"data/lm/heights_weights_genders.csv\")\nhead(htwtgen)\n\n  Gender   Height   Weight\n1   Male 73.84702 241.8936\n2   Male 68.78190 162.3105\n3   Male 74.11011 212.7409\n4   Male 71.73098 220.0425\n5   Male 69.88180 206.3498\n6   Male 67.25302 152.2122\n\n# a)\nmodel1 <- lm(Height ~ Gender, data = htwtgen)\nmodel2 <- lm(Height ~ Gender + Weight, data = htwtgen)\nmodel3 <- lm(Height ~ Gender * Weight, data = htwtgen)\n\n# print(summary(model1))\n# print(summary(model2))\n# print(summary(model3))\n\n\nuse equations to find the height for men and women respectively:\n\n\\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\]\n\\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\]\n\n\n\n\n# for men\nnew.obs <- data.frame(Weight=120, Gender=\"Male\")\npredict(model3, newdata = new.obs)\n\n       1 \n60.65427 \n\n# for female\nnew.obs <- data.frame(Weight=120, Gender=\"Female\")\npredict(model3, newdata = new.obs)\n\n       1 \n61.79882 \n\n\n\nSolution. Exercise 3.2\n\n\n# read in data and show preview\ntrout <- read.csv(\"data/lm/trout.csv\")\n\n# recode the Group variable and treat like categories (factor)\ntrout$Group <- factor(trout$Group, labels=c(\"Dominant\", \"Subordinate\"))\nhead(trout)\n##   Energy Ration    Group\n## 1  44.26  81.35 Dominant\n## 2  67.16  91.68 Dominant\n## 3  48.15  58.00 Dominant\n## 4  34.53  58.63 Dominant\n## 5  67.93  91.93 Dominant\n## 6  72.45  96.56 Dominant\n\n# plot data\n# boxplots of Energy and Ration per group\nboxplot(trout$Energy ~ trout$Group, xlab=\"\", ylab=\"Energy\")\n\n\n\nboxplot(trout$Ration ~ trout$Group, xlab=\"\", ylab=\"Ration\")\n\n\n\n\n# scatter plot of Ration vs. Energy\nplot(trout$Ration, trout$Energy, pch=19, xlab=\"Ration\", ylab=\"Energy\")\n\n\n\n\n\nFrom the exploratory plots we see that there is some sort of relationship between ratio and energy, i.e. energy increase while ration obtained increases\nFrom box plots we see that the ration obtained may be different in two groups\n\n\n# Is there a relationship between ration obtained and energy expenditure\nmodel1 <- lm(Energy ~ Ration, data = trout)\nprint(summary(model1))\n## \n## Call:\n## lm(formula = Energy ~ Ration, data = trout)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.704  -4.703  -0.578   2.432  33.506 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   4.3037    12.5156   0.344 0.734930    \n## Ration        0.7211     0.1716   4.203 0.000535 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.05 on 18 degrees of freedom\n## Multiple R-squared:  0.4953, Adjusted R-squared:  0.4673 \n## F-statistic: 17.66 on 1 and 18 DF,  p-value: 0.0005348\n# from the regression output we can see that yes, a unit increase in ratio increase energy expenditure by 0.72\n\n# Is there a relationship between ration obtained and energy expenditure different for each type of fish?\n# we first check if there is a group effect\nmodel2 <- lm(Energy ~ Ration + Group, data = trout)\nprint(summary(model2))\n## \n## Call:\n## lm(formula = Energy ~ Ration + Group, data = trout)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.130  -5.139  -0.870   2.199  25.622 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)      -24.8506    13.3031  -1.868  0.07910 .  \n## Ration             1.0109     0.1626   6.218 9.36e-06 ***\n## GroupSubordinate  17.0120     5.1075   3.331  0.00396 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.647 on 17 degrees of freedom\n## Multiple R-squared:  0.6946, Adjusted R-squared:  0.6587 \n## F-statistic: 19.33 on 2 and 17 DF,  p-value: 4.182e-05\nggPredict(model2) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))\n\n\n\n\n# and whether there is an interaction effect\nmodel3 <- lm(Energy ~ Ration * Group, data = trout)\nprint(summary(model3))\n## \n## Call:\n## lm(formula = Energy ~ Ration * Group, data = trout)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.7951  -6.0981  -0.1554   3.9612  23.5946 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)              -9.2330    15.9394  -0.579 0.570483    \n## Ration                    0.8149     0.1968   4.141 0.000767 ***\n## GroupSubordinate        -18.9558    22.6934  -0.835 0.415848    \n## Ration:GroupSubordinate   0.5200     0.3204   1.623 0.124148    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.214 on 16 degrees of freedom\n## Multiple R-squared:  0.7378, Adjusted R-squared:  0.6886 \n## F-statistic:    15 on 3 and 16 DF,  p-value: 6.537e-05\nggPredict(model3) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))\n\n\n\n\nBased on the regression output and plots we can say:\n\nthere is a relationship between ration obtained and energy expenditure\nthat this relationship is the same in the two groups although the energy expenditure is higher in the dominant fish\n\n\nSolution. Exercise 3.3\n\n\n\n\nYes. The redn and initial were significantly associated (p-value = 0.00312, linear regression).\n\nmodel1 <- lm(redn ~ initial, data = blooddrug)\nsummary(model1)\n## \n## Call:\n## lm(formula = redn ~ initial, data = blooddrug)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -23.476 -11.705   1.558   9.197  24.392 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)   \n## (Intercept) -72.7302    29.1879  -2.492  0.02036 * \n## initial       0.5902     0.1788   3.301  0.00312 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.79 on 23 degrees of freedom\n## Multiple R-squared:  0.3214, Adjusted R-squared:  0.2919 \n## F-statistic: 10.89 on 1 and 23 DF,  p-value: 0.003125\n\n\n\n\nNo. The drug2 and drug3 were not significantly different from drug1 (p-value = 0.714 and p-value = 0.628, respectively). The patients of the drug 1 group had 2.750 higher blood pressure drop (redn) than those of the drug 2 group. However, the difference was relatively small comparing to the standard error of the estimate, which was 7.402. The difference between drug 1 and 3 was relatively small, too.\n\nmodel2 <- lm(redn ~ drug, data = blooddrug)\nsummary(model2)\n## \n## Call:\n## lm(formula = redn ~ drug, data = blooddrug)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -32.000  -9.286   0.000  12.714  26.000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   23.250      5.517   4.214 0.000358 ***\n## drug2          2.750      7.402   0.372 0.713796    \n## drug3         -3.964      8.076  -0.491 0.628379    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 15.6 on 22 degrees of freedom\n## Multiple R-squared:  0.03349,    Adjusted R-squared:  -0.05437 \n## F-statistic: 0.3812 on 2 and 22 DF,  p-value: 0.6875\n\n\n\n\nYes. The redn of the drug2 group was significantly higher than that of the drug1 group after adjustment for the effects of the initial (P = 0.018). The reduction of the patients who got the drug 2 was much higher (13.6906) than the drug 1, comparing to the standard error of the difference (5.3534) after accounting for initial blood pressure.\n\nmodel3 <- lm(redn ~ drug + initial, data = blooddrug)\nsummary(model3)\n## \n## Call:\n## lm(formula = redn ~ drug + initial, data = blooddrug)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.8114 -10.5842  -0.4959   6.2834  16.4265 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -124.8488    28.0674  -4.448 0.000223 ***\n## drug2         13.6906     5.3534   2.557 0.018346 *  \n## drug3         -7.2045     5.4275  -1.327 0.198625    \n## initial        0.8895     0.1671   5.323 2.81e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.42 on 21 degrees of freedom\n## Multiple R-squared:  0.5886, Adjusted R-squared:  0.5298 \n## F-statistic: 10.01 on 3 and 21 DF,  p-value: 0.0002666"
  },
  {
    "objectID": "lm-diagn.html",
    "href": "lm-diagn.html",
    "title": "4  Model diagnostics",
    "section": "",
    "text": "Aims\nLearning outcomes"
  },
  {
    "objectID": "lm-diagn.html#assessing-model-fit",
    "href": "lm-diagn.html#assessing-model-fit",
    "title": "4  Model diagnostics",
    "section": "4.1 Assessing model fit",
    "text": "4.1 Assessing model fit\n\nearlier we learned how to estimate parameters in a liner model using least squares\nnow we will consider how to assess the goodness of fit of a model\nwe do that by calculating the amount of variability in the response that is explained by the model"
  },
  {
    "objectID": "lm-diagn.html#r2-summary-of-the-fitted-model",
    "href": "lm-diagn.html#r2-summary-of-the-fitted-model",
    "title": "4  Model diagnostics",
    "section": "4.2 \\(R^2\\): summary of the fitted model",
    "text": "4.2 \\(R^2\\): summary of the fitted model\n\nconsidering a simple linear regression, the simplest model, Model 0, we could consider fitting is \\[Y_i = \\beta_0+ \\epsilon_i\\] that corresponds to a line that run through the data but lies parallel to the horizontal axis\nin our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple)\n\n\n\n\n\n\n\n\n\n\n\nTSS, denoted Total corrected sum-of-squares is the residual sum-of-squares for Model 0\n\n\\[S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}\\] corresponding the to the sum of squared distances to the purple line\n\n\n\n\n\n\n\n\n\n\nFitting Model 1 of the form \\[Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\] we have earlier defined\nRSS, the residual sum-of-squares as:\n\n\\[RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\]\n\nthat corresponds to the squared distances between the observed values \\(y_i, \\dots,y_n\\) to fitted values \\(\\hat{y_1}, \\dots \\hat{y_n}\\), i.e. distances to the red fitted line\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4.1 (\\(R^2\\)) A simple but useful measure of model fit is given by \\[R^2 = 1 - \\frac{RSS}{TSS}\\] where:\n\nRSS is the residual sum-of-squares for Model 1, the fitted model of interest\nTSS is the sum of squares of the null model\n\n\n\n\\(R^2\\) quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model\n\\(R^2\\) is also referred as coefficient of determination\nIt is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data\nValues of \\(R^2\\) approaching 1 indicate the model to be a good fit\nValues of \\(R^2\\) less than 0.5 suggest that the model gives rather a poor fit to the data"
  },
  {
    "objectID": "lm-diagn.html#r2-and-correlation-coefficient",
    "href": "lm-diagn.html#r2-and-correlation-coefficient",
    "title": "4  Model diagnostics",
    "section": "4.3 \\(R^2\\) and correlation coefficient",
    "text": "4.3 \\(R^2\\) and correlation coefficient\n\nTheorem 4.1 (\\(R^2\\)) In the case of simple linear regression:\nModel 1: \\(Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\)\n\\[R^2 = r^2\\]\nwhere:\n\n\\(R^2\\) is the coefficient of determination\n\\(r^2\\) is the sample correlation coefficient"
  },
  {
    "objectID": "lm-diagn.html#r2adj",
    "href": "lm-diagn.html#r2adj",
    "title": "4  Model diagnostics",
    "section": "4.4 \\(R^2(adj)\\)",
    "text": "4.4 \\(R^2(adj)\\)\n\nin the case of multiple linear regression, where there is more than one explanatory variable in the model\nwe are using the adjusted version of \\(R^2\\) to assess the model fit\nas the number of explanatory variables increase, \\(R^2\\) also increases\n\\(R^2(adj)\\) takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model\n\n\nTheorem 4.2 (\\(R^2(adj)\\)) For any multiple linear regression \\[Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i\\] \\(R^2(adj)\\) is defined as\n\\[R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}\\] where\n\n\\(p\\) is the number of independent predictors, i.e. the number of variables in the model, excluding the constant\n\n\\(R^2(adj)\\) can also be calculated from \\(R^2\\):\n\\[R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}\\]\n\nWe can calculate the values in R and compare the results to the output of linear regression\n\nhtwtgen <- read.csv(\"data/lm/heights_weights_genders.csv\")\nhead(htwtgen)\n##   Gender   Height   Weight\n## 1   Male 73.84702 241.8936\n## 2   Male 68.78190 162.3105\n## 3   Male 74.11011 212.7409\n## 4   Male 71.73098 220.0425\n## 5   Male 69.88180 206.3498\n## 6   Male 67.25302 152.2122\nattach(htwtgen)\n\n## Simple linear regression\nmodel.simple <- lm(Height ~ Weight, data=htwtgen)\n\n# TSS\nTSS <- sum((Height - mean(Height))^2)\n\n# RSS\n# residuals are returned in the model type names(model.simple)\nRSS <- sum((model.simple$residuals)^2)\nR2 <- 1 - (RSS/TSS)\n\nprint(R2)\n## [1] 0.8551742\nprint(summary(model.simple))\n## \n## Call:\n## lm(formula = Height ~ Weight, data = htwtgen)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8142 -0.9907  0.0263  0.9918  5.5950 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 4.848e+01  7.507e-02   645.8   <2e-16 ***\n## Weight      1.108e-01  4.561e-04   243.0   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.464 on 9998 degrees of freedom\n## Multiple R-squared:  0.8552, Adjusted R-squared:  0.8552 \n## F-statistic: 5.904e+04 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n## Multiple regression\nmodel.multiple <- lm(Height ~ Weight + Gender, data=htwtgen)\nn <- length(Weight)\np <- 1\n\nRSS <- sum((model.multiple$residuals)^2)\nR2_adj <- 1 - (RSS/(n-p-1))/(TSS/(n-1))\n\nprint(R2_adj)\n## [1] 0.8608793\nprint(summary(model.multiple))\n## \n## Call:\n## lm(formula = Height ~ Weight + Gender, data = htwtgen)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.4956 -0.9583  0.0126  0.9867  5.8358 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 47.0306678  0.1025161  458.76   <2e-16 ***\n## Weight       0.1227594  0.0007396  165.97   <2e-16 ***\n## GenderMale  -0.9628643  0.0474947  -20.27   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.435 on 9997 degrees of freedom\n## Multiple R-squared:  0.8609, Adjusted R-squared:  0.8609 \n## F-statistic: 3.093e+04 on 2 and 9997 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "lm-diagn.html#the-assumptions-of-a-linear-model",
    "href": "lm-diagn.html#the-assumptions-of-a-linear-model",
    "title": "4  Model diagnostics",
    "section": "4.5 The assumptions of a linear model",
    "text": "4.5 The assumptions of a linear model\n\nup until now we were fitting models and discussed how to assess the model fit\nbefore making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data\ninformally we have been using box plots and scatter plots to look at the data\nthere are however formal definitions of the assumptions\n\nAssumption A: The deterministic part of the model captures all the non-random structure in the data\n\nthis implies that the mean of the errors \\(\\epsilon_i\\) is zero\nit applies only over the range of explanatory variables\n\nAssumption B: the scale of variability of the errors is constant at all values of the explanatory variables\n\npractically we are looking at whether the observations are equally spread on both side of the regression line\n\nAssumption C: the errors are independent\n\nbroadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another\n\nAssumptions D: the errors are normally distributed\n\nthis will allow us to describe the variation in the model’s parameters estimates and therefore make inferences about the population from which our sample was taken\n\nAssumption E: the values of the explanatory variables are recorded without error\n\nthis one is not possible to check via examining the data, instead we have to consider the nature of the experiment"
  },
  {
    "objectID": "lm-diagn.html#checking-assumptions",
    "href": "lm-diagn.html#checking-assumptions",
    "title": "4  Model diagnostics",
    "section": "4.6 Checking assumptions",
    "text": "4.6 Checking assumptions\nResiduals, \\(\\hat{\\epsilon_i} = y_i - \\hat{y_i}\\) are the main ingredient to check model assumptions. We use plots such as:\n\nHistograms or normal probability plots of \\(\\hat{\\epsilon_i}\\)\n\n\nuseful to check the assumption of normality\n\n\nPlots of \\(\\hat{\\epsilon_i}\\) versus the fitted values \\(\\hat{y_i}\\)\n\n\nused to detect changes in error variance\nused to check if the mean of the errors is zero\n\n\nPlots of \\(\\hat{\\epsilon_i}\\) vs. an explanatory variable \\(x_{ij}\\)\n\n\nthis helps to check that the variable \\(x_j\\) has a linear relationship with the response variable\n\n\nPlots of \\(\\hat{\\epsilon_i}\\) vs. an explanatory variable \\(x_{kj}\\) that is not in the model\n\n\nthis helps to check whether the additional variable \\(x_k\\) might have a relationship with the response variable\n\n\nPlots of \\(\\hat{\\epsilon_i}\\) in the order of the observations were collected\n\n\nthis is useful to check whether errors might be correlated over time\n\nLet’s look at the “good” example going back to our data of protein levels during pregnancy\n\n\nCode\n# read in data\ndata.protein <- read.csv(\"data/lm/protein.csv\")\n\nprotein <- data.protein$Protein # our Y\ngestation <- data.protein$Gestation # our X\n\nmodel <- lm(protein ~ gestation)\n\n# plot diagnostic plots of the linear model\n# by default plot(model) calls four diagnostics plots\n# par() divides plot window in 2 x 2 grid\npar(mfrow=c(2,2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nthe residual plots provides examples of a situation where the assumptions appear to be met\nthe linear regression appears to describe data quite well\nthere is no obvious trend of any kind in the residuals vs. fitted values (the shape is scattered)\npoints lie reasonably well along the line in the normal probability plot, hence normality appears to be met\n\nExamples of assumptions not being met\n\n\n\n\n\nExample of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case\n\n\n\n\n\n\n\n\n\nExample of non-constant variance\n\n\n\n\n\n\n\n\n\nExample of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected"
  },
  {
    "objectID": "lm-diagn.html#influential-observations",
    "href": "lm-diagn.html#influential-observations",
    "title": "4  Model diagnostics",
    "section": "4.7 Influential observations",
    "text": "4.7 Influential observations\n\nSometimes individual observations can exert a great deal of influence on the fitted model\nOne routine way of checking for this is to fit the model \\(n\\) times, missing out each observation in turn\nIf we removed i-th observation and compared the fitted value from the full model, say \\(\\hat{y_j}\\) to those obtained by removing this point, denoted \\(\\hat{y_{j(i)}}\\) then\nobservations with a high Cook’s distance (measuring the effect of deleting a given observation) could be influential\n\nLet’s remove some observation with higher Cook’s distance from protein data set, re-fit our model and compare the diagnostics plots\n\n# observations to be removed (based on Residuals vs. Leverage plot)\nobs <- c(18,7)\n\n# fit models removing observations\nmodel.2 <- lm(protein[-obs] ~ gestation[-obs])\n\n# plot diagnostics plot\npar(mfrow=c(2,2))\nplot(model.2)"
  },
  {
    "objectID": "lm-diagn.html#selecting-best-model",
    "href": "lm-diagn.html#selecting-best-model",
    "title": "4  Model diagnostics",
    "section": "4.8 Selecting best model",
    "text": "4.8 Selecting best model\n\nWe have learned what linear models are, how to find estimates and interpret model coefficients and how to check for the overall relationship between response and predictors. We also know how to assess model fit, check model assumptions and find potential outliers. Given a set of predictors, e.g. many genes, how do we arrive at the best model?\nAs a rule of thumb, we want a model that fits the data best and is as simple as possible, meaning it contains only relevant predictors.\nIn practice, this means, that for smaller data sets, e.g. with up to 10 predictors, one works with manually trying different models, including different subsets of predictors, interactions terms and/or their transformations.\nWhen the number of predictors is large, one can try automated approaches of feature selection like forward selection or stepwise regression, the last one demonstrated in the exercises below.\nFinally, as we will learn later in the course, we can use regularization techniques that allow including all parameters in the model but constrain (regularizes) coefficient estimates towards zero for the less relevant predictors, preventing building complex models and thus overfitting."
  },
  {
    "objectID": "lm-diagn-exercises.html",
    "href": "lm-diagn-exercises.html",
    "title": "5  Exercises (Model diagnostics)",
    "section": "",
    "text": "Data for exercises are on Canvas under Files -> data_exercises –> linear-models\nTo access and preview the data:"
  },
  {
    "objectID": "lm-diagn-exercises.html#answers-to-selected-exercises",
    "href": "lm-diagn-exercises.html#answers-to-selected-exercises",
    "title": "5  Exercises (Model diagnostics)",
    "section": "Answers to selected exercises",
    "text": "Answers to selected exercises\n\nSolution. Exercise 5.1\n\n\n# access and preview data\ndata(fat, package = \"faraway\")\nhead(fat)\n##   brozek siri density age weight height adipos  free neck chest abdom   hip\n## 1   12.6 12.3  1.0708  23 154.25  67.75   23.7 134.9 36.2  93.1  85.2  94.5\n## 2    6.9  6.1  1.0853  22 173.25  72.25   23.4 161.3 38.5  93.6  83.0  98.7\n## 3   24.6 25.3  1.0414  22 154.00  66.25   24.7 116.0 34.0  95.8  87.9  99.2\n## 4   10.9 10.4  1.0751  26 184.75  72.25   24.9 164.7 37.4 101.8  86.4 101.2\n## 5   27.8 28.7  1.0340  24 184.25  71.25   25.6 133.1 34.4  97.3 100.0 101.9\n## 6   20.6 20.9  1.0502  24 210.25  74.75   26.5 167.0 39.0 104.5  94.4 107.8\n##   thigh knee ankle biceps forearm wrist\n## 1  59.0 37.3  21.9   32.0    27.4  17.1\n## 2  58.7 37.3  23.4   30.5    28.9  18.2\n## 3  59.6 38.9  24.0   28.8    25.2  16.6\n## 4  60.1 37.3  22.8   32.4    29.4  18.2\n## 5  63.2 42.2  24.0   32.2    27.7  17.7\n## 6  66.0 42.0  25.6   35.7    30.6  18.8\n\n# fit linear regression model\nmodel.all <- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n\n# print model summary\nprint(summary(model.all))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  < 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: < 2.2e-16\n\n# diagnostics plots\npar(mfrow=c(2,2))\nplot(model.all)\n\n# remove potentially influential observations\nobs <- c(86)\nfat2 <- fat[-obs, ]\n\n# re-fit the model\nmodel.clean <- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n\n# diagnostics plots\npar(mfrow=c(2,2))\nplot(model.clean)\n\n\n\n\n\n\n\n\n# model summary\nprint(summary(model.clean))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  < 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: < 2.2e-16\n\n# re-fit the model (no height)\nmodel.red1 <- lm(brozek ~ age + weight + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\nprint(summary(model.red1))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2830  -2.6162  -0.1017   2.8789   9.3713 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -22.66569   11.97691  -1.892  0.05963 .  \n## age           0.05948    0.02954   2.013  0.04521 *  \n## weight       -0.09829    0.04114  -2.389  0.01765 *  \n## neck         -0.43444    0.21445  -2.026  0.04389 *  \n## abdom         0.88762    0.06839  12.979  < 2e-16 ***\n## hip          -0.17180    0.12919  -1.330  0.18483    \n## thigh         0.25327    0.12960   1.954  0.05183 .  \n## knee         -0.02318    0.22128  -0.105  0.91665    \n## ankle         0.17300    0.20411   0.848  0.39752    \n## biceps        0.15695    0.15715   0.999  0.31894    \n## forearm       0.43091    0.18239   2.363  0.01895 *  \n## wrist        -1.51011    0.49120  -3.074  0.00235 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.976 on 240 degrees of freedom\n## Multiple R-squared:  0.7484, Adjusted R-squared:  0.7369 \n## F-statistic:  64.9 on 11 and 240 DF,  p-value: < 2.2e-16\n\n# re-fit the model (no knee)\nmodel.red2 <- lm(brozek ~ age + weight + neck + abdom + hip + thigh + ankle + biceps + forearm + wrist, data = fat)\nprint(summary(model.red2))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2552  -2.5979  -0.1133   2.8693   9.3584 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -23.08716   11.25781  -2.051  0.04137 *  \n## age           0.05875    0.02864   2.051  0.04134 *  \n## weight       -0.09965    0.03897  -2.557  0.01117 *  \n## neck         -0.43088    0.21131  -2.039  0.04253 *  \n## abdom         0.88875    0.06740  13.186  < 2e-16 ***\n## hip          -0.17231    0.12884  -1.337  0.18234    \n## thigh         0.24942    0.12403   2.011  0.04544 *  \n## ankle         0.16946    0.20089   0.844  0.39974    \n## biceps        0.15847    0.15616   1.015  0.31123    \n## forearm       0.42946    0.18150   2.366  0.01876 *  \n## wrist        -1.51470    0.48823  -3.102  0.00215 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.968 on 241 degrees of freedom\n## Multiple R-squared:  0.7484, Adjusted R-squared:  0.738 \n## F-statistic: 71.69 on 10 and 241 DF,  p-value: < 2.2e-16\n\n# re-fit the model (no ankle)\nmodel.red3 <- lm(brozek ~ age + weight + neck + abdom + hip + thigh  + biceps + forearm + wrist, data = fat)\nprint(summary(model.red3))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0740  -2.5615  -0.1021   2.7999   9.3199 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -20.61247   10.86240  -1.898   0.0589 .  \n## age           0.05727    0.02857   2.004   0.0461 *  \n## weight       -0.09141    0.03770  -2.424   0.0161 *  \n## neck         -0.45458    0.20931  -2.172   0.0308 *  \n## abdom         0.88098    0.06673  13.203   <2e-16 ***\n## hip          -0.17575    0.12870  -1.366   0.1733    \n## thigh         0.25504    0.12378   2.061   0.0404 *  \n## biceps        0.15178    0.15587   0.974   0.3311    \n## forearm       0.42805    0.18138   2.360   0.0191 *  \n## wrist        -1.40948    0.47175  -2.988   0.0031 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.965 on 242 degrees of freedom\n## Multiple R-squared:  0.7477, Adjusted R-squared:  0.7383 \n## F-statistic: 79.67 on 9 and 242 DF,  p-value: < 2.2e-16\n\n# re-fit the model (no biceps)\nmodel.red4 <- lm(brozek ~ age + weight + neck + abdom + hip + thigh  + forearm + wrist, data = fat)\nprint(summary(model.red4))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0574  -2.7411  -0.1912   2.6929   9.4977 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -20.06213   10.84654  -1.850  0.06558 .  \n## age           0.05922    0.02850   2.078  0.03876 *  \n## weight       -0.08414    0.03695  -2.277  0.02366 *  \n## neck         -0.43189    0.20799  -2.077  0.03889 *  \n## abdom         0.87721    0.06661  13.170  < 2e-16 ***\n## hip          -0.18641    0.12821  -1.454  0.14727    \n## thigh         0.28644    0.11949   2.397  0.01727 *  \n## forearm       0.48255    0.17251   2.797  0.00557 ** \n## wrist        -1.40487    0.47167  -2.978  0.00319 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.965 on 243 degrees of freedom\n## Multiple R-squared:  0.7467, Adjusted R-squared:  0.7383 \n## F-statistic: 89.53 on 8 and 243 DF,  p-value: < 2.2e-16\n\n# re-fit the model (no hip)\nmodel.red5 <- lm(brozek ~ age + weight + neck + abdom  + thigh  + forearm + wrist, data = fat)\nprint(summary(model.red5))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + \n##     wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0193  -2.8016  -0.1234   2.9387   9.0019 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -30.17420    8.34200  -3.617 0.000362 ***\n## age           0.06149    0.02852   2.156 0.032047 *  \n## weight       -0.11236    0.03151  -3.565 0.000437 ***\n## neck         -0.37203    0.20434  -1.821 0.069876 .  \n## abdom         0.85152    0.06437  13.229  < 2e-16 ***\n## thigh         0.20973    0.10745   1.952 0.052099 .  \n## forearm       0.51824    0.17115   3.028 0.002726 ** \n## wrist        -1.40081    0.47274  -2.963 0.003346 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.974 on 244 degrees of freedom\n## Multiple R-squared:  0.7445, Adjusted R-squared:  0.7371 \n## F-statistic: 101.6 on 7 and 244 DF,  p-value: < 2.2e-16\n\n# compare model.clean and final model\nprint(summary(model.clean))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  < 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: < 2.2e-16\nprint(summary(model.red5))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + \n##     wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0193  -2.8016  -0.1234   2.9387   9.0019 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -30.17420    8.34200  -3.617 0.000362 ***\n## age           0.06149    0.02852   2.156 0.032047 *  \n## weight       -0.11236    0.03151  -3.565 0.000437 ***\n## neck         -0.37203    0.20434  -1.821 0.069876 .  \n## abdom         0.85152    0.06437  13.229  < 2e-16 ***\n## thigh         0.20973    0.10745   1.952 0.052099 .  \n## forearm       0.51824    0.17115   3.028 0.002726 ** \n## wrist        -1.40081    0.47274  -2.963 0.003346 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.974 on 244 degrees of freedom\n## Multiple R-squared:  0.7445, Adjusted R-squared:  0.7371 \n## F-statistic: 101.6 on 7 and 244 DF,  p-value: < 2.2e-16\n\nNote: we have just run a very simple feature selection using stepwise regression. In this method, using backward elimination, we build a model containing all the variables and remove them one by one based on defined criteria (here we have used p-values) and we stop when we have a justifiable model or when removing a predictor does not change the chosen criterion significantly."
  }
]