---
output: html_document
editor_options: 
  chunk_output_type: console
---

## Linear models in the ML context

- It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response. 
- Including such **irrelevant** variables leads to unnecessary complexity in the resulting model, and worse prediction results. 
- There are few approaches to perform **feature selection** or **variable selection**, that is for excluding irrelevant variables from a multiple regression model.
- Here, we can also think of linear models in machine learning context. By using data splitting strategies, we can find best linear models for prediction tasks. 
- Similar to evaluation classification results, we need some metrics to evaluate regression results. In terms of assessing model fit, we have already seen one metric i.e. adjusted $R^2$. Other metrics than can also be expressed in terms of RSS include **Akaike information criterion (AIC)** and **Bayesian information criterion (BIC)**. 
- In addition, validation set and cross-validation methods can be used to directly evaluate the prediction error. Here, we use metrics such as **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)**.


## Evaluating regression

### model fit

**Adjusted R-squared** (as seen before)
$$
R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
$$

**AIC** and **BIC**

AIC is grounded in information theory and BIC is derived from a Bayesian point of view. Both are formally defined in likelihood functions. For regression models they can be expressed in terms of RSS because the likelihood of a model in the context of normal errors is directly related to the RSS. 
 
$$\text{AIC} = n \ln(\text{RSS}/n) + 2p$$

where:

- $n$ is the number of observations.
- $\text{RSS}$ is the residual sum of squares.
- $p$ is the number of parameters in the model (including the intercept).


$$\text{BIC} = n \ln(\text{RSS}/n) + p \ln(n)$$

where:

- $n$ is the number of observations.
- $\text{RSS}$ is the residual sum of squares.
- $p$ is the number of parameters in the model.


Both criteria, AIC and BIC, introduce penalties for the number of parameters to avoid overfitting. BIC introduces a stronger penalty based on the sample size, making it more conservative than AIC. When comparing models using AIC or BIC that incorporate RSS, the objective remains the same: select the model that provides the best balance between goodness of fit and model simplicity. The model with the lower AIC or BIC value is generally preferred, as it indicates either a more parsimonious model or a model that better fits the data (or both). 

### predictions

- **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values.
$$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$
- **Root Mean Squared Error (RMSE)**: square root of the MSE
$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$
- **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$
- **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.

The smaller the difference between the predicted values vs. the validation or cross-validation predicted values, the better the model. 

## Feature selection

The main classes include **Subset Selection**, **Shrinkage methods** and **Dimension Reduction**.


