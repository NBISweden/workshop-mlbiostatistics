---
title: 'Session regression I: simple linear regression'
output:
  pdf_document: 
    toc: true
    number_sections: true
    fig_caption: yes
  md_document:
    toc: true
    number_sections: true
    variant: markdown_github
  html_document:
    df_print: paged
    toc: true
    number_sections: true
header-includes: \usepackage{float}
urlcolor: blue

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path="session-regression-I-files/figures/")
knitr::opts_chunk$set(fig.pos = 'H')
knitr::opts_chunk$set(cache.path = "tmp")
knitr::opts_chunk$set(cache = TRUE)
```

These lecture notes are based on and closely follow section 3.1 Simple Linear Regression in 
*An Introduction to Statistical Learning, with applications in R* 
by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani
(2013, DOI: 10.1007/978-1-4614-7138-7).

# Learning outcomes
- understand simple linear regression model incl. terminology and mathematical notations
- estimate model parameters and their standard error
- use model for checking the association between _x_ and _y_
- use model for prediction
- assess model accuracy with RSE and R$^2$
- check model assumptions
- to be able to use `lm` function in R for model fitting, obtaining confidence interval and predictions

-------

# Introduction
## [Quiz](https://forms.gle/bHZr1MP454npysAFA): What do we already know about `simple linear regression`? 


#### Description
- Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative, numerical) variables
  - one variable, denoted `x` is regarded as the *predictor*, *explanatory*, or *indepedent variable*, e.g. body weight (kg)
  - the other variable, denoted `y`, is regarded as the *response*, *outcome*, or *dependent variable*, e.g. plasma volume (liters)
- It is used to estimate the best-fitting straight line to describe the association

#### Used for to answer questions such as: 
- is there a relationship between `x` exposure (e.g. body weight) and `y` outcome (e.g. plasma volume)?
- how strong is the relationship between the two variables?
- what will be a predicted value of the `y` outcome given a new set of exposure values?
- how accurately can we predict the outcome?

\newpage

```{r, fig-intro-det-vs-stat, echo=F, fig.height=5, fig.cap="Determinisitc vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. $Fahrenheit=9/5*Celcius+32$; b) statistical relationship between x and y is not perfect (increasing), c)  statistical relationship between x and y is not perfect (decreasing), d) random signal", fig.align="center"}

par(mfrow=c(2,2), mar=c(3,4,3,3))

# Deterministic relationship example
x_celcius <- seq(from=0, to=50, by=5)
y_fahr <- 9/5*x_celcius+32
plot(x_celcius, y_fahr, type="b", pch=19, xlab="Celcius", ylab="Fahrenheit", main="a)", cex.main=0.8)

# Statistical relationship (increasing)
x <- seq(from=0, to=100, by=5)
y_increasing <- 2*x + rnorm(length(x), mean=100, sd=25)
plot(x, y_increasing, pch=19, xlab="x", ylab="y", main="b)", cex.main=0.8)

# Statistical relationship (decreasing)
y_decreasing <- -2*x + rnorm(length(x), mean=100, sd=25)
plot(x, y_decreasing, pch=19, xlab="x", ylab="y", main="c)", cex.main=0.8)

# Statistical relationship (random)
y_random <- - rnorm(length(x), mean=100, sd=25)
plot(x, y_random, pch=19, xlab="x", ylab="y", main="d)", cex.main=0.8)

```


### Example data

Example data contain the body weight (kg) and plasma volume (liters) for eight healthy men. 
```{r}

weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)

```


```{r, fig-intro-example, echo=F, fig.align="center", fig.height=4, fig.width=4, fig.cap="Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*."}

plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid())
#abline(lm(plasma~weight), col="red") # regression line

```


```{r, fig-intro-example-reg, echo=F, fig.align="center", fig.height=4, fig.width=4, fig.cap="Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)"}

plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]", panel.first = grid())
abline(lm(plasma~weight), col="red") # regression line

```

The equation of the regression line is: 

$$y=\beta_0 + \beta_1x$$ 

or mathematically using matrix notation
$$Y=\beta_0 + \beta_1X$$ 

```{r, fig-intro-example-reg-parameters, echo=F, fig.align="center", fig.height=7, fig.width=7, fig.cap="Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red). Parameters explanation", eval=T}

par(mfcol=c(2,2), mar=c(4,4,3,2))

# Values from regression model: plasma_volume = 0.0857 + 0.043615*x

# Fitted line
plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid())
abline(lm(plasma~weight), col="red") # regression line
text(65, 3.3, "plasma = 0.0857 + 0.0436 * weight", cex=1) 

# Beta 1 example b
plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid(), xlim=c(60, 70), ylim=c(2.8, 3.2))
abline(lm(plasma~weight), col="red") # regression line
segments(x0=65, y0=2.92, x1=66, y1=2.92, col="blue")
segments(x0=66, y0=2.92, x1=66, y1=2.964, col="blue")
text(67, 2.92, expression(beta[1]), cex=1.2, col="blue")

# Beta 1 example a
plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid())
abline(lm(plasma~weight), col="red") # regression line
segments(x0=65, y0=2.92, x1=70, y1=2.92, col="blue")
segments(x0=70, y0=2.92, x1=70, y1=3.14, col="blue")
text(72, 2.95, expression(beta[1]), cex=1.2, col="blue")

# Beta 0 example a
plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid(), xlim=c(-5, 80), ylim=c(0, 5))
abline(lm(plasma~weight), col="red") # regression line
abline(h=0.0857) # regression line
segments(x0=65, y0=2.92, x1=66, y1=2.92, col="blue")
segments(x0=66, y0=2.92, x1=66, y1=2.964, col="blue")
text(0, 0.5, expression(beta[0]), cex=1.2, col="blue")

```

[Quiz](https://forms.gle/8jSsdQehGhjw87E38): Regression model parameters
--------
\newpage


# Estimating model coefficients

In practice, $\beta_0$ and $\beta_1$ are usually unknown. The best-fitting line is derived using the method of **least squares**, i.e. by finding the values of the parameters $\beta_0$ and $\beta_1$ tht minimize the sum of the squared vertical distances of the points from the line. 

```{r, coeff-residuals, echo=F, warning=F, message=F, fig.align="center", fig.width=4, fig.height=3}

data.reg <- data.frame(plasma=plasma, weight=weight)
fit.reg <- lm(plasma~weight, data=data.reg)
data.reg$predicted <- predict(fit.reg)
data.reg$residuals <- residuals((fit.reg))

library(ggplot2)
ggplot(data.reg, aes(x=data.reg$weight, data.reg$plasma)) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") + 
  geom_segment(aes(xend = weight, yend = predicted)) + 
  geom_point(aes(y = predicted), shape = 1) +
  geom_point(aes(y = predicted), shape = 1) + 
  theme_bw() + xlab("body weight [kg]") + ylab("plasma volume [liters]")


```

Let $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$ represent $n$ observation pairs, each of which consists of a measurement of $X$ and $Y$, e.g. in our example we have 8 pairs of observations, e.g. (58, 2.75), (70, 2.86) etc. 

```{r}
weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
```

We seek to find coefficients estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ such that liner model fits the available data well, i.e. such that the resulting line is as close as possible to the 8 data points. 

There are a number of ways of measuring _closeness_. By far the most common approach involves minimizing the _least squares_ criterion. 

Let $\hat{y_i}=\hat{\beta_0} + \hat{\beta_1}x_i$ be the prediction $Y$ based on the $i$th value of $X$. Then $\epsilon_i = y_i - \hat{y_i}$ represents the $i$th *residual*, i.e. the difference between the $i$th observed response value and the $i$th response value that is predicted by the linear model.

RSS, the *residual sum of squares* is defined as: $$RSS = \epsilon_1^2 + \epsilon_2^2 + ... \epsilon_n^2$$ or
equivalently as: $$RSS=(y_1-\hat{\beta_0}-\hat{\beta_1}x_1)^2+(y_2-\hat{\beta_0}-\hat{\beta_1}x_2)^2+...+(y_n-\hat{\beta_0}-\hat{\beta_1}x_n)^2$$

The least squares approach chooses $\hat{\beta_0}$ and $\hat{\beta_1}$ to minimize the RSS. With some calculus one gets: 

$$\hat{\beta_1}= \frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}{\sum_{i=1}^{n}(x_i-\overline{x})^2}$$
$$\hat{\beta_0}=\overline{y}-\hat{\beta_1}\overline{x}$$

where $\overline{y}=\frac{1}{n}\sum_{i=1}^{n}y_i$ and $\overline{x}=\frac{1}{n}\sum_{i=1}^{n}x_i$ are the sample means. 

[Pen and Paper exercise](pen-and-paper-plasma-volume.pdf): Estimating model coefficients


More on deriving the equations for model coefficients:

- [prof. Autar Kaw on youtube](https://www.youtube.com/watch?v=DSQ2plMtbLc)
- [http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf)


# Hypothesis testing
## Accuracy of the coefficient estimates
- The calculated $\hat{\beta_0}$ and $\hat{\beta_1}$ are estimates of the population values of the intercept and slope and are, therefore, subject to sampling variation
- Their precision is measure by their standard errors
$$s.e(\hat{\beta_0})=s*\sqrt{[\frac{1}{n}+\frac{x_i^2}{\sum_{i=1}^{n}(x_i-\overline{x})^2}]}$$
$$s.e(\hat{\beta_1})=\frac{s}{\sqrt{\sum_{i=1}^{n}(x_i-\overline{x})^2}}$$
where, $s$ is the *standard deviation of the points about the line*. It has $(n-2)$ degrees of freedom, i.e. the sample size minus the number of regression coefficients
$$s=\sqrt{[\frac{\sum_{i=1}^{n}(y_i-\overline{y})^2-\overline{\beta_1}\sum_{i=1}^{n}(x_i-\overline{x})^2}{n-2}]}$$

[Pen and Paper exercise](pen-and-paper-plasma-volume.pdf): Accuracy of the coefficient estimates

## Confidence interval
- Standard errors can be used to compute ``confidence interval``.
- A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. 
- The range is defined in terms of lower and upper limits computed from the data. For linear regression, the 95% confidence intervals takes form:
$$[\hat{\beta_1}-2*s.e.(\hat{\beta_1}), \hat{\beta_1}+2*s.e.(\hat{\beta_1})]$$ and 
$$[\hat{\beta_1}-2*s.e.(\hat{\beta_0}), \hat{\beta_1}+2*s.e.(\hat{\beta_0})]$$

## Hypothesis testing
- Standard errors can also be used to perform ``hypothesis testing`` on the coefficients. 
- The most common hypothesis test involves testing the ``null hypothesis`` of: \newline

$H_0:$ There is no relationship between $X$ and $Y$ \newline

versus the ``alternative hypothesis`` \newline

$H_a:$ There is some relationship between $X$ and $Y$

Mathematically, this corresponds to testing
$$H_0: \beta_1=0$$
versus
$$H_0: \beta_1\neq0$$
since if 
$$\beta_1=0$$ then the model $$Y=\beta_0+\beta_1X + \epsilon$$ reduces to $$Y=\beta_0 + \epsilon$$

To test the null hypothesis we need to determine whether $\hat{\beta_1}$, our estimate of $\beta_1$, is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero. 

How far is far enough? This depends on the accuracy of $\hat{\beta_1}$, that is standard error $s.e.(\hat{\beta_1})$. If $s.e.(\hat{\beta_1})$ is small, then small values of $\hat{\beta_1}$ may provide strong evidence that $\hat{\beta_1}\neq0$ and _vice verca_. In practice, we compute a ``t-statistics`` given by $$t=\frac{\hat{\beta_1}-0}{s.e.(\hat{\beta_1})}$$ which measures the standard deviations that $\hat{\beta_1}$ is away from 0. 

If there really is no relationship between $X$ and $Y$, then we this will have a $t$-distribution with $n-2$ degrees of freedom. From previous sessions, we now know how to compute probability of observing any value equal to $|t|$. We call this probability the $p-value$. 

We can interpret the $p-value$ as follows: a small p-value indicates that it is unlikely to observe such a substantial association between $X$ and $Y$ due to chance, i.e. in the absence of any real association. We therefore can ``reject the null hypothesis``. 

Typical p-value cutoffs for rejecting the null hypothesis are 5 or 1%. 

[Pen and Paper exercise](pen-and-paper-plasma-volume.pdf): Hypothesis testing

## Live coding demo
```{r, fig.width=4, fig.height=4, fig.align="center", fig.cap="Body weight vs. plasma volume", results="hold"}

# Data
weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0)
plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) 

# Plot
plot(weight, plasma)

# Regression
reg <- lm(plasma~weight)
summary(reg)

# Coefficients
coef(reg)

# Confidence intervals
confint(reg)

# Add regression line to the plot
abline(reg)

```

# Prediction
Sometimes it may be useful to use the regression equation to predict the value of $y_i$ for a particular value of $x_i$, say $x_i´$. The `predicted value` is: $$y_i'=\hat{\beta_0}+\hat{\beta_1}x_i'$$ and its standard error is: $$s.e.(y_i')=s\sqrt{[1+\frac{1}{n}+\frac{(x_i-\overline{x_i})^2}{\sum_{i=1}^{n}(x_i-\overline{x_i})^2}]}$$

The standard error is least when $x_i´$ is close to the mean, $\overline{x}$

In general, one should be reluctant to use the regression line for predicting values outside the range of $x$ in the original data, as the linear relationship will not necessarily hold true beyond the range over which it has been fitted. 

## Prediction interval
There is also a concept called `prediction interval`. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval.

Prediction interval regression vs. confidence interval

- 95% confidence interval: there is 95% probability that the true best fit-line for the population lies within the confidence interval
- 95% prediction interval: 95% of the $y$ values found for a certain $x$ value will be within the interval range around the linear regression line
- prediction interval $>$ than a confidence interval, as it must account for both the uncertainty in knowing the value of the population mean, plus data scatter.

The 95% prediction interval of the forecasted value $y_i'$: 

## Live coding demo
```{r}
# Prediction
predict(reg, data.frame(weight=60))
predict(reg, data.frame(weight=c(60, 70)))

# Prediction with confidence intervals
predict(reg, data.frame(weight=66), interval="prediction")

```

# Asesssing the Accuracy of the Model & Correlation
Once we have rejected the null hypothesis ($H_0:$there is no relationship between X and Y) in favor of the alternative hypothesis ($H_a$there is some relationship between X and Y) we may want to quantify `the extent to which the model fits the data`. 

The quality of a linear regression fit is typically assessed using two related quantities:
- RSE, the residual standard error
- $R^2$ statistics

## RSE, Residual standard error

- RSE is a measure of `lack of fit` of the model to the data
- It is measured in units of $Y$. 

Going back to linear regression model, and writing it in the formal complete way:
$$Y=\beta_0+\beta_1X+\epsilon$$ we can see that each observation is associated with an error term $\epsilon$. This means that even knowing $\beta_0$ and $\beta_1$ one cannot perfectly predict $Y$ from $X$. 

`RSE` is an estimate of the standard deviation of $\epsilon$, that can be viewed as the average amount that the response will deviate from the true regression line. It is calculated $$RSE=\sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}{n}(y_i-\hat{y_i})^2}$$ where $$RSE=\sum_{i=1}^{n}(y_i-\hat{y_i})^2$$

```{r, rse, echo=F, fig.align="center", fig.cap="", fig.width=4, fig.height=3}

weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)

data.reg <- data.frame(plasma=plasma, weight=weight)
fit.reg <- lm(plasma~weight, data=data.reg)
data.reg$predicted <- predict(fit.reg)
data.reg$residuals <- residuals((fit.reg))

library(ggplot2)
ggplot(data.reg, aes(x=data.reg$weight, data.reg$plasma)) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") + 
  geom_segment(aes(xend = weight, yend = predicted)) + 
  geom_point(aes(y = predicted), shape = 1) +
  geom_point(aes(y = predicted), shape = 1) + 
  theme_bw() + xlab("body weight [kg]") + ylab("plasma volume [liters]")

```


### Live coding demo
```{r}

#weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
#plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)

plasma
weight 
head(data.reg)

reg <- lm(data.reg$plasma~data.reg$weight)

# predict Y given the values of X and regression model reg
y.pred <- predict(reg, data.frame(weight=data.reg$weight))
y.pred

# calculate residuals
e.terms <- data.reg$plasma-y.pred
e.terms

# calculate RSS
RSS=sum(e.terms^2)

# calculate RSE
n=nrow(data.reg)
RSE <- sqrt((1/(n-2))*RSS)

# R reg objects contains it all
names(reg)
reg$fitted.values
reg$residuals

# RSE
summary(reg)

```

## $R^2$ statistics
- $R^2$ statistics is an alternative measure of fit and measure of linear relationship between $X$ and $Y$
- It takes the form of a proportion, the proportion of variance explained, hence is independent of the scale of $Y$
- $0 \leq R^2 \leq 1$

$$R^2=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}$$ where $$TSS=\sum_{i=1}^{n}(y_i-\overline{y})$$

## Correlation
- is also a measure of linear regression between $X$ and $Y$
$$Cor(X,Y)=\frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\overline{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\overline{y})^2}}$$

This suggests that we should be able to use $r=Cor(X,Y)$ to assess the fit of the linear model

In fact, For simple linear regression, it can be shown that $$R^2=r^2$$

### Live coding demo
```{r}

#summary(reg)
names(reg)

r2 <- cor(data.reg$plasma, data.reg$weight)^2

```

# Assumptions 
- The regression model is linear in parameters, i.e. the true relationship is linear
- Errors, $\epsilon_i$, are independent 
- Errors, $\epsilon_i$, at each value of predictor, $x_i$, are normally distributed
- Errors, $\epsilon_i$, at each value of predictor, $x_i$, have equal variances, $\sigma^2$ (homoscedasticity of errors)


The residuals provide information about the noise term in the model, and allow limited checks on model assumptions. 
Note that in small data set, departures from assumptions may be hard to detect. 
- A plot of residuals versus fitted values allows a visual check for any pattern in the residuals that might suggest a curve rather than a line
- A normal probability plot of residuals: if residuals are from a normal distribution points should like, to within statistical error, close to a

## Live coding demo
```{r, fig.width=10, fig.height=5, fig.align="center", fig.cap="Diagnostic plots for the regression"}

par(mfrow=c(1,2))
plot(reg, which=1:2)


```

# Beyond
- [Linear regression, ANOVA and t-test relationship](https://newonlinecourses.science.psu.edu/stat501/node/286/)
- [Outliers and influential observations](https://newonlinecourses.science.psu.edu/stat501/node/286/) (best to read after multiple regression session)
- [Data transformations](https://newonlinecourses.science.psu.edu/stat501/node/318/)
- [Linear models chapter to dig more into mathematics behind lm](http://www.rwdc2.com/files/rafa.pdf)












