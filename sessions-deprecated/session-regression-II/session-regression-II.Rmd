---
title: 'Regression session II: multiple linear regression'
author: 'Warren W. Kretzschmar'
date: "`r Sys.Date()`"
output:
  md_document:
    toc: true
    variant: markdown_github
  html_document:
    df_print: paged
    toc: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
header-includes: \usepackage{float}
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path="session-regression-II-files/figures/")
knitr::opts_chunk$set(fig.pos = 'H')
knitr::opts_chunk$set(cache.path = "tmp")
knitr::opts_chunk$set(cache = TRUE)
```

These lecture notes are based on and closely follow section 3.2 in 
*An Introduction to Statistical Learning, with applications in R* 
by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani
(2013, DOI: 10.1007/978-1-4614-7138-7).

# Learning outcomes

After this session, a student should be able to:

- visualize bivariate relationships
- fit a linear regression model containing main effects
- assess the quality of the model fit
- determine if at least one predictor can predict the response
- detaermine which predictors predict the response
- asses the accuracy of predictions from the model

# Warmup: [Quiz: revisiting linear model specifications](https://forms.gle/z4qKdrcQj1wqUxBV7)

# Visualizing the Advertising dataset

In this session we will use the [Advertising dataset](http://www-bcf.usc.edu/~gareth/ISL/data.html).
This simple dataset consists of sales data for 200 products along with the amount of money spent 
on TV, radio, and newspaper ads. We would like to know how best to spend advertizing money to
maximize sales.

To begin with, let us familiarize ourselves with the dataset. 
The data are stored in the `data` subdirectory of this session directory.

```{r}
# load the data
ads = read.csv('./data/Advertising.csv')
```

First, we check to see what columns were imported

```{r}
head(ads)
```

It looks like a redundant column of row numbers, `X`, has made it into the table.

The other columns look like numbers. That's good.

Now, let us use the `pairs` function to get a quick overview of linear relationships within the dataset. 

```{r}
# visualize all pairwise relationships
pairs(ads)
```

The pairs plot creates a scatter plot of every pair of variables in a data frame.
First, to clear things up, the variable `X` is uncorrelated with the other columns and does not 
add anything to the dataset. We should probably remove it and replot:

```{r}
ads = ads[-1]
pairs(ads)
```

Ah, that's better.

From the pairs plot we can see that:

1. TV expenditure appears to be correlated with sales
2. As TV expenditure goes up, the variance associated with sales increases as well
3. radio expenditure appears to be correlated with sales
4. newspaper sales do not look very correlated to sales

It looks like more than one variable could be used to predict sales. 
How would we handle this in the simple regression? 

## Excercise: fitting simple linear regressions on multivariate data

We can fit a simple linear regression for TV vs Sales this way:
```{r error=TRUE}
lm(Sales ~ TV, data=ads)
```

Oops that did not work! Let's see what's up:

```{r}
names(ads)
```
Ah! sales is lower case:
```{r}

lm(sales ~ TV, data=ads)

```
For every \$1000 spent on TV ads, our average sales went up by five units.
Pretty sweet! What about radio?

```{r}
lm(sales ~ radio, data=ads)
```
radio appears to help even more!

```{r}
lm(sales ~ newspaper, data=ads)
```
newspaper appears to have a similar effect to TV. 
But there seemed to be much more noise between sales and newspaper in the pairs plot.  What's going on?

```{r}
summary(lm(sales ~ TV, data=ads))
summary(lm(sales ~ newspaper, data=ads))
```
The answer is in the $R^2$ values: TV has a much higher $R^2$ than newspaper.

We can fit a linear model for each predictor separately, but we are left
with two problems:

1. How would be combine the three models into a single model to create a single prediction for Sales?
2. The pairs plot shows us that the predictors are correlated. The simple linear regression
   fits ignore all other predictors, and this can lead to incorrect predictions.

This is where multiple linear regression comes in.
It allows us to create a single model for predicting sales from multiple predictors, and it allows us to
create a model that takes correlation between predictors into account.

# Multiple linear regression model specifications

A multiple linear regression model that incorporates $p$ predictors can be expressed as:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_p + \epsilon$$

where $Y$ is the response variable, $X_j$ is the $j^{th}$ predictor, and 
$\beta_j$ is the $j^{th}$ model coefficient. $\beta_j$ can be interpreted as the average
increase in $Y$ for one unit increase in $X_j$ while holding all other predictors fixed.

For the `Advertising` dataset we can express a regression model as:

$$Sales = \beta_0 + \beta_1 newspaper + \beta_2 radio + \beta_3 TV + \epsilon$$

# Estimating regression coefficients

We can estimate the regression coefficients $\hat\beta_j$ from the data in the same
manner as for simple linear regression. We can then use $\hat\beta_j$ to make predictions $\hat{y}$ 
using the formula:

$$\hat{y} = \hat\beta_0 + \hat\beta_1x_1 + ... + \hat\beta_px_p$$

As in simple linear regression, we choose $\beta_j$ such that we minimize the residual sum of squares:

$$\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$
, which is equivalent to
$$\text{RSS} = \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_{i1} - ... - \hat\beta_px_{ip})^2$$

The mathematical formulas for estimating the model coefficients in multiple linear regression
work similarly to the formulas for simple linear regression. However, they are more complex and 
require some linear algebra to understand, so we will skip those formulas here. 

## Quiz: [What was $y_i$ again?](https://forms.gle/XUxpgxJbkTp1QfDG8)

## Exercise: Fitting a multiple linear regression model

Above, we fit a simple linear regression model to the Advertising dataset.
We had to fit each predictor separately. Here we will fit a joint
model:

```{r}
summary(lm(sales ~ TV + newspaper + radio, data=ads))
```

We note that the coefficient estimates have changed! TV looks similar 
to what we saw in simple linear regression, but newspaper and radio have
both decreased. This is expected when predictors are correlated.
The multiple linear regression fit takes the correlation between predictors 
into account. This fit states that once we take TV and radio into account, newspaper 
ads do not increase sales.

# Questions we can answer with a multiple linear regression

Now that we have a better sense of what a multiple linear regression is, let us
focus on the questions we might want to answer with with the help of such
a regression:

1. Can any of the predictors predict response?
2. If so, to what degree are the predictors important?
3. How well does the model explain the data?
4. What is the model's prediction accuracy?

## Can any of the predictors predict response?

The hypothesis that no predictor predicts response can be coded as the null hypothesis

$$H_0: \beta_0 = \beta_1 = ... = \beta_p = 0$$

The alternate hypothesis is then 

$$H_A: \text{at least one } \beta_j \text{ is not 0}$$

We can use the $F$ statistic to test the null hypothesis.
The $F$-test (named in honor of [R. A. Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher))
uses the ratio:

$$F = \frac{\text{explained variance}}{\text{unexplained variance}}$$

In a model in which our assumptions hold, and for which our null hypothesis is true, 
we expect the explained and unexplained variances to be equal. Therefore, under the 
null hypothesis, the $F$-statistic should be around 1.

We can estimate the variance explained by our model as

$$\frac{TSS-RSS}{p}$$

For any model, as we increase the number of predictors the
amount of variance that we explain with our model increases. 
The denominator, $p$, corrects for this.

We can estimate the unexplained variance as

$$\frac{RSS}{(n-p-1)}$$

As with the explained variance above, $p$ in this denominator corrects for 
how unexplained variance decreases as we add more predictors. We can also see
that as our sample size, $n$, increases, our RSS increases as well.

This leads to the following equation for calculating the $F$-statistic for a linear model:

$$F = \left. {\frac{(TSS - RSS)}{p}} \middle/ {\frac{RSS}{(n-p-1)}} \right.$$

If our null hypothesis is not true, then, assuming that the $TSS$ remains constant,
we can expect the $RSS$ to become smaller compared to the $TSS$, and
for the ratio to increase. Therefore, the larger the $F$-statistic, the more evidence there is for
rejecting the null hypothesis.

`R` will happily calculate the $F$-statistic of a linear model fit for us. One way to get the
$F$ statistic is using the `summary()` function:

```{r}
summary(lm(sales ~ TV + newspaper + radio, data=ads))
```

In this case, the $F$-statistic is 570. That looks like a lot, and the low
$p$-value confirms that it is.

### Excercise: playing around with $n$

We can get an idea of how sample size changes the $F$-statistic and the 
$p$-value associated with the statistic. 

We can use `sample()` to reduce the sample size:

```{r, results=FALSE}
?sample
```

We want to be sure to sample without replacement, and the help page tells us that this is the default

```{r}
summary(lm(sales ~ TV + newspaper + radio, data=ads[sample(nrow(ads), 200),]))
summary(lm(sales ~ TV + newspaper + radio, data=ads[sample(nrow(ads), 50),]))
summary(lm(sales ~ TV + newspaper + radio, data=ads[sample(nrow(ads), 10),]))
```

It looks like the signal is very strong, and we can still reject our null hypothesis 
with 10 data points.

The model summaries have p-values next to each model coefficient. 
These p-values are based on the $t$-statistic and are calculated in the same way as 
for simple linear regression (see previous section). 
Why can't we just use these p-values instead of the $F$-statistic to determine
if any coefficients are non-zero? 
Without correction for multiple testing we will get false positives at the probability of
$\alpha$ per test.
If we do correct for multiple testing, then the $F$-test is more powerful and/or
requires fewer assumptions about the correlation structure of the $t$-statistics.

## Which predictors predict response?

Usually, we will not want to include all possible predictors in our model. 
But how can we choose the predictors? We could use the $t$-test, but then 
we will have to deal with false positives. The process of choosing predictors
in this way is called *variable selection*. There is a substantial body of 
literature on variable selection in linear models.

Out of all possible models, we would like to choose the one that is closest
to the true model. We face two problems in this endeavor:

1. Determining which model fits best while correcting for multiple testing and overfitting
2. The number of models that can be created from a subset of $p$ predictors is
   $2^p$, and this number can get large very quickly.

For point one there are several statistics that can be used. These statistics
include *Mallow's C*, *Akaike's Information Criterion* (AIC), 
and the *Bayesian Information Criterion* (BIC). These statistics balance the number 
of predictors used against the amount of variance explained by a model.

For point two we can use a step-wise selection process in which we add, remove,
or add and remove
predictors in order to minimize an information criterion. This approach does not
explore the full model space, but in practice it can be quite useful.

### Exercise: Let's select predictors in a model

The `stepAIC()` function can be used to perform step-wise model selection in `R`.
By default it uses AIC, but other statistics are also supported.

We can start with the full model:
```{r}
library(MASS)
full = lm(sales ~ TV + newspaper + radio, data=ads)
summary(stepAIC(full))
```

This returns a model in which newspaper (unsurprisingly) has been removed.

Or we can start with an empty model:

```{r}
empty = lm(sales ~ 1, data=ads)
summary(stepAIC(empty, scope=sales ~ TV + newspaper + radio))
```

And in this case we get the same model as above.

## How well does the model explain the data?

As in simple linear regression, two important statistics for assessing the
quality of the model fit are $R^2$ and RSE. 

### $R^2$

- In simple linear linear regression: $R^2 = \text{Cor}(X, Y)^2$
- In multiple linear regression: $R^2 = \text{Cor}(Y, \hat{Y})$

Let's see what the $R^2$ of our models look like:

```{r}
summary(lm(sales ~ TV , data=ads))$r.squared
summary(lm(sales ~ TV + radio, data=ads))$r.squared
summary(lm(sales ~ TV + radio + newspaper, data=ads))$r.squared
```

As we add predictors $R^2$ always increases. When the predictors explain no variance,
then $R^2 = 0$. When they explain all variance, then $R^2 = 1$.

### RSE

- In simple linear regression we learned: $\text{RSE} = \sqrt{RSS/(n-2)}$
- In multiple linear regression: $\text{RSE} = \sqrt{RSS/(n-p-1)}$

Let's see what the RSE of our models look like:

```{r}
summary(lm(sales ~ TV , data=ads))$sigma
summary(lm(sales ~ TV + radio, data=ads))$sigma
summary(lm(sales ~ TV + radio + newspaper, data=ads))$sigma
```

- The model that includes `newspaper` has a higher RSE than the model with only
`TV` and `radio`
- This is because RSE depends on $p$

### Visual inspection

- Plotting the data can reveal problems that are not evident from metrics

The model that includes `TV` and `radio` has some issues. Let's see if we can discover
them by visualization. 

Let's start with my favorite quality assessment plot: the quantiles plot:

```{r, collapse=TRUE}
fit = lm(sales ~ TV + radio, data=ads)
plot(fit, which=2)
```

This is not what I would expect normally distributed residuals to look like!

Let's see if we can get a better idea of where things are going wrong. 
Let's try and plot $\hat{Y}$ (fitted values) versus the residuals:

```{r}
# NB: predict()  
plot(fit, which=1)
```

We see a dip in residuals and an increase in variance towards the middle of the range
of fitted values. It looks like our model is not adequate. 

In fact, this model fails to capture a synergy between TV and radio advertizing:

```{r}
# Here we rely on resid() returning the residuals in the same order as the ads data.frame
plot_dat = data.frame(TV=ads$TV, radio=ads$radio, residual=resid(fit))

library(ggplot2)
ggplot(aes(x=TV, y=radio, color=residual), data=plot_dat) + geom_point() +
  scale_color_gradient2()
```

- The model overestimates sales generated from investment in a single ads platform 
  (top left and bottom right)
- The model underestimates sales generated from investment in both add platforms
  (top right and bottom left)
  
We can model this synergy as an "interaction term". Unfortunately, interaction terms
are beyond the scope of this session. 
See [further reading](#further-reading) for more on interaction terms.

## What is the model's prediction accuracy?

- Previously: we can make predictions from our model using the `predict()` function.

```{r}
# making a prediction on a new data point
predict(fit, newdata=data.frame(TV=100, radio=30))
```

There are three sources of error when making predictions from a linear model

1. *reducible error*: A result of the difference between the estimates 
   $$\hat{Y} = \hat\beta_0 + \hat\beta_1 X_1 + ... + \hat\beta_p X_p$$
   and the *true population regression plane*
   $$f(x) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p$$
   We use *confidence intervals* to estimate this error.
2. *model bias*: When our model differs from the true model 
   (see for example the previous section)
3. *irreducible error*: The random noise that is part of our system, $\epsilon$. We can
   use *prediction intervals* to estimate this error.

If we assume that we have the correct model, then we can ask two kinds of questions:

1. How far is $\hat{Y}$ from $f(x)$? 
   - We use *confidence intervals* to talk about how 
     our estimate of average sales will differ from the true average of sales
2. How far is any one prediction from its true value?
   - For this, we use *prediction intervals*

Prediction intervals are always larger than confidence intervals because prediction
intervals quantify both the reducible and irreducible error.

Let's try and calculate the confidence and prediction intervals around $\hat{Y}$ of our
(dubious) model fit:

```{r}
preds = as.data.frame(predict(fit, interval="confidence"))
head(preds)
preds2 = as.data.frame(predict(fit, interval="prediction"))
head(preds2)
preds$lwr_pred = preds2$lwr
preds$upr_pred = preds2$upr

preds = preds[order(preds$fit),]
preds$index = 1:nrow(preds)
head(preds)
```

```{r, collapse=TRUE}
plot(preds$fit, type='l')
lines(preds$index, preds$upr, col='red')
lines(preds$index, preds$lwr, col='red')
lines(preds$index, preds$upr_pred, col='blue')
lines(preds$index, preds$lwr_pred, col='blue')
```

We can see that the prediction intervals are larger than the confidence intervals. 
However, neither the confidence intervals nor the prediction intervals are valid here.

**Caveats**:

  - Our model does not fit the data well, and so we are also dealing with model bias.
    The confidence interval calculations assume a good model fit, which is clearly not 
    the case here.
  - Prediction intervals are, of course, for new data. Being able to calculate prediction
    accuracy on new data from training data is a substantial upside of linear models 
    (provided their assumptions are met)

# Further reading

There is much more to linear regression. Section 3.3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/data.html) is worth a read before you start 
fitting linear models to your data. That section discusses the following topics:

  - Qualitative predictors 
  - Interaction terms
  - Non-linear transformation of the predictors 
  - Potential problems: non-linearity, collinearity, outliers, heteroskedasticity
  - Logistic regression
  
The R builtin functions for visualization are sometimes not as helpful for quickly 
looking at data in many different ways. I find the `R` library
[`ggplot2`](https://ggplot2.tidyverse.org/) very useful in such cases.
