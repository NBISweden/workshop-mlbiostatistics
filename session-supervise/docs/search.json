[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to supervised learning",
    "section": "",
    "text": "Aims\n\nto introduce supervised learning for classification and regression\n\nLearning outcomes\n\nto be able to explain supervised learning\nto be able to split data into training, validation and test sets\nto be able to explain basic performance metrics for classification and regression\nto be able to use knn() function to select the optimal value of \\(k\\) and build KNN classifier\n\nDo you see a mistake or a typo? We would be grateful if you let us know via edu.ml-biostats@nbis.se\nThis repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to supervised learning",
    "section": "",
    "text": "When we talked earlier about PCA and clustering, we were interested in finding patterns in the data. We treated data set as a whole, using all the samples, and we did not use samples labels in any way to find the components with the highest variables (PCA) or the number of clusters (k-means).\nIn supervised learning, we are using samples labels to build (train) our models. When then use these trained models for interpretation and prediction."
  },
  {
    "objectID": "intro.html#supervised-classification",
    "href": "intro.html#supervised-classification",
    "title": "1  Introduction to supervised learning",
    "section": "1.2 Supervised classification",
    "text": "1.2 Supervised classification\n\nClassification methods are algorithms used to categorize (classify) objects based on their measurements.\nThey belong under supervised learning as we usually start off with labeled data, i.e. observations with measurements for which we know the label (class) of.\nIf we have a pair \\(\\{\\mathbf{x_i}, g_i\\}\\) for each observation \\(i\\), with \\(g_i \\in \\{1, \\dots, G\\}\\) being the class label, where \\(G\\) is the number of different classes and \\(\\mathbf{x_i}\\) a set of exploratory variables, that can be continuous, categorical or a mix of both, then we want to find a classification rule \\(f(.)\\) (model) such that \\[f(\\mathbf{x_i})=g_i\\]"
  },
  {
    "objectID": "intro.html#knn-example",
    "href": "intro.html#knn-example",
    "title": "1  Introduction to supervised learning",
    "section": "1.3 KNN example",
    "text": "1.3 KNN example\n\n\n\n\n\nFigure 1.1: An example of k-nearest neighbours algorithm with k=3; in the top new observation (blue) is closest to three red triangales and thus classified as a red triangle; in the bottom, a new observation (blue) is closest to 2 black dots and 1 red triangle thus classified as a black dot (majority vote)"
  },
  {
    "objectID": "intro.html#data-splitting",
    "href": "intro.html#data-splitting",
    "title": "1  Introduction to supervised learning",
    "section": "1.4 Data splitting",
    "text": "1.4 Data splitting\n\n1.4.1 train, validation & test sets\n\nPart of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\nAs a results the trained model may not be generalizable to future data due to the added complexity that only works for given unique data set, leading to so called overfitting.\nTo deal with overconfident estimation of future performance we randomly split data into training data, validation data and test data.\nCommon split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\nTraining data: this is data used to fit (train) the classification model, i.e. derive the classification rule\nValidation data: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters\nTest data: this data is used to give an estimate of future prediction performance for the model and parameters chosen\n\n\n\n\n\n\nFigure 1.2: Example of splitting data into train (50%), validation (25%) and test (25%) set\n\n\n\n\n\n\n1.4.2 cross validation\n\nIt could happen that despite random splitting in train/validation/test dataset one of the subsets does not represent data. e.g. gets all the difficult observation to classify.\nOr that we do not have enough data in each subset after performing the split.\nIn K-fold cross-validation we split data into \\(K\\) roughly equal-sized parts.\nWe start by setting the validation data to be the first set of data and the training data to be all other sets.\nWe estimate the validation error rate / correct classification rate for the split.\nWe then repeat the process \\(K-1\\) times, each time with a different part of the data set to be the validation data and the remainder being the training data.\nWe finish with \\(K\\) different error of correct classification rates.\nIn this way, every data point has its class membership predicted once.\nThe final reported error rate is usually the average of \\(K\\) error rates.\n\n\n\n\n\n\nFigure 1.3: Example of k-fold cross validaiton split (k = 3)\n\n\n\n\n\nLeave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.\n\n\n\n\n\n\nFigure 1.4: Example of LOOCV, leave-out-out cross validation"
  },
  {
    "objectID": "intro.html#evaluating-classification-model-performance",
    "href": "intro.html#evaluating-classification-model-performance",
    "title": "1  Introduction to supervised learning",
    "section": "1.5 Evaluating Classification Model Performance",
    "text": "1.5 Evaluating Classification Model Performance\n\nTo train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of \\(k\\) in KNN.\nThere are few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\nCommon measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n\nCorrect (miss)classification rate\n\nThe simplest way to evaluate in which we count for all the \\(n\\) predictions how many times we got the classification right.\n\n\\[Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}\\]\nwhere \\(1[]\\) is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\nMissclassification Rate = 1 - Correct Classification Rate"
  },
  {
    "objectID": "intro.html#putting-it-together-k-nearest-neighbours",
    "href": "intro.html#putting-it-together-k-nearest-neighbours",
    "title": "1  Introduction to supervised learning",
    "section": "1.6 Putting it together: k-nearest neighbours",
    "text": "1.6 Putting it together: k-nearest neighbours\nAlgorithm\n\nDecide on the value of \\(k\\)\nCalculate the distance between the query-instance (new observation) and all the training samples\nSort the distances and determine the nearest neighbors based on the \\(k\\)-th minimum distance\nGather the categories of the nearest neighbors\nUse simple majority of the categories of the nearest neighbors as the prediction value of the new observation\n\nEuclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhatan distance, maximum distance etc.\nchoosing k\n\nfor problems with 2 classes, choose an odd number of \\(k\\) to avoid ties\nuse validation data to fit the model for a series of \\(k\\) values\npick the value of \\(k\\) which results in the best model (as assessed by the method of choice, e.g. overall classification rate)\n\nLet’s see how it works in practice on a classical iris dataset containing measurements on petals and sepals as well as species information (setosa, versicolor, virginica)\n\nlibrary(class) # library with knn() function\nlibrary(splitTools) # load library for data splitting\n## Warning: package 'splitTools' was built under R version 4.0.5\n\n# preview iris dataset\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\ntail(iris)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 145          6.7         3.3          5.7         2.5 virginica\n## 146          6.7         3.0          5.2         2.3 virginica\n## 147          6.3         2.5          5.0         1.9 virginica\n## 148          6.5         3.0          5.2         2.0 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9         3.0          5.1         1.8 virginica\n\n# summary statistics\nsummary(iris)\n##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \n\n# split data into train 50%, validation 25% and test dataset 25%\n# \nrandseed <- 102\nset.seed(randseed)\ninds <- partition(iris$Species, p = c(train = 0.5, valid = 0.25, test = 0.25), seed = randseed)\nstr(inds)\n## List of 3\n##  $ train: int [1:74] 1 4 5 6 8 11 13 15 17 19 ...\n##  $ valid: int [1:38] 3 10 12 18 21 27 28 30 32 33 ...\n##  $ test : int [1:38] 2 7 9 14 16 20 23 29 31 35 ...\n\ndata.train <- iris[inds$train, ]\ndata.valid <- iris[inds$valid,]\ndata.test <- iris[inds$test, ]\n\ndim(data.train)\n## [1] 74  5\ndim(data.valid)\n## [1] 38  5\ndim(data.test)\n## [1] 38  5\n\nsummary(data.train$Species)\n##     setosa versicolor  virginica \n##         24         25         25\nsummary(data.valid$Species)\n##     setosa versicolor  virginica \n##         13         12         13\nsummary(data.test$Species)\n##     setosa versicolor  virginica \n##         13         13         12\n\n# run knn with different values of k from 1, 3, 5 to 51\nk.values <- seq(1, 51, 2)\nclass.rate <- rep(0, length(k.values)) # allocate empty vector to collect correct classification rates\nfor (k in seq_along(k.values))\n{\n  \n  pred.class <- knn(train = data.train[, -5], \n                    test = data.valid[, -5], \n                    cl = data.train[, 5], k.values[k])\n  \n  class.rate[k] <- sum((pred.class==data.valid[,5]))/length(pred.class)\n}\n\n# for which value of k we reach the highest classification rate\nk.best <- k.values[which.max(class.rate)]\nprint(k.best)\n## [1] 7\n\n# plot classification rate as a function of k\nplot(k.values, class.rate, type=\"l\", xlab=\"k\", ylab=\"class. rate\")\n\n\n\n\n\n\n\n\n# how would our model perform on the future data using the optimal k?\npred.class <- knn(train = data.train[, -5], data.test[, -5], data.train[,5], k=k.best)\nclass.rate <- sum((pred.class==data.test[,5]))/length(pred.class)\nprint(class.rate)\n## [1] 0.9473684"
  },
  {
    "objectID": "intro.html#going-back-to-regression",
    "href": "intro.html#going-back-to-regression",
    "title": "1  Introduction to supervised learning",
    "section": "1.7 Going back to regression",
    "text": "1.7 Going back to regression\n\nThe idea of using data splits to train the model holds for fitting (training) regression models.\nEarlier, we used the entire data set to fit the model and we used the fitted model for prediction given a new observation.\nIf we were to use regression in supervised learning context, we would use data splits to train and assess the regression model.\nFor instance, given a number of variables of interest, we could try to find the best regression model using train data to fit the model and assess on the validation data; while keeping the test to assess the performance on the final model. Or we could use cross validation (We have seen before how to fit the model and assess the model fit, e.g. with \\(R^2\\).)\nOther popular regression performance metrics include RMSE, root mean square error \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y_i})^2}\\]\nand MAE, mean absolute error, \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y_i}|\\]"
  },
  {
    "objectID": "intro-exercises.html",
    "href": "intro-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercise 1 (Breast Cancer classifier) Given BreastCancer data shown below build the best KNN classification model that you can to predict the cancer type (benign or malignant).\nNote: you may need to do some data cleaning.\n\nFirst column contains patients IDs that should not be used\nData set contains some missing values. You can keep only the complete cases e.g. using na.omit() function."
  },
  {
    "objectID": "intro-exercises.html#answers-to-selected-exercises",
    "href": "intro-exercises.html#answers-to-selected-exercises",
    "title": "Exercises",
    "section": "Answers to selected exercises",
    "text": "Answers to selected exercises\n\nSolution. Exercise 1\n\n\n\nCode\nlibrary(splitTools)\nlibrary(class)\n\n# set random seed to be able to repeat analysis\nrandseed <- 123\nset.seed(randseed)\n\n# clean data by removing first column of IDs\ndata.cancer <- BreastCancer[, -1]\n\n# keep only complete cases\ndata.cancer <- na.omit(data.cancer)\n\n# split data into train, validation and test\ninds <- partition(data.cancer$Class,\n                  p = c(train = 0.5, valid = 0.25, test = 0.25),\n                  seed = randseed)\n\n# preview lists with splits\nstr(inds)\n## List of 3\n##  $ train: int [1:341] 2 3 4 8 13 17 18 20 21 24 ...\n##  $ valid: int [1:171] 7 9 10 11 22 23 25 30 35 49 ...\n##  $ test : int [1:171] 1 5 6 12 14 15 16 19 26 27 ...\n\n# make train, validation and test sets\ndata.train <- data.cancer[inds$train, ]\ndata.valid <- data.cancer[inds$valid,]\ndata.test <- data.cancer[inds$test, ]\n\n# check their dimensions\ndim(data.train)\n## [1] 341  10\ndim(data.valid)\n## [1] 171  10\ndim(data.test)\n## [1] 171  10\n\n# and print our group summaries\nsummary(data.train$Class)\n##    benign malignant \n##       222       119\nsummary(data.valid$Class)\n##    benign malignant \n##       111        60\nsummary(data.test$Class)\n##    benign malignant \n##       111        60\n\n# find optimal value of k\nk.values <- seq(1, 51, 2)\nclass.rate <- rep(0, length(k.values))\nfor (k in seq_along(k.values))\n{\n\n  pred.class <- knn(train = data.train[, -10],\n                    test = data.valid[, -10],\n                    cl = data.train[,10], k=k.values[k])\n\n  class.rate[k] <- sum((pred.class==data.valid[,10]))/length(pred.class)\n}\n\n# for which value of k we reach the highest classification rate\nk.best <- k.values[which.max(class.rate)]\nprint(k.best)\n## [1] 13\n\n# plot classification rate as a function of k\nplot(k.values, class.rate, type=\"l\", xlab=\"k\", ylab=\"class. rate\")\n\n\n\n\n\nCode\n\n# how would our model perform on the future data using the optimal k?\npred.class <- knn(train = data.train[, -10], data.test[, -10], data.train[,10], k=k.best)\nclass.rate <- sum((pred.class==data.test[,10]))/length(pred.class)\nprint(class.rate)\n## [1] 0.9707602\n\n\nOther solutions could include:\n\ntrying and comparing models with different number of variables\ntrying to implement k-fold cross validation or LOOVC to assess model performance when selecting the optimal value of \\(k\\)\n\n\nSolution. Exercise 2\n\n\nlibrary(tidyverse)\nlibrary(splitTools)\n\n# access data\n# data(fat, package = \"faraway\")\n# data.fat <- fat\n\ndata.fat <- read_csv(\"data/brozek.csv\")\n\n# split into train, validation and test: stratify by Brozek score\ninds <- partition(data.fat$brozek,\n                  p = c(train = 0.6, valid = 0.2, test = 0.2),\n                  seed = randseed)\nstr(inds)\n## List of 3\n##  $ train: int [1:145] 1 2 4 5 6 7 11 12 13 15 ...\n##  $ valid: int [1:54] 3 8 9 10 14 18 22 27 29 34 ...\n##  $ test : int [1:53] 16 37 38 40 42 43 48 49 54 62 ...\n\ndata.train <- data.fat[inds$train, ]\ndata.valid <- data.fat[inds$valid,]\ndata.test <- data.fat[inds$test, ]\n\n# Model 1\nm1 <- lm(brozek ~ age, data = data.train) # fit model on train\nm1.pred <- predict(m1, newdata = data.valid[,-1]) # predict brozek score using validation set\nm1.rmse <- sqrt((1/nrow(data.valid))*sum((data.valid$brozek - m1.pred)^2)) # calculate RMSE\n\n# Model 2\nm2 <- lm(brozek ~ age + weight + height, data = data.train) # fit model on train\nm2.pred <- predict(m2, newdata = data.valid[,-1]) # predict brozek score using validation set\nm2.rmse <- sqrt((1/nrow(data.valid))*sum((data.valid$brozek - m2.pred)^2)) # calculate RMSE\n\n# Model 3\nm3 <- lm(brozek ~ age + wrist, data = data.train) # fit model on train\nm3.pred <- predict(m3, newdata = data.valid[,-1]) # predict brozek score using validation set\nm3.rmse <- sqrt((1/nrow(data.valid))*sum((data.valid$brozek - m3.pred)^2)) # calculate RMSE\n\n# Compare models\nrmse <- data.frame(model = c(\"Model 1\", \"Model 2\", \"Model 3\"), rmse = c(m1.rmse, m2.rmse, m3.rmse))\nrmse\n##     model     rmse\n## 1 Model 1 6.970207\n## 2 Model 2 6.281986\n## 3 Model 3 6.543864\n\n# Out of the three models, Model 2, has the smallest RMSE and is thus selected as best\n\n# Expected performance on the test data\nm.pred <- predict(m2, newdata = data.test[,-1])\nsqrt((1/nrow(data.test))*sum((data.test$brozek - m.pred)^2))\n## [1] 8.129801\n\nNote: It is possible to use glm() function to fit linear regression. The advantage of this is that one can use easily use cross validation with cv.glm(). More about that on Friday."
  }
]