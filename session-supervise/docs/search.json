[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to supervised learning",
    "section": "",
    "text": "Preface\nAims\n\nto introduce supervised learning for classification and regression\n\nLearning outcomes\n\nto be able to explain supervised learning\nto be able to split data into training, validation and test sets\nto be able to explain basic performance metrics for classification and regression\nto be able to use kknn() function to select the optimal value of \\(k\\) and build KNN classifier\n\nDo you see a mistake or a typo? We would be grateful if you let us know via edu.ml-biostats@nbis.se\nThis repository contains teaching and learning materials prepared for and used during “Introduction to biostatistics and Machine Learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use the basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/"
  },
  {
    "objectID": "intro.html#what-is-supervised-learning",
    "href": "intro.html#what-is-supervised-learning",
    "title": "1  Supervised learning",
    "section": "1.1 What is supervised learning?",
    "text": "1.1 What is supervised learning?\n\nSupervised learning can be used for classification, e.g. given a new biopsy sample we want to tell whether it contains tumor tissue (Yes/No) and for regression, e.g. given a new measurements of the methylation sites we want to forecast epigenomic age.\nIn supervised learning we are using sample labels to train (build) a model. We then use the trained model for interpretation and prediction.\nThis is in contrast to previously discussed unsupervised learning such as clustering or PCA - methods that we were using to find patterns in the data. We treated data set a a whole, using measurements for all samples but not the samples labels such as sample groups to find the components with the highest variables (PCA) or the optimal number of clusters (k-means).\nTraining a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).\nCommon supervised machine learning algorithms include K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). Many can be implemented to work both for classifying samples and forecasting numeric outcome."
  },
  {
    "objectID": "intro.html#outline",
    "href": "intro.html#outline",
    "title": "1  Supervised learning",
    "section": "1.2 Outline",
    "text": "1.2 Outline\nAcross many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:\n\ndeciding on the task: classification or regression\n\nsplitting data to keep part of data for training and part for testing\nselecting supervised machine learning algorithms to be trained (or a set of these)\ndeciding on the training strategy, i.e. which performance metrics to use and how to search for the best model parameters\nrunning feature engineering: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables\nperforming feature selection: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present\n\nThe diagram below shows a basic strategy on how to train KNN for classification, given a data set with \\(n\\) samples, \\(p\\) variables and \\(y\\) categorical outcome\n\n\n\n\nflowchart TD\n  A([data]) -. split data \\n e.g. basic, stratified, grouped -.-&gt; B([non-test set])\n  A([data]) -.-&gt; C([test set])\n  B -.-&gt; D(choose algorithm \\n e.g. KNN)\n  D -.-&gt; E(choose evaluation metric \\n e.g. overall accuracy)\n  E -.-&gt; F(feature engineering & selection)\n  F -.-&gt; G(prepare parameter space, e.g. odd k-values from 3 to 30)\n  G -. split non-test -.-&gt; H([train set & validation set])\n  H -.-&gt; J(fit model on train set)\n  J -.-&gt; K(collect evaluation metric on validation set)\n  K -.-&gt; L{all values checked? \\n e.g. k more than 30}\n  L -. No .-&gt; J\n  L -. Yes .-&gt; M(select best parameter values)\n  M -.-&gt; N(fit model on all non-test data)\n  N -.-&gt; O(assess model on test data)\n  C -.-&gt; O\n  \n\n\n\n\n\n\n \nBefore we see how this training may look like in R, let’s talk more about\n\nKNN, K-nearest neighbor algorithm\ndata splitting and\nperformance metrics useful for evaluating models\n\nAnd we will leave feature engineering and feature selection for the next session."
  },
  {
    "objectID": "intro.html#classification",
    "href": "intro.html#classification",
    "title": "1  Supervised learning",
    "section": "1.3 Classification",
    "text": "1.3 Classification\n\nClassification methods are algorithms used to categorize (classify) objects based on their measurements.\nThey belong under supervised learning as we usually start off with labeled data, i.e. observations with measurements for which we know the label (class) of.\nIf we have a pair \\(\\{\\mathbf{x_i}, g_i\\}\\) for each observation \\(i\\), with \\(g_i \\in \\{1, \\dots, G\\}\\) being the class label, where \\(G\\) is the number of different classes and \\(\\mathbf{x_i}\\) a set of exploratory variables, that can be continuous, categorical or a mix of both, then we want to find a classification rule \\(f(.)\\) (model) such that \\[f(\\mathbf{x_i})=g_i\\]"
  },
  {
    "objectID": "intro.html#knn-example",
    "href": "intro.html#knn-example",
    "title": "1  Supervised learning",
    "section": "1.4 KNN example",
    "text": "1.4 KNN example\n\n\n\n\n\nFigure 1.1: An example of k-nearest neighbors algorithm with k=3; A) in the top a new sample (blue) is closest to three red triangle samples based on its gene A and gene B measurements and thus is classified as a red (B); in the bottom (C), a new sample (blue) is closest to 2 black dots and 1 red triangle based on its gene A and B measurements and is thus classified by majority vote as a black dot (D).\n\n\n\n\nAlgorithm\n\nDecide on the value of \\(k\\)\nCalculate the distance between the query-instance (observations for new sample) and all the training samples\nSort the distances and determine the nearest neighbors based on the \\(k\\)-th minimum distance\nGather the categories of the nearest neighbors\nUse majority voting of the categories of the nearest neighbors as the prediction value for the new sample\n\nEuclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc."
  },
  {
    "objectID": "intro.html#data-splitting",
    "href": "intro.html#data-splitting",
    "title": "1  Supervised learning",
    "section": "1.5 Data splitting",
    "text": "1.5 Data splitting\n\nPart of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\nAs a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to overfitting.\nTo deal with overconfident estimation of future performance we can implement various data splitting strategies.\n\n\ntrain, validation & test sets\n\nCommon split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\nTraining data: this is data used to fit (train) the classification or regression model, i.e. derive the classification rule\nValidation data: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters\nTest data: this data is used to give an estimate of future prediction performance for the model and parameters chosen\n\n\n\n\n\n\nFigure 1.2: Example of splitting data into train (50%), validation (25%) and test (25%) set\n\n\n\n\n\n\ncross validation\n\nIt can happen that despite random splitting in train/validation/test dataset one of the subsets does not represent data. e.g. gets all the difficult observation to classify.\nOr that we do not have enough data in each subset after performing the split.\nIn k-fold cross-validation we split data into \\(k\\) roughly equal-sized parts.\nWe start by setting the validation data to be the first set of data and the training data to be all other sets.\nWe estimate the validation error rate / correct classification rate for the split.\nWe then repeat the process \\(k-1\\) times, each time with a different part of the data set to be the validation data and the remainder being the training data.\nWe finish with \\(k\\) different error or correct classification rates.\nIn this way, every data point has its class membership predicted once.\nThe final reported error rate is usually the average of \\(k\\) error rates.\n\n\n\n\n\n\nFigure 1.3: Example of k-fold cross validation split (k = 3)\n\n\n\n\n\n\nrepeated cross validation\n\nIn repeated cross-validation we are repeating the cross-validation many times, e.g. we can create 5 validation folds 3 times\n\n\n\nLeave-one-out cross-validation\n\nLeave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.\n\n\n\n\n\n\nFigure 1.4: Example of LOOCV, leave-one-out cross validation"
  },
  {
    "objectID": "intro.html#evaluating-classification",
    "href": "intro.html#evaluating-classification",
    "title": "1  Supervised learning",
    "section": "1.6 Evaluating classification",
    "text": "1.6 Evaluating classification\n\nTo train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of \\(k\\) in KNN.\nThere are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\nCommon measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n\nCorrect (miss)classification rate\n\nThe simplest way to evaluate in which we count for all the \\(n\\) predictions how many times we got the classification right. \\[Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}\\] where \\(1[]\\) is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\n\nMissclassification Rate\nMissclassification Rate = 1 - Correct Classification Rate\nConfusion matrix\nConfusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes. For a binary classifier we have:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nBased on the confusion matrix, we can derive common performance metrics of a binary classifier:\n\nAccuracy: measures the proportion of correctly classified samples over the total number of samples. \\[ACC = \\frac{TP+TN}{TP+TN+FP+FN}\\].\nSensitivity: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as true positive rate and recall. \\[TPR = \\frac{TP}{TP + FN}\\]\nSpecificity: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as true negative rate and selectivity. \\[TNR = \\frac{TN}{TN+FP}\\]\nPrecision: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives. It is also known as positive predictive value \\[PPV = \\frac{TP}{TP + FP}\\]\nROC AUC: the receiver operating characteristic (ROC) curve is a graphical representation of the trade off between sensitivity and specificity for different threshold values. The area under the ROC curve (AUC) is a performance metric that ranges from 0 to 1, with a higher value indicating better performance. AUC is a measure of how well the classifier is able to distinguish between positive and negative samples."
  },
  {
    "objectID": "intro.html#evaluating-regression",
    "href": "intro.html#evaluating-regression",
    "title": "1  Supervised learning",
    "section": "1.7 Evaluating regression",
    "text": "1.7 Evaluating regression\nThe idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include:\n\nR-squared: As seen in the linear regression session. \\[\nR^2=1-\\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n\\]\nAdjusted R-squared: seen before \\[\nR_{adj}^2=1-\\frac{RSS}{TSS}\\frac{n-1}{n-p-1} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\frac{n-1}{n-p-1}\n\\]\nMean Squared Error (MSE): average squared difference between the predicted values and the actual values. \\[MSE = \\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2\\]\nRoot Mean Squared Error (RMSE): square root of the MSE \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2}\\]\nMAE: average absolute difference between the predicted values and the actual values \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y}_i|\\]\nMean Absolute Percentage Error (MAPE): average percentage difference between the predicted values and the actual values."
  },
  {
    "objectID": "KNN-demo.html",
    "href": "KNN-demo.html",
    "title": "2  Demo: KNN model for classification",
    "section": "",
    "text": "Let’s try to build a classifier to predict obesity (Obese vs Non-obese) given our diabetes data set. To start simple:\n\nwe will see how well we can predict obesity given waist and hdl variables\nwe will use data splitting into train, validation and test, i.e. not cross-validation with the help of splitTools() library\nwe will use KNN algorithm as implemented in kknn() function in library(kknn)\n\nReading in data\n\n# load libraries\nlibrary(tidyverse)\nlibrary(splitTools)\nlibrary(kknn)\n\n# input data\ninput_diabetes &lt;- read_csv(\"data/data-diabetes.csv\")\n\n# clean data\ninch2cm &lt;- 2.54\npound2kg &lt;- 0.45\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(height  = height * inch2cm / 100, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2cm) %&gt;% \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;%\n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %&gt;%\n  mutate(diabetic = ifelse(glyhb &gt; 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %&gt;%\n  mutate(location = factor(location)) %&gt;%\n  mutate(frame = factor(frame)) %&gt;%\n  mutate(gender = factor(gender))\n  \n# select data for KNN\ndata_input &lt;- data_diabetes %&gt;%\n  select(obese, waist, hdl) %&gt;%\n  na.omit()\n\n# How many obese and non-obese in our data set?\ndata_input %&gt;%\n  count(obese)\n## # A tibble: 2 × 2\n##   obese     n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 No      250\n## 2 Yes     144\n\n# preview data\nglimpse(data_input)\n## Rows: 394\n## Columns: 3\n## $ obese &lt;fct&gt; No, Yes, Yes, No, No, No, No, Yes, No, Yes, No, Yes, Yes, Yes, N…\n## $ waist &lt;dbl&gt; 73.66, 116.84, 124.46, 83.82, 111.76, 91.44, 116.84, 86.36, 86.3…\n## $ hdl   &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 47, …\n\ndata_input %&gt;%\n  ggplot(aes(x = waist, y = hdl, fill = obese)) + \n  geom_point(shape=21, alpha = 0.7, size = 2) + \n  theme_bw() + \n  scale_fill_manual(values = c(\"blue3\", \"brown1\")) + \n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nSplitting data\n\n# split data into train (40%), validation (40%) and test (20%)\n# stratify by obese\nrandseed &lt;- 123\nset.seed(randseed)\ninds &lt;- partition(data_input$obese, p = c(train = 0.4, valid = 0.4, test = 0.2), seed = randseed)\nstr(inds)\n## List of 3\n##  $ train: int [1:158] 6 8 11 16 17 22 26 28 32 35 ...\n##  $ valid: int [1:157] 1 3 5 7 9 10 13 14 15 24 ...\n##  $ test : int [1:79] 2 4 12 18 19 20 21 23 27 30 ...\ndata_train &lt;- data_input[inds$train, ]\ndata_valid &lt;- data_input[inds$valid,]\ndata_test &lt;- data_input[inds$test, ]\n\n# check dimensions of data\ndata_train %&gt;% dim()\n## [1] 158   3\ndata_valid %&gt;% dim()\n## [1] 157   3\ndata_test %&gt;% dim()\n## [1] 79  3\n\n# check distribution of obese and non-obese\ndata_train %&gt;%\n  group_by(obese) %&gt;%\n  count()\n## # A tibble: 2 × 2\n## # Groups:   obese [2]\n##   obese     n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 No      100\n## 2 Yes      58\n\ndata_valid %&gt;%\n  group_by(obese) %&gt;%\n  count()\n## # A tibble: 2 × 2\n## # Groups:   obese [2]\n##   obese     n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 No      100\n## 2 Yes      57\n\ndata_test %&gt;%\n  group_by(obese) %&gt;%\n  count()\n## # A tibble: 2 × 2\n## # Groups:   obese [2]\n##   obese     n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 No       50\n## 2 Yes      29\n\nTraining KNN\n\n# prepare parameters search space\nn &lt;- nrow(data_train)\nk_values &lt;- seq(1, n-1, 2) # check every odd value of k between 1 and number of samples-1\n\n# allocate empty vector to collect overall classification rate for each k\ncls_rate &lt;- rep(0, length(k_values)) \n\nfor (l in seq_along(k_values))\n{\n  \n  # fit model given k value\n  model &lt;- kknn(obese ~., data_train, data_valid, \n                k = k_values[l], \n                kernel = \"rectangular\")\n  \n  # extract predicted class (predicted obesity status)\n  cls_pred &lt;- model$fitted.values\n  \n  # define actual class (actual obesity status)\n  cls_true &lt;- data_valid$obese\n  \n  # calculate overall classification rate\n  cls_rate[l] &lt;- sum((cls_pred == cls_true))/length(cls_pred)\n  \n}\n\nSelecting best \\(k\\)\n\n# plot classification rate as a function of k\nplot(k_values, cls_rate, type=\"l\", xlab=\"k\", ylab=\"cls rate\")\n\n\n\n\nOverall classification rate as a function of k\n\n\n\n# For which value of k do we reach the highest classification rate?\nk_best &lt;- k_values[which.max(cls_rate)]\nprint(k_best)\n## [1] 51\n\nFinal model and performance on future unseen data (test data)\n\n# How would our model perform on the future data using the optimal k?\nmodel_final &lt;- kknn(obese ~., data_train, data_test, k=k_best, kernel = \"rectangular\")\ncls_pred &lt;- model_final$fitted.values\ncls_true &lt;- data_test$obese\n\ncls_rate &lt;- sum((cls_pred == cls_true))/length(cls_pred)\nprint(cls_rate)\n## [1] 0.8481013"
  },
  {
    "objectID": "exercises.html#answers-to-exercises",
    "href": "exercises.html#answers-to-exercises",
    "title": "Exercises",
    "section": "Answers to exercises",
    "text": "Answers to exercises\n\nSolution. Exercise 1\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(splitTools)\nlibrary(kknn)\n\n# input data\ninput_diabetes &lt;- read_csv(\"data/data-diabetes.csv\")\n\n# clean data\ninch2cm &lt;- 2.54\npound2kg &lt;- 0.45\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(height  = height * inch2cm / 100, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2cm) %&gt;% \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;%\n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %&gt;%\n  mutate(diabetic = ifelse(glyhb &gt; 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %&gt;%\n  mutate(location = factor(location)) %&gt;%\n  mutate(frame = factor(frame)) %&gt;%\n  mutate(gender = factor(gender))\n  \n# select data for KNN\ndata_input &lt;- data_diabetes %&gt;%\n  select(obese, waist, hdl) %&gt;%\n  na.omit()\n\n# set random seed\nrandseed &lt;- 123\nset.seed(randseed)\n\n# split data into other (non-test) and test\nsplits &lt;- partition(data_input$obese, p = c(other = 0.80, test = 0.20))\ndata_test &lt;- data_input[splits$test, ]\ndata_other &lt;- data_input[splits$other, ]\n\n# create train and validation folds\nkfolds_train &lt;- create_folds(data_other$obese, k = 5, seed = randseed)\nkfolds_valid &lt;- create_folds(data_other$obese, k = 5, \n                             invert = TRUE, # gives back indices of the validation samples\n                             seed = randseed) # OBS! use the same seed as above in kfolds_train()\n\n\n# prepare parameters search space\nn &lt;- nrow(data_other)\nk_values &lt;- seq(1, 100, 2) # check every odd value of k between 1 and 50\n\n# allocate empty matrix to collect overall classification rate for each k and 5-folds\nfolds &lt;- 5\ncls_rate &lt;- matrix(data = NA, ncol = folds, nrow = length(k_values))\ncolnames(cls_rate) &lt;- paste(\"kfold\", 1:folds, sep=\"\")\nrownames(cls_rate) &lt;- paste(\"k\", k_values, sep=\"\")\n\nfor (k in seq_along(k_values))\n{\n  # for every value of k \n  # fit model on every train fold and evaluate on every validation fold\n  for (f in 1:folds){\n    \n    data_train &lt;- data_other[kfolds_train[[f]], ]\n    data_valid &lt;- data_other[kfolds_valid[[f]], ]\n    \n    # fit model given k value\n    model &lt;- kknn(obese ~., data_train, data_valid, \n                k = k_values[k], \n                kernel = \"rectangular\")\n    \n    # extract predicted class (predicted obesity status)\n    cls_pred &lt;- model$fitted.values\n  \n    # define actual class (actual obesity status)\n    cls_true &lt;- data_valid$obese\n  \n    # calculate overall classification rate\n    cls_rate[k, f] &lt;- sum((cls_pred == cls_true))/length(cls_pred)\n    \n  }\n  \n}\n\n# plot average classification rate (across folds) as a function of k\ncls_rate_avg &lt;- apply(cls_rate, 1, mean)\nplot(k_values, cls_rate_avg, type=\"l\", xlab=\"k\", ylab=\"cls rate (avg)\")\n\n\n\n\n\n\n\n\n# For which value of k do we reach the highest classification rate?\nk_best &lt;- k_values[which.max(cls_rate_avg)]\nprint(k_best)\n## [1] 71\n\n# How would our model perform on the future data using the optimal k?\nmodel_final &lt;- kknn(obese ~., data_other, data_test, k=k_best, kernel = \"rectangular\")\ncls_pred &lt;- model_final$fitted.values\ncls_true &lt;- data_test$obese\n\ncls_rate &lt;- sum((cls_pred == cls_true))/length(cls_pred)\nprint(cls_rate)\n## [1] 0.8481013\n\n\nSolution. Exercise 2\n\n\n# input and clean data\n# load libraries\nlibrary(tidyverse)\nlibrary(splitTools)\nlibrary(kknn)\n\n# input data\ninput_diabetes &lt;- read_csv(\"data/data-diabetes.csv\")\n\n# clean data\ninch2cm &lt;- 2.54\npound2kg &lt;- 0.45\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(height  = height * inch2cm / 100, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2cm) %&gt;% \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;%\n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\"No\", \"Yes\"))) %&gt;%\n  mutate(diabetic = ifelse(glyhb &gt; 7, \"Yes\", \"No\"), diabetic = factor(diabetic, levels = c(\"No\", \"Yes\"))) %&gt;%\n  mutate(location = factor(location)) %&gt;%\n  mutate(frame = factor(frame)) %&gt;%\n  mutate(gender = factor(gender))\n\n# select relevant data and remove missing data\ndata_input &lt;- \n  data_diabetes %&gt;%\n  select(BMI, age, hdl, waist) %&gt;%\n  na.omit()\nglimpse(data_input)\n## Rows: 394\n## Columns: 4\n## $ BMI   &lt;dbl&gt; 22.09, 36.92, 47.95, 18.53, 27.52, 26.39, 28.07, 34.00, 24.39, 3…\n## $ age   &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 37, 45, 55, 60, 38, 27, 40, 36, 33, …\n## $ hdl   &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 47, …\n## $ waist &lt;dbl&gt; 73.66, 116.84, 124.46, 83.82, 111.76, 91.44, 116.84, 86.36, 86.3…\n\n# define calculate_rmse() function\ncalculate_rmse &lt;- function(y_true, y_pred){\n  rmse = sqrt(mean((y_true - y_pred)^2))\n}\n\n# split into train, validation and test: stratify by BMI\nrandseed &lt;- 123\ninds &lt;- partition(data_input$BMI, \n                  p = c(train = 0.6, valid = 0.2, test = 0.2),\n                  seed = randseed)\n\ndata_train &lt;- data_input[inds$train, ]\ndata_valid &lt;- data_input[inds$valid,]\ndata_test &lt;- data_input[inds$test, ]\n\n# Model 1\nm1 &lt;- lm(BMI ~ age, data = data_train) # fit model on train\npred_bmi_1 &lt;- predict(m1, newdata = data_valid) # predict BMI using validation set\nm1_rmse &lt;- calculate_rmse(data_valid$BMI, pred_bmi_1) # calculate RMSE\n\n# Model 2\nm2 &lt;- lm(BMI ~ age + hdl, data = data_train) \npred_bmi_2 &lt;- predict(m2, newdata = data_valid) \nm2_rmse &lt;- calculate_rmse(data_valid$BMI, pred_bmi_2) \n\n# Model 3\nm3 &lt;- lm(BMI ~ age + hdl + waist, data = data_train) \npred_bmi_3 &lt;- predict(m3, newdata = data_valid) \nm3_rmse &lt;- calculate_rmse(data_valid$BMI, pred_bmi_3) \n\n# Compare models\nrmse &lt;- data.frame(model = c(\"Model 1\", \"Model 2\", \"Model 3\"), rmse = c(m1_rmse, m2_rmse, m3_rmse))\nprint(rmse)\n##     model     rmse\n## 1 Model 1 6.206929\n## 2 Model 2 5.879602\n## 3 Model 3 3.418270\n\n# Out of the three models, Model 3, has the smallest RMSE and is thus selected as best\n\n# Expected performance on the test data\npred_bmi_final &lt;- predict(m3, newdata = data_test)\nrmse_final &lt;- calculate_rmse(data_test$BMI, pred_bmi_final)\nprint(rmse_final)\n## [1] 3.180441"
  },
  {
    "objectID": "intro.html#live-demo",
    "href": "intro.html#live-demo",
    "title": "1  Supervised learning",
    "section": "1.8 Live demo",
    "text": "1.8 Live demo"
  }
]