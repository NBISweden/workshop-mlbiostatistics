<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.333">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to supervised learning - 1&nbsp; Supervised learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./KNN-demo.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<meta name="mermaid-theme" content="forest">
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./intro.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Supervised learning</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to supervised learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./KNN-demo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Demo: KNN model for classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-is-supervised-learning" id="toc-what-is-supervised-learning" class="nav-link active" data-scroll-target="#what-is-supervised-learning"><span class="header-section-number">1.1</span> What is supervised learning?</a></li>
  <li><a href="#outline" id="toc-outline" class="nav-link" data-scroll-target="#outline"><span class="header-section-number">1.2</span> Outline</a></li>
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification"><span class="header-section-number">1.3</span> Classification</a></li>
  <li><a href="#knn-example" id="toc-knn-example" class="nav-link" data-scroll-target="#knn-example"><span class="header-section-number">1.4</span> KNN example</a></li>
  <li><a href="#data-splitting" id="toc-data-splitting" class="nav-link" data-scroll-target="#data-splitting"><span class="header-section-number">1.5</span> Data splitting</a>
  <ul class="collapse">
  <li><a href="#train-validation-test-sets" id="toc-train-validation-test-sets" class="nav-link" data-scroll-target="#train-validation-test-sets">train, validation &amp; test sets</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation">cross validation</a></li>
  <li><a href="#repeated-cross-validation" id="toc-repeated-cross-validation" class="nav-link" data-scroll-target="#repeated-cross-validation">repeated cross validation</a></li>
  <li><a href="#leave-one-out-cross-validation" id="toc-leave-one-out-cross-validation" class="nav-link" data-scroll-target="#leave-one-out-cross-validation">Leave-one-out cross-validation</a></li>
  </ul></li>
  <li><a href="#evaluating-classification" id="toc-evaluating-classification" class="nav-link" data-scroll-target="#evaluating-classification"><span class="header-section-number">1.6</span> Evaluating classification</a></li>
  <li><a href="#evaluating-regression" id="toc-evaluating-regression" class="nav-link" data-scroll-target="#evaluating-regression"><span class="header-section-number">1.7</span> Evaluating regression</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Supervised learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="what-is-supervised-learning" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="what-is-supervised-learning"><span class="header-section-number">1.1</span> What is supervised learning?</h2>
<ul>
<li>Supervised learning can be used for classification, e.g.&nbsp;given a new biopsy sample we want to tell whether it contains tumor tissue (Yes/No) and for regression, e.g.&nbsp;given a new measurements of the methylation sites we want to forecast epigenomic age.</li>
<li>In supervised learning we are using sample <strong>labels</strong> to <strong>train</strong> (build) a model. We then use the trained model for interpretation and <strong>prediction</strong>.</li>
<li>This is in contrast to previously discussed unsupervised learning such as clustering or PCA - methods that we were using to find patterns in the data. We treated data set a a whole, using measurements for all samples but not the samples labels such as sample groups to find the components with the highest variables (PCA) or the optimal number of clusters (k-means).</li>
<li><strong>Training</strong> a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).</li>
<li>Common supervised machine learning algorithms include K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). Many can be implemented to work both for classifying samples and forecasting numeric outcome.</li>
</ul>
</section>
<section id="outline" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="outline"><span class="header-section-number">1.2</span> Outline</h2>
<p>Across many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:</p>
<ul>
<li>deciding on the task: classification or regression<br>
</li>
<li><strong>splitting data</strong> to keep part of data for training and part for testing</li>
<li>selecting supervised machine learning algorithms to be trained (or a set of these)</li>
<li>deciding on the training strategy, i.e.&nbsp;which <strong>performance metrics</strong> to use and how to search for the best model parameters</li>
<li>running <strong>feature engineering</strong>: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables</li>
<li>performing <strong>feature selection</strong>: reducing number of features by keeping only the relevant ones, e.g.&nbsp;by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present</li>
</ul>
<p>The diagram below shows a basic strategy on how to train KNN for classification, given a data set with <span class="math inline">\(n\)</span> samples, <span class="math inline">\(p\)</span> variables and <span class="math inline">\(y\)</span> categorical outcome</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart TD
  A([data]) -. split data \n e.g. basic, stratified, grouped -.-&gt; B([non-test set])
  A([data]) -.-&gt; C([test set])
  B -.-&gt; D(choose algorithm \n e.g. KNN)
  D -.-&gt; E(choose evaluation metric \n e.g. overall accuracy)
  E -.-&gt; F(feature engineering &amp; selection)
  F -.-&gt; G(prepare parameter space, e.g. odd k-values from 3 to 30)
  G -. split non-test -.-&gt; H([train set &amp; validation set])
  H -.-&gt; J(fit model on train set)
  J -.-&gt; K(collect evaluation metric on validation set)
  K -.-&gt; L{all values checked? \n e.g. k more than 30}
  L -. No .-&gt; J
  L -. Yes .-&gt; M(select best parameter values)
  M -.-&gt; N(fit model on all non-test data)
  N -.-&gt; O(assess model on test data)
  C -.-&gt; O
  

</pre>
</div>
</div>
</div>
</div>
<p>&nbsp;</p>
<p>Before we see how this training may look like in R, let’s talk more about</p>
<ul>
<li><strong>KNN</strong>, K-nearest neighbor algorithm</li>
<li><strong>data splitting</strong> and</li>
<li><strong>performance metrics</strong> useful for evaluating models</li>
</ul>
<p>And we will leave feature engineering and feature selection for the next session.</p>
</section>
<section id="classification" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="classification"><span class="header-section-number">1.3</span> Classification</h2>
<ul>
<li>Classification methods are algorithms used to categorize (classify) objects based on their measurements.</li>
<li>They belong under <strong>supervised learning</strong> as we usually start off with <strong>labeled</strong> data, i.e.&nbsp;observations with measurements for which we know the label (class) of.</li>
<li>If we have a pair <span class="math inline">\(\{\mathbf{x_i}, g_i\}\)</span> for each observation <span class="math inline">\(i\)</span>, with <span class="math inline">\(g_i \in \{1, \dots, G\}\)</span> being the class label, where <span class="math inline">\(G\)</span> is the number of different classes and <span class="math inline">\(\mathbf{x_i}\)</span> a set of exploratory variables, that can be continuous, categorical or a mix of both, then we want to find a <strong>classification rule</strong> <span class="math inline">\(f(.)\)</span> (model) such that <span class="math display">\[f(\mathbf{x_i})=g_i\]</span></li>
</ul>
</section>
<section id="knn-example" class="level2 page-columns page-full" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="knn-example"><span class="header-section-number">1.4</span> KNN example</h2>
<div class="cell page-columns page-full" data-layout-align="center">
<div class="cell-output-display page-columns page-full">
<div id="fig-knn" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="intro_files/figure-html/fig-knn-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;1.1: An example of k-nearest neighbors algorithm with k=3; A) in the top a new sample (blue) is closest to three red triangle samples based on its gene A and gene B measurements and thus is classified as a red (B); in the bottom (C), a new sample (blue) is closest to 2 black dots and 1 red triangle based on its gene A and B measurements and is thus classified by majority vote as a black dot (D).</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Algorithm</strong></p>
<ul>
<li>Decide on the value of <span class="math inline">\(k\)</span></li>
<li>Calculate the distance between the query-instance (observations for new sample) and all the training samples</li>
<li>Sort the distances and determine the nearest neighbors based on the <span class="math inline">\(k\)</span>-th minimum distance</li>
<li>Gather the categories of the nearest neighbors</li>
<li>Use majority voting of the categories of the nearest neighbors as the prediction value for the new sample</li>
</ul>
<p><em>Euclidean distance is a classic distance used with KNN; other distance measures are also used incl.&nbsp;weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.</em></p>
</section>
<section id="data-splitting" class="level2 page-columns page-full" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="data-splitting"><span class="header-section-number">1.5</span> Data splitting</h2>
<ul>
<li>Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.</li>
<li>As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to <strong>overfitting</strong>.</li>
<li>To deal with overconfident estimation of future performance we can implement various data splitting strategies.</li>
</ul>
<section id="train-validation-test-sets" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="train-validation-test-sets">train, validation &amp; test sets</h3>
<ul>
<li>Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively</li>
<li><strong>Training data</strong>: this is data used to fit (train) the classification or regression model, i.e.&nbsp;derive the classification rule</li>
<li><strong>Validation data</strong>: this is data used to select which parameters or types of model perform best, i.e.&nbsp;to validate the performance of model parameters</li>
<li><strong>Test data</strong>: this data is used to give an estimate of future prediction performance for the model and parameters chosen</li>
</ul>
<div class="cell page-columns page-full" data-layout-align="center">
<div class="cell-output-display page-columns page-full">
<div id="fig-data-split" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figures/data-split-02.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;1.2: Example of splitting data into train (50%), validation (25%) and test (25%) set</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="cross-validation" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="cross-validation">cross validation</h3>
<ul>
<li>It can happen that despite random splitting in train/validation/test dataset one of the subsets does not represent data. e.g.&nbsp;gets all the difficult observation to classify.</li>
<li>Or that we do not have enough data in each subset after performing the split.</li>
<li>In <strong>k-fold cross-validation</strong> we split data into <span class="math inline">\(k\)</span> roughly equal-sized parts.</li>
<li>We start by setting the validation data to be the first set of data and the training data to be all other sets.</li>
<li>We estimate the validation error rate / correct classification rate for the split.</li>
<li>We then repeat the process <span class="math inline">\(k-1\)</span> times, each time with a different part of the data set to be the validation data and the remainder being the training data.</li>
<li>We finish with <span class="math inline">\(k\)</span> different error or correct classification rates.</li>
<li>In this way, every data point has its class membership predicted once.</li>
<li>The final reported error rate is usually the average of <span class="math inline">\(k\)</span> error rates.</li>
</ul>
<div class="cell page-columns page-full" data-layout-align="center">
<div class="cell-output-display page-columns page-full">
<div id="fig-data-split-kfold" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figures/data-split-kfolds-02.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;1.3: Example of k-fold cross validation split (k = 3)</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="repeated-cross-validation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="repeated-cross-validation">repeated cross validation</h3>
<ul>
<li>In repeated cross-validation we are repeating the cross-validation many times, e.g.&nbsp;we can create 5 validation folds 3 times</li>
</ul>
</section>
<section id="leave-one-out-cross-validation" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="leave-one-out-cross-validation">Leave-one-out cross-validation</h3>
<ul>
<li>Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.</li>
</ul>
<div class="cell page-columns page-full" data-layout-align="center">
<div class="cell-output-display page-columns page-full">
<div id="fig-data-split-loocv" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figures/data-split-loocv-02.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;1.4: Example of LOOCV, leave-one-out cross validation</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="evaluating-classification" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="evaluating-classification"><span class="header-section-number">1.6</span> Evaluating classification</h2>
<ul>
<li>To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g.&nbsp;change the value of <span class="math inline">\(k\)</span> in KNN.</li>
<li>There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.</li>
<li>Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.</li>
</ul>
<p><strong>Correct (miss)classification rate</strong></p>
<ul>
<li>The simplest way to evaluate in which we count for all the <span class="math inline">\(n\)</span> predictions how many times we got the classification right. <span class="math display">\[Correct\; Classifcation \; Rate = \frac{\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}\]</span> where <span class="math inline">\(1[]\)</span> is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise</li>
</ul>
<p><strong>Missclassification Rate</strong></p>
<p>Missclassification Rate = 1 - Correct Classification Rate</p>
<p><strong>Confusion matrix</strong></p>
<p>Confusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes. For a binary classifier we have:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual Positive</strong></td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr class="even">
<td><strong>Actual Negative</strong></td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table>
<p>Based on the confusion matrix, we can derive common performance metrics of a binary classifier:</p>
<ul>
<li><p><strong>Accuracy</strong>: measures the proportion of correctly classified samples over the total number of samples. <span class="math display">\[ACC = \frac{TP+TN}{TP+TN+FP+FN}\]</span>.</p></li>
<li><p><strong>Sensitivity</strong>: measures the proportion of true positives over all actual positive samples, i.e.&nbsp;how well the classifier is able to detect positive samples. It is also known as <strong>true positive rate</strong> and <strong>recall</strong>. <span class="math display">\[TPR = \frac{TP}{TP + FN}\]</span></p></li>
<li><p><strong>Specificity</strong>: measures the proportion of true negatives over all actual negative samples, i.e.&nbsp;how well the classifier is able to avoid false negatives. It is also known as <strong>true negative rate</strong> and <strong>selectivity</strong>. <span class="math display">\[TNR = \frac{TN}{TN+FP}\]</span></p></li>
<li><p><strong>Precision</strong>: measures the proportion of true positives over all positive predictions made by the classifier, i.e.&nbsp;how well the classifier is able to avoid false positives. It is also known as <strong>positive predictive value</strong> <span class="math display">\[PPV = \frac{TP}{TP + FP}\]</span></p></li>
<li><p><strong>ROC AUC</strong>: the receiver operating characteristic (ROC) curve is a graphical representation of the trade off between sensitivity and specificity for different threshold values. The area under the ROC curve (AUC) is a performance metric that ranges from 0 to 1, with a higher value indicating better performance. AUC is a measure of how well the classifier is able to distinguish between positive and negative samples.</p></li>
</ul>
</section>
<section id="evaluating-regression" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="evaluating-regression"><span class="header-section-number">1.7</span> Evaluating regression</h2>
<p>The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include:</p>
<ul>
<li><strong>R-squared</strong>: As seen in the linear regression session. <span class="math display">\[
R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
\]</span></li>
<li><strong>Adjusted R-squared</strong>: seen before <span class="math display">\[
R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
\]</span></li>
<li><strong>Mean Squared Error (MSE)</strong>: average squared difference between the predicted values and the actual values. <span class="math display">\[MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2\]</span></li>
<li><strong>Root Mean Squared Error (RMSE)</strong>: square root of the MSE <span class="math display">\[RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}\]</span></li>
<li><strong>MAE</strong>: average absolute difference between the predicted values and the actual values <span class="math display">\[MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|\]</span></li>
<li><strong>Mean Absolute Percentage Error (MAPE)</strong>: average percentage difference between the predicted values and the actual values.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./KNN-demo.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Demo: KNN model for classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>