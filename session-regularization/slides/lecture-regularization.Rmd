---
title: "Feature selection & model evaluation"
author: Bengt Sennblad, NBIS
date: 8 October 2021
output: 
  xaringan::moon_reader:
    encoding: 'UTF-8'
    self_contained: false
    css: [default, metropolis, metropolis-fonts]
    lib_dir: 'libs'
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
---
```{css, echo=F}
.hljs-github .hljs {
    background: #E8E8E8;
}
```

```{r setup, message=FALSE, echo=FALSE, show=FALSE}
#setwd("workshop-mlbiostatistics/session-regularization")

# Reminder to self: 
# To stop the infinite moon reader add in type in console:
# servr::daemon_stop(1)
# To access console the following can be useful
# options(servr.daemon = TRUE)

library(knitr)
library(ggplot2)
opts_chunk$set(echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE, out.width='100%', fig.height=3) #, fig.width=6)#, knitr.kable.NA = '-')
knit_hooks$set(inline = function(x) {if(is.numeric(x)){
  prettyNum(round(x,3), big.mark=" ")}else{x}
})
```


# Learning outcomes


* Likelihood
    + Relation to OLS for linear model
* Overfitting
* Regularization
    + Model comparison 
        - AIC, BIC
    + Feature selection
        - LASSO, ridge regression, elastic net
* Cross validation

---
layout: true

# Likelihood

---

- Given a generative model for $X\rightarrow Y$, with parameters $\theta$, we can compute $$Pr[Y| X, \theta],$$ the probability that the model with parameters $\theta$ and given $X$ has generated $Y$... 

--

- .. but we are often more interested in the probability that the parameters, $\theta$, are correct given the observed data, $X,Y$, i.e., ideally we want $$Pr[\theta|X,Y].$$

???

- However, it is not obvious how to compute this probability
- This is where likelihood comes in.

---

### Likelihood intuition:

- If parameters $\theta_T$ is closer to the 'truth' than $\theta_W$, then we would expect $$Pr[Y| X, \theta = \theta_T] > Pr[Y| X, \theta = \theta_T]$$
--

- We should therefore select the $\theta$ that maximizes $Pr[Y| X, \theta]$ as the best estimate; this is called  

.center[
**Maximum Likelihood estimation (MLE) of $\theta$.**
]

--

- _on average_ correct for large sample sizes.

???

Since statistical model contain an element of randomness, the reasoning above might not always be correct for any single observation. However, if we sum over a large number of observations it will be true on average. Hence the need for datasets that are large enough.

---

### Formal Likelihood definition

- The likelihood of model parameters being true given observed data is $$L[\theta|Y,X] = k\times Pr[Y|X, \theta],$$ with $k$ being an arbitrary konstant (Edwards, 1972)

???

- **Proportional**
- The proportionality (indicated by '$\propto$') means there are some unknown constant factor, $k$,
- However, the factor $k$ is assumed to be constant over $\theta$s and over models. 

--

- $L[\theta|Y,X]$ is not a proper probability, hence the term *likelihood* is used.

???

- Sum/integral of likelihoods $\neq 1$
- possibly bring up Bayes formula $$Pr[\theta|Y,X] = \frac{Pr[Y|\theta,X]}{Pr[Y,X]}$$

--

- In practice, proportionality is (almost alwyays) ignored and we set

$$L[\theta|Y,X] = Pr[Y|X, \theta]$$
--

### Likelihood ratio 
$$\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]} = \frac{k Pr[Y|X, \theta_1]}{ k  Pr[Y|X, \theta_0]} =\frac{Pr[Y|X, \theta_1]}{ Pr[Y|X, \theta_0]}.$$

???

- When the likelihood of two $\theta$s (or models) are compared this is almost always done as a _likelihood ratio_, 
- The proportionality  factor, $k$, such that $L[\theta|Y,X] = k Pr[Y|X, \theta]$ csancels out
- Hence the factor $k$ is always ignored. 
- Likelihood ratios is the basis of most model comparison statistics, e.g., the Wald test, the Score test, regularization... 

---

### Maximum Likelihood estimation

- Select the estimate $\widehat\theta$ that gives the highest likelihood, i.e.,  
$$\widehat{\theta} = argmax_{\theta}L[\theta|X,Y] \qquad \Leftrightarrow \qquad  max_{\theta}L[\theta|X,Y] = L[\widehat\theta|X,Y].$$
???

- Select as the best estimate that which...
- $\argmax_{\theta}$ is the argument $\theta$ that maximizes...

--

- Often, it is practical to use the logarithm of the likelihood, the _logLikelihood_, $$\log L[\theta_1|Y,X].$$ Notice that

--
  - $\widehat{\theta} = argmax_\theta \log L[\theta|Y,X]  = argmax_\theta L[\theta|Y,X]$ 

--
  - $\widehat{\theta} = argmin_\theta -\log L[\theta|Y,X]  = argmax_\theta L[\theta|Y,X]$ 

???

- Also the negative logLikelihood is used
- minimization instead of maximization

--
  - $\log\left(\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]}\right) = \frac{\log L[\theta_1|Y,X]}{\log L[\theta_0|Y,X]} = \log L[\theta_1|Y,X] - \log L[\theta_0|Y,X].$

???

Likelihood and maximum likelihood estimation are central concepts in statistics. Many statistical tests and methods uses or is based on the concept of maximum likelihood.

(In the following, I will simplify notation and not differentiate between estimates and random variables, e.g., $\theta$ will be used also for $\widehat\theta$.)

  - A likelihood ratio corresponds to a logLikelihood difference,

---

## Likelihood | `Likelihood and OLS for linear models`


- Why have we used ordinary least squares (OLS), $\min_\theta RSS$ when estimating linear model parameters

???

- Why have we used ordinary least squares (OLS), $\min_\theta RSS$ when estimating linear model parameters $\beta$ rather than maximum likelihood estimation?

--

Consider a simple linear regression model, $$ y = \beta x + \epsilon, \epsilon\sim N(0,\sigma^2).$$ 

???

- Linear models is a special case with some nice properties when it comes to  likelihood. 

--

- The maximum likelihood estimates of both $\beta$ and $\sigma^2$ are functions of the  RSS, and $$\log L[\beta, \sigma^2|Y,X] \approx -\frac{N}{2} \log RSS$$

  - This means that $$\widehat{\beta}_{ML} = \widehat{\beta}_{OLS}$$

???

- It turns out that the maximum likelihood estimates of both $\beta$ and $\sigma^2$ are functions of the  RSS of the residuals, so that the likelihood can be very well approximated by

--

- _**NB!** This is a special case for linear models and are not generally true for other models. For example, logistic regression is typically fitted using a maximum likelihood estimation _

???

- In general, full-on likelihood computation and maximum likelihood estimation is relatively slow, so alternative and faster methods has been developed, e.g., OLS.

---
layout: false
layout: true

#  Overfitting

---

 $\quad$ 

???

We will now look at a general problem in statistical modeling that can be visualized quite well with Likelihoods. We will later look at some solutions to this problem.

---

### Task |  Example Data

```{r, echo=TRUE}
# To obtain exactly the same result as in the demo, set seed to 85
set.seed(85)
N=100 # number of samples
P=10 # number of variables
# Generate X from uniform distribution
X=matrix(round(runif(N*(P+1),min=0, max=2), 2), nrow=N, ncol=P)
# generate Y from a multivariate lm of 3 first X variables only
b0=3 #intercept
b=c(runif(3, min=0.5, max=1.0)) # effect sizes for 3 first X variables
# generate Y for all samples (=rows in X) 
Y = b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)
```

```{text include = FALSE}
- $X =m\times n$ matrix:  
    - $m =  100$ samples 
    - $n =10$ variables $(x_1,x_2,\ldots, x_{0})$ from $Unif(0,1)$
- $Y = m\times 1$ vector generated from linear model
    - $Y = \beta_0 + \beta_1 x_i + \beta_2 x_2 + \beta_3 x_3 + \epsilon$, with $\epsilon\sim N(0,\sigma^2=1)$
    - $\beta_0 = 3$ 
    - $\beta_1 = `r b[1]`$, $\beta_2 = `r b[2]`$, $\beta_3 = `r b[3]`$ generated from $Unif(0.5, 1.0)$
```

???
- Draw variables, x_{i,1},...,x_{i,P} for all N individuals, uniformly
- 1. runif generates N*P+1 values, round them off to 2 decimals
- 2. Put values into a matrix with N rows and P columns

--

#### Think about: What can simulated data be used for?
--

* Estimating probabilities and probability distributions of, e.g., data and summary statistics of data
* Oracle knowledge when evaluating performance of methods, e.g., Type I and II errors 

???

- NULL distribution

---

### Model comparison

Now consider the following two models for our example data

\begin{eqnarray}
y & \sim & \beta_0 + \beta_1 x_1 & (1) \\
y & \sim &  \beta_0 + \beta_1 x_1 + \beta_2 x_2 & (2)
\end{eqnarray}

What are the max Likelihood estimates of the two models? Let's plot them!

---

### Task | `plot two likelihoods`

```{r,echo=T, fig.height=4, echo=TRUE, eval=FALSE}
library(stats)
ll= vector()
for(i in seq(1,2)){
  Xi=X[,seq(1,i)] # use variables 1..i
  ll[i] <- logLik(lm(Y~Xi)) # logLik extract loglikelihood from lm 
}
# plot likelihoods for models with 1 and 2 variables
plot(ll, ylab="log L", xlab="model #", type = "b", xlim=c(1,P),
     ylim=c(floor(min(ll)),ceiling(max(ll)))) 

```

--

```{r,echo=T, dpi=600, fig.height=2, echo=FALSE}
library(stats)
ll= vector()
for(i in seq(1,P)){
  Xi=X[,seq(1,i)]
  ll[i] <- logLik(lm(Y~Xi))
}
par(mar = c(4, 4, 0.1, 0.1)) 
# plot likelihoods for models with 1 and 2 vaiables
plot(ll[1:2], ylab="log L", xlab="model #", type = "b", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
# xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar
```

???
... 2 variables are clearly better than 1 variable -- What if we add more variables?
---

### Task | `plot all likelihoods`

.pull-left[
* Use sequence model obtained by adding $X$ variables in order.


```{r,echo=T, dpi=600, fig.height=3, echo =FALSE}
# compute loglikelihood (ll) for all models including variables
# 1--i, for i <= P; store results in vector ll
ll= vector()
for(i in seq(1,P)){
  Xi=X[,seq(1,i)]
  ll[i] <- logLik(lm(Y~Xi))
}
par(mar = c(4, 4, 0.1, 0.1)) 
# plot ll for all models
plot(ll, ylab="log L", xlab="model #", type = "b", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 

```
{{content}}
]
--

#### Think about:

* How does the Likelihood behave as more variables are added? Why?
* Which is the ML model? Is this correct?
* How would we like it to behave?
* How can this be obtained?

--
.pull-right[
#### Some possible answers 
_Nested models_

* Model (1) is a special case of Model (2) with constraints that $\beta_2=0$
* Therefore Model (2) will always have equal or better ML than Model (1)
{{content}}
]

???

- ML increases the more variables we add
--

_Overfitting_
* Since the simulated data was generated from the 3 first variables...
  + ...thus, the subsequent variables increase ML by modeling noise in data
  
{{content}}  

???

- The ML model is the one with all variables

--
  
* Solution:  Regularization

---

layout: false
layout: true

# Regularization

---

### Regularized/Penalized Likelihood

- Intuition: find the simplest model that is "good enough"
- Add auxiliary criteria, so-called *regularization terms*, to likelihood
- Typically, the regularization term is a function of parameters $\beta$:

$$\log rL[\beta | X, Y]  = \log L[\beta | X, Y] - f(\beta).$$
???

- "Simplast" = few parameters/variables

--

#### Example | A very simple regularization model

- $f(\beta) = \#\beta = \#X$ (that is the number of $X$ variables).  
$$\log rL[{\beta} | X, Y]  = \log L[\beta | X, Y] - \#X,$$
???
- A very simple regularized likelihood model uses 
- Let's apply this to our problem 

--
```{r test1, fig.height=4, echo=TRUE, eval=FALSE}
rl= vector() 
for(i in seq(1,P)){
  xi=X[,seq(1,i), drop=FALSE]
  # Compute the regularized Likelihood
  rl[i] = logLik(lm(Y~xi)) - i
}
```  

---
.pull-left[
```{r test2, dpi=600, fig.height=3, echo=FALSE}
# compute loglikelihood (ll) for all models including 1-P variables
rl= vector() 
for(i in seq(1,P)){
  xi=X[,seq(1,i), drop=FALSE]
  fit = lm(Y~xi)
  # Compute the regularized Likelihood
  rl[i] = logLik(fit) - i
}
par(mar = c(4, 4, 0.1, 0.1)) 
p = plot(rl, xlim=c(1,P), ylab="log rL", xlab="model #", type = "b")
```  
{{content}}
]
???

Applying this rL to our example data, solves the overfitting problem.

--

### Mini-task: Think about:

* Which is the best model? Is this correct according to *oracle knowledge*?
* Can you see a drawback in our model testing approach above? If so, how can we solve that?

--
.pull-right[
#### Some possible answers 

* The best model, 3, is the correct one.
* What if the right variables had been among the last? 

{{content}}
]
--
* How solve this?
  - Best subset method; (test all possible models)
  - Lasso; see next section


???
* We see that the best model is the one with the 3 first X-variables (in line with our *oracle knowledge*) and that the likelihood of second best model (with the first 4 X-variables) is $\approx 40\%$ of the best likelihood.
* Now, In this case, the first 3 variables was among the right one, so the order we choose to include them was correct. However, in the general case, we do not know this. How solve this?
  - Best subset method; (test all possible models)
  - Lasso; see next section
---

###  LASSO (*Least absolute shrinkage and selection operator*) 

- Feature selection
- Include all variables and apply regularization on $\beta$, so that only important variables have a $\beta_i> 0$.


--

- **LASSO model**
  - linear regression model (or GLM) $Y \sim X{\beta}$
  - regularization $f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i|$
      - $\lambda$ defines strength of regularization
          - higher $\lambda \Rightarrow$  stricter limit on individual $\beta_i$ values. 

--

- **LASSO notation**
  - $min\left\{RSS\right\} + \lambda\sum_{\beta_i\in\beta} |\beta_i|$
  - $max \log L[\beta|X,Y] - \lambda\sum_{\beta_i\in\beta} |\beta_i|.$

--

- **Optimization algorithms**
  - _lars_ or _coordinate descent_

???

- Lasso is traditionally described as RSS with an auxiliary criterion/constraint: 

Other common notation for LASSO:

* You might often see the notation $$min_{{\beta}}\left\{RSS\right\} \textrm{ subject to } ||{\beta}||_1 <= t$$
  where $t$ is related to $\lambda$.
  

---

### Example | Lasso using the `glmnet` R-package

* preprocessing (actually done by `glmnet`):  
  - center and standardize variables to ensure equal weighting
      + standardization of $X$ to unit variance (`standardize=TRUE` = default)
      + the values of $Y$ are always standardized (?) for `family=gaussian` (LASSO)
          + and the coefficients are back-standardized before reported
* Linear regression (`family='gaussian'` = default)
* preform LASSO (`alpha=1` = default)

```{r, echo=T, eval=TRUE} 
library(glmnet)
# run lasso (alpha=1) for linear model (family=gaussian)
fit = glmnet(X,Y, family="gaussian", alpha=1, standardize=T)
```

???
- Y and X must be centered and standardized to ensure that all variables are given equal weight in the model selection.
- where $\sqrt{\frac{1}{n}\sum_i(x_i-\bar{x})^2}$ is the uncorrected sample standard deviation.
- $\alpha$ determines what type of regularization is used

---

#### Visualization


```{r, echo=T, fig.height=5, eval=FALSE} 
plot(fit, xvar="lambda",label=T)
```


```{r, echo=FALSE, fig.height=4} 
par(mfrow=c(1,1))
par(mar = c(4, 4, 0.1, 0.1)) 
p=plot(fit, xvar="lambda",label=T)
p
```


???
 
* A graphical way to view the result is to `plot` the paths of $\beta$ for increasing values of $\lambda$. 
* This plot shows how the $\beta_i$ for different variables $i$ changes with $\lambda$. 
* The plot is perhaps best read from right to left, going from higher and thereby stricter, $\lambda$ values to lower $\lambda$ values including more and more variables/non-zero $\beta_i$.

---

.pull-left[
```{r, echo=FALSE,fig.height=4} 
par(mfrow=c(1,1))
par(mar = c(4, 4, 0.1, 0.1)) 
plot(fit, xvar="lambda",label=T)
```

#### Mini-task | Think about
* In which order are variables included (i.e., when $\beta_i > 0$)?
* In which direction is the effect?
* Which lambda should we select?
  - Given our *oracle knowledge*, where would an appropriate $\lambda$ be?
  - Is this useful for the general case?
  - Can we use anything else?
]

--

.pull-right[
##### Some possible answers
{{content}}
]
--

* The order appears to be $(2,1,3,6,4,10,5,8,9,7)$
* $\beta_i > 0, i\in \{1,2,3,4,78\}$, while $\beta_i<0, i\in \{5,6,9,10\}$
* Given *oracle knowledge*, the correct $\lambda$ appears lie somewhere in the interval $[\approx \exp(-1).1, \approx\exp(-2.5)]$
* In the normal case, we do not have *oracle knowledge*.
{{content}}

--
* Next: **Cross validation**


???

So how shall we know how what $\lambda$ to use?

---

### Cross-validation

How to decide the optimal $\lambda$ to use. 

* $\lambda$ too *high*: risk of missing relevant variables
* $\lambda$ too *low*: risk of overfitting 

`glmnet` addresses this using $k$*-fold cross-validation* -- what is that?

---
.pull-left[
#### Validation by replication

* Estimate a model on your data
* Test the general validity of this model 
  - run the estimated model on other data
  - measure the error it does
      - e.g., using _mean squared error_ $(MSE = RSS/N)$
].pull-right[

{{content}}

]

???

- Classsical/Optimal way of validation
- The ultimate way of testing an estimated model (with parameters) would be to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, $MSE$ $(=RSS/N)$.
Naturally, we want to minimize $MSE$, i.e., the error of the model. In our LASSO application, this means that we want to select the $\lambda$ that minimizes the $MSE$

--
#### Cross-validation

* Split data into _training_ and _validation_ data
* train model on _training data_
* validate on _validatation_ data

{{content}}

???

- In cross validation, this approach is emulated by partitioning the data at hand into a *training* and  *validation* data set. The model parameters are estimated ('trained') on the the training data and the validated on the validation data. (Optionally, a *test* partition can be assigned in cross-validation on which the final, selected model is evaluated; this is not employed here).
- By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partitioning.

--
#### $k$-fold cross validation

* Split data, $D$,  into $k$ partitions $(1,...,k)$
* for $i \in [k]$:
  - train model on $D\setminus i$
  - validate on $i$
* Error distribution

??? 
- In $k$-fold cross validation, the original data is split into $k$ sub-datasets $\{D_1,D_2,\ldots, D_k\}$.
- For $i \in \{1,2,\ldots, k\}$, set $D_i$ as the validation data set and the union of the other datasets be the training data. Perform cross validation as above.

-This gives a distribution of $MSE$ from which we can estimate, e.g., mean and standard deviation.

- This distribution allows us to use more elaborate means to select $\lambda$. One common suggestion is to use the largest $\lambda$ whose $MSE$ is within 1 standard error from the minimum value (called `lambda.1se` in `glmnet`). The motivation argued for this choice is *parsimony*, in the sense that larger $\lambda$ will include fewer variables (hence it is parsimonious in terms of number of included variables). 

- Here we will limit ourselves to finding the minimum $\lambda$, called `lambda.min` in `glmnet`, but anyone is free to test if `lambda.1se` gives a different result.

---

#### Example | Determine optimal LASSO $\lambda$ using cross-validation

```{r, echo=T,fig.height=5, eval=FALSE}
library(glmnet)
# run lasso (alpha=1) for linear model (family=gaussian)
cvglm=cv.glmnet(X,Y, family="gaussian", alpha=1, 
                standardize=T, nfolds=100)
plot(cvglm)
# minlambda=cvglm$lambda.min 
```


???

* Use the function `cv.glmnet` to perform cross validation (same options as for `glmnet`), store it in a R variable, e.g., `cvglm`,
* `plot` the cross-validation results 
* Compare with the plot of estimated $\beta_i$ under different $\lambda$ (these can be accessed from the result as `cvglm$glmnet.fix`).
* Determine the optimal $\lambda$ (the one with minimal error, can be found in `cvglm$lambda.min`) 

--

```{r, echo=FALSE,fig.height=3}
library(glmnet)
par(mfrow=c(1,1))
# run lasso (alpha=1) for linear model (family=gaussian)
cvglm=cv.glmnet(X,Y, family="gaussian", alpha=1, standardize=T, nfolds=100)
par(mfrow=c(1,1))
par(mar = c(4, 4, 0.1, 0.1)) 
plot(cvglm)
minlambda=cvglm$lambda.min
print(minlambda)
```

---

.pull-left[
```{r,echo=FALSE, fig.height=3}
plot(cvglm)
plot(cvglm$glmnet.fit, xvar="lambda",label=T)
```
]

--

.pull-right[

##### Think about
* Which is the $\lambda$ selected by `cv.glmnet`?
* Does this make sense given our *oracle knowledge*?
{{content}}

]

???

- optimal \lambda giving the minimal MSE can be read from plot

--

##### Some possible answers</h4>
* Cross-validation-selected optimal $\lambda$ is `r minlambda` $(\log(\lambda) = `r log(minlambda)`)$
* Yes, this includes only the *oracle knowledge* correct variables $X_1, X_2, X_3$

???

- optimal \lambda -> include variables read from plot



---
#### Example| Final LASSO effect sizes

.pull-left[
```{r, echo =T, eval=FALSE}
coef(cvglm, s="lambda.min")
```

{{content}}
]
--
```{r, echo =FALSE}
library(dplyr)      # for nice table
library(kableExtra) #for nice table

coefglm=as.data.frame(as.matrix(coef(cvglm, s="lambda.min")))
coefglm=cbind(seq(0,10),c(b0, b, rep(0, 7)),coefglm)
names(coefglm)=c("beta","value (oracle)", paste0("estimate(lambda=",signif(minlambda,3),")"))
kable(coefglm, row.names=F) %>%   kable_styling( font_size = 14)
```
--
.pull-right[
##### Think about
* Does the effect sizes make sense -- if not can you think of why?

{{content}}
]
--
 
##### Some possible answers

* Well...yes!
  + $\beta_i$ is non-zero only for _oracle_-known variables $X_1, X_2, X_3$
  + they don't exactly equate our *oracle knowledge* parameter values -- they appear to be scaled.
  + but their relative order of amplitude is right.
* Perhaps the normalization affected scaling.


???
* Finally print a table with the $\beta$ coefficients (including the intercept, $\beta_0$) for the optimal model (i.e.,  at minimum $\lambda$); compare with oracle knowledge. (Hint: see `? coef.cv.glmnet`).
---
layout: false
layout: true
# More about regularization 

---

### General cost function

- Regularization can be generalized to apply not only to linear models, for example, GLMs

???

- (as in glmnet).
--

- Regularization can be formulated for a general (Machine Learning) loss function, $\mathcal{L}(Y|X, \beta).$
- The modified loss function (including the regularization) can be written $$\mathcal{L}(Y|\beta, X, \lambda) = \mathcal{L}(Y|\beta, X) + \lambda f(\beta).$$
    - This formulation allows application to various regresssion and classification problems.

--

- Examples of loss functions:
    - $RSS$ (least squares method)
    - mean square error (MSE) = $RSS/N$
    - cross entropy $\sum_i y_i \times Pr[y_i|\beta, x]$

???

- Here, for consistency, I use $\beta$ for the parameters of the model, but other notation is commonly also used, e.g., $w$ (for weights). In fact, $\beta$ is often associated with regression parameters; when more types of parameters are used, it is common to use, e.g., $\theta$ as a collective notation for all those parameters.
    

---

### Reasons for regularizing

--

- Avoid false positives dues to overfitting

???

- In inference, where you want to identify the causative variables that contribute to the outcome. overfitting may cause false positives, that is, variables that simply explains random noise in the training data.
  - Applying regularization reduces these kinds of false positives.

--

- Model selection 
  - compare competing models

???

- Compare competing models, how much better is the best model. (anive regularization example)

--

- Feature selection/sparsity
  - minimal model
  - easier interpretation

???
- In many applications, e.g., for clinical diagnosis, a small model with few variables explaining the bulk of the outcome is preferred over a detailed model that explains as much of the outcome as possible. In these applications, regularization can be used for selecting the most important features and ignoring the other variables.
  - More generally, sparsity may also lead to more easily interpreted models, e.g., for biological questions.
  - LASSO

--

- Generalization to other data 
  - prediction

???

- In Machine learning the goal is often prediction, i.e., using the estimated model to predict outcomes $Y'$ from a completely new set of predictors, $X'$. Overfitting will make the estimated model to specialized to prediction of the training outcomes including the noise it contains. 
  - Regularization can therefore be motivated as a technique to improve the generalizability of a learned model by reducing its overfitting to the training data..

---

### Cross validation as "regularization"

- Iterative ML approaches
  - Number of iterations balance between
      - suboptimal model (too few)
      - risk of overfitting (too many)
      
???

- Many Machine learning techniques uses iteration to improve the model estimates incrementally. 
- The number of iterations then becomes an important parameter 
    - enough to get a good model, 
    - but avoid overfitting. 
    
--

##### Cross-validation as regularization
- Partition the data into three sets, *training*, *validation*, and *test* data sets. 
- In each iteration, 
  - train/improve model using _training_ data
  - validate on _validation_ data. 
  - continue until the _validation_ loss score does not get better
- Evaluate final model against the _test_ data.

---

### Norms 

- Norms are a kind of summary statistic over vectors 
  - function that has a vector as input and a (non-negative) number as output. 
- The most common notation for norms is $$||\beta||_p,$$

  - where $p$ is the "degree" of the norm.
  - (NB! $\beta$ could be substituted for another vector)
  

???

- *Norms* is a concept that commonly pops up when regularization is discussed.
- Since this special notation is used for norms, they can at first look very incomprehensible, but we have already used a number of them!
- Here I will use $\beta $ as example but could be any vector or expression describing a vector

---

#### The $L_0$ "norm"

- the number of non-zero $\beta$ $$||\beta||_0 = \sum_i I(\beta_i=0),$$ where $I(expr)$ equals 1 if $expr$ is true and 0 otherwise. 
  - corresponds to $\#X$ in our initial simple naive regularization approach

--

- Uses in regularization:
  - The *Akaike Information Criterion*, $AIC$, $$AIC_m = 2||\beta||_0 - 2\log L[{\beta}|X,Y]$$
  -  *Bayesian Information Criterion*, $BIC$,  $$BIC_m = \log(n)||\beta||_0 - 2\log L[{\beta}|X,Y]$$

???

- In fact, regularization with the $L_0$-norm would be the desired approach in most cases. However, as we have seen, finding the optimal model is really hard; in fact this is an *NP-hard* problem. Hence, alternative approaches using other norms has been investigated. 

---

#### The $L_1$ norm

- the sum of the absolute values of $\beta_i$, $$||\beta||_1 = \sum_{\beta_i\in\beta} |\beta_i|.$$

--

- Uses in regularization:
  - LASSO, $$min\left\{RSS\right\} + \lambda ||\beta||_1.$$

???

- As we have seen, LASSO is an effective algorithm for regularization with the $L_1$-norm. It also does a decent job of mimicking the $L_0$-"norm", i.e., setting $\beta_i=0$ for certain variables $i$. However, it has been shown that it can occasionally produce non-unique solutions.

---


#### The $L_2$ norm

- There is also a $L_2$ norm, $$||\beta||_2 = \sqrt{\sum_{\beta_i\in{\beta}} \beta_i^2}.$$

???

- If you substitute $\beta$ for some other expressin, does this feeel familiar?

--

- You have already have been working with  $L_2$ norms
  - $RSS = ||Y-X\beta||_2^2$ is the square of the $L_2$ norm of the residual vector.
  - the sample standard deviation can be written using a $L_2$ norm, $$sd(x) = \frac{||x-\bar{x}||_2}{\sqrt{n-1}}.$$

--

- Uses in regularization:
 - For regularization, the $L_2$-norm is used in *Ridge Regression*.
    - `glmnet(..., alpha = 0,...)`
 
???

- Computationally, regularization with an $L_2$ norm is even more efficient than with an $L_1$ norm. It works well to prevent over-fitting, but because it is bad at shrinking $\beta_i$'s all the way to zero, it is not so good for feature selection.

---


#### Combination of norms

- *Elastic net* regularization uses a mixed model combination of the $L_1$ norm and the $L_2$ norm,$$RSS+\alpha\lambda_1 ||\beta||_1 + (1-\alpha) \lambda_2 ||\beta||_2.,$$ where $\alpha$ determines the weight of the different regularizations

--

- "intermediate" between LASSO and Ridge Regression

--

- The aim is to avoid some drawbacks of LASSO.


???

- Elastic net regularization is closely related to the Machine Learning approach (linear) *Support Vector Machines*, $SVN$s


---
#### Geometry of norms

.pull-left[
Norms can also have a geometry interpretation:
{{content}}
]
.pull-right[
![manhattan distance](../images/manhattan_distance.jpg)
- B is a 2-dimensional vector (4,3)
- A is the _origo_-vector (0,0)
]
--
- The $L_2$-norm is the *Euclidean distance* from origo. 
{{content}}
--

- The $L_1$-norm is the *Manhattan distance* from origo. 
{{content}}
--

- The $L_0$-"norm" is the *Hamming distance* from origo. the Hamming distance between two vectors is the number of places they disagree.


???
- Hamming dist would be 2 here
- In formal mathematics, the $L_0$ "norm" does not properly fulfill all criteria for a norm; hence it is often, as here, written within quotation marks.

---
layout: false


# That's it

- End of lecture...

--

- Lecture notes for the session is available from Schedule.
  - contains some extra reading

--

- Next is the Exercises...

---

# Exercises

### Data 

- Biological data from TEX Human Tissue Gene Expression Consortium 
- $X$: skeletal muscle gene expression subset (randomly sampled 1000 genes)  
- $Y$: Gender

### Methods

- Model testing with _AIC_ or _BIC_

- Feature selection with _LASSO_

##### Tip:

- Reuse code from lecture notes;)





