<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Clustering</title>
    <meta charset="utf-8" />
    <meta name="author" content="Eva Freyhult, Mun-Gwan Hong" />
    <meta name="date" content="2022-09-15" />
    <script src="clustering_files/header-attrs-2.14/header-attrs.js"></script>
    <link href="clustering_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="clustering_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Clustering
]
.author[
### Eva Freyhult, Mun-Gwan Hong
]
.institute[
### NBIS, SciLifeLab
]
.date[
### 2022-09-15
]

---

class: spaced



&lt;style type="text/css"&gt;
quote {
  display: block;
  font-family: sans-serif;
  text-indent: 10px;
  font-size: 13px;
  color: green;
  line-height: 1.2;
}
bold {
  font-style: bold;
  color: blue;
}
smaller {
  font-family: sans-serif;
  font-size: 15px;
}
&lt;/style&gt;

# Clustering

&lt;img src="clustering_files/figure-html/clusters-1.png" width="70%" style="display: block; margin: auto;" /&gt;

Clustering is about grouping objects together according to similarity. 
The objects are grouped into clusters so that objects within the same cluster are more similar to one another than to objects in other clusters.

---

# Examples

.pull-left[
  ![](images/Ahlqvist_novel_subgroups.png)
  
  &lt;font size="2"&gt;January 2021&lt;/font&gt;
  ![](images/Wagner_Pathophysiology-based subphenotyping.png)
]
.pull-right[
* Diabetes. Type 1 and 2. Any subtypes?
* Clustering
  * Ahlqvist _et al._: 
    
    &lt;quote&gt;
    "Model &lt;bold&gt; variables were selected &lt;/bold&gt; on the premise that patients develop diabetes when they can no longer increase their insulin secretion..."
    &lt;/quote&gt;
    &lt;quote&gt;
    "...TwoStep clustering, in which the first step estimates the optimal number of clusters on the basis of &lt;bold&gt; silhouette width &lt;/bold&gt; and the second step does &lt;bold&gt; hierarchical clustering &lt;/bold&gt;, was done in SPSS version 23 for two to 15 clusters using log-likelihood as a &lt;bold&gt;distance&lt;/bold&gt; measure and Schwarz’s Bayesian criterion for clustering. &lt;bold&gt;k-means clustering&lt;/bold&gt; was done with a k value of 4..."
    &lt;/quote&gt;
    
  * Wagner _et al._: 
  
    &lt;quote&gt;
    "We used &lt;bold&gt;partitioning on variables &lt;/bold&gt; derived from oral glucose tolerance tests, MRI-measured body fat distribution, liver fat content and genetic risk"
    &lt;/quote&gt;
    &lt;quote&gt;
    "...&lt;bold&gt;distances&lt;/bold&gt; were computed as Gower distances using standardized variables..."
    "To find the optimal cluster count, we evaluated the dendogram and &lt;bold&gt;silhouette-widths&lt;/bold&gt;. The clustering procedure was performed with the partitioning around medoids (pam) method in the R-package ‘cluster’, which is a more robust version of &lt;bold&gt;k-means clustering&lt;/bold&gt;."
    &lt;/quote&gt;
]
---

# Clustering

.pull-left[
Clustering for a set of `\(n\)` objects (or observations) is usually performed based on a selection of `\(p\)` variables (or measurements). 
A single observation `\(i\)` can thus be described by the vector `\({\mathbf x}_i = [x_{i1}, x_{i2}, \dots, x_{ip}]\)`.

Clustering is commonly used for **exploratory data analysis** and to identify **sub-structures** in a data set.

There are many types of clustering algorithms, here we will only discuss two of them, *K-means* and *hierarchical* clustering.
]
.pull-right[
![](clustering_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
]

---
layout: true

# K-means

---

The K-means algorithm aims to divide all objects into exactly `\(K\)` clusters. 
`\(K\)` has to be given to the algorithm. 
The K-means algorithm minimize the variance within clusters, by iteratively assigning each object to the cluster with the closest centroid.

The centroid of cluster `\(k\)` is the arithmetic mean of all `\(n_k\)` objects in the cluster.

`$${\mathbf m}_k = \frac{1}{n_k} \sum_{i=1}^{n_k} {\mathbf x}_{i}$$`



---

The algorithm can be performed as follows;

.pull-left[
1. Initialization. Select `\(k\)` initial centroids.

&lt;/br&gt;

&lt;smaller&gt;The initial centroids can be selected in several different ways. Two common methods are;&lt;/smaller&gt;

* &lt;smaller&gt;Select &lt;span style="font-style: italic;"&gt;k&lt;/span&gt; data points as initial centroids&lt;/smaller&gt;
* &lt;smaller&gt;Randomly assign each data point to one out of &lt;span style="font-style: italic;"&gt;k&lt;/span&gt; clusters and compute the centroids for these initial clusters.&lt;/smaller&gt;
]

.pull-right[
![](clustering_files/figure-html/km_step1-1.png)&lt;!-- --&gt;
]

---

The algorithm can be performed as follows;

.pull-left[
1. Initialization. Select `\(k\)` initial centroids.

2. Assign each object to the closest centroid (in terms of squared Euclidean distance).
The squared Euclidean distance between an object (a data point) and a cluster centroid `\(m_k\)` is `\(d_i = \sum_{j=1}^p (x_{ij} - m_{kj})^2\)`. 
By assigning each object to closest centroid the total within cluster sum of squares (WSS) is minimized.
`$$WSS = \sum_{k=1}^K\sum_{i \in C_k}\sum_{j=1}^p (x_{ij} - m_{kj})^2$$`
]

.pull-right[
![](clustering_files/figure-html/km_step2-1.png)&lt;!-- --&gt;
]


---

The algorithm can be performed as follows;

.pull-left[
1. Initialization. Select `\(k\)` initial centroids.
2. Assign each object to the closest centroid (in terms of squared Euclidean distance).
By assigning each object to closest centroid the total within cluster sum of squares (WSS) is minimized.
3. Update the centroids for each of the `\(k\)` clusters by computing the centroid for all the objects in each of the clusters.
]

.pull-right[
![](clustering_files/figure-html/km_step3-1.png)&lt;!-- --&gt;
]

---

The algorithm can be performed as follows;

.pull-left[
1. Initialization. Select `\(k\)` initial centroids.
2. Assign each object to the closest centroid (in terms of squared Euclidean distance).
By assigning each object to closest centroid the total within cluster sum of squares (WSS) is minimized.
3. Update the centroids for each of the `\(k\)` clusters by computing the centroid for all the objects in each of the clusters.
4. Repeat 2-3 until convergence
]



.pull-right[
![](clustering_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]

---

Repeat until convergence

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]

---

Repeat until convergence

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]

---

Repeat until convergence

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]


---

Repeat until convergence

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

---

Repeat until convergence

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

---

Repeat until convergence

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;
]

---

Convergence is reached.

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
]


---
layout:false
layout:true

# K-means


---
layout: true
# Choosing the number of clusters

---

K-means clustering requires that we specify the number of clusters, `\(k\)`.
How do we select such a `\(k\)`?

---
### Elbow method

&lt;!-- The Elbow method is based on the total within sum of squares, `\(WSS\)`, that the K-means algorithm tries to minimize. By running the algorithm with several different values of `\(k\)`, e.g. 1--10, we can plot WSS as a function of `\(k\)`. --&gt;

![](clustering_files/figure-html/elbow-1.png)&lt;!-- --&gt;

&lt;!-- WSS is constantly decreasing as `\(k\)` increases. The elbow methods suggests that The inflection (bend, elbow) on the curve indicate an optimal number of clusters. In this case, the elbow method suggests that the optimal number of clusters is `\(k=3\)`. --&gt;

---

### Gap statistic

The gap statistic measures the distance between the observed `\(WSS\)` and an expected `\(WSS\)` under a reference (null) model.

`\(G(k) = E[\ln(WSS_{unif}(k))] - \ln(WSS(k))\)`

--

Choose `\(k\)` as the smallest `\(k\)` such that `\(G(k) \geq G(k+1) - s_{k+1}\)`.

--

&lt;img src="clustering_files/figure-html/gap-1.png" width="70%" /&gt;

---

### Silhouette method

The silhouette value for a single object `\(i\)` is a value between -1 ans 1 that measure how similar the object is to other objects in the same cluster as compared to how similar it is to objects in other clusters.

The average silhouette over all objects is a measure of how good the clustering is, the higher the value the better is the clustering.

&lt;img src="clustering_files/figure-html/silhouette-1.png" width="70%" /&gt;

---

### Silhouette method

The silhouette value, `\(s(i)\)`, for an object `\(i\)` in cluster `\(C_a\)` is calculated as follows;

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
]


---

### Silhouette method

The silhouette value, `\(s(i)\)`, for an object `\(i\)` in cluster `\(C_a\)` is calculated as follows;

.pull-left[
1. Average distance between `\(i\)` and all other objects in &lt;span style="color: blue;"&gt;the same cluster `\(C_a\)`&lt;/span&gt;;
`$$a(i) = \frac{1}{|C_a|-1} \sum_{j \neq i, j \in C_a} d(i, j)$$`
2. Average distance between `\(i\)` and all objects in &lt;span style="color: red;"&gt;another cluster `\(C_b\)`, `\(d(i,C_b)\)`&lt;/span&gt; and define the minimum;
`$$b(i) = \min_{b \neq a} d(i, C_b)$$`
3. The Silhouette index is defined as;
`$$s(i) = \frac{b(i) - a(i)}{max(a(i), b(i))}$$`
]

.pull-right[
![](clustering_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]

A silhouette value close to 1 means that the object is very well clustered, a value close to 0 means that the object lies between two clusters. A negative silhouette value means that the object is incorrectly clustered.


---
layout: false
name: diss
layout: true

# Dissimilarity matrix

---

&lt;!-- All clustering algorithms need a measure of similarity or dissimilarity between objects. As a similarity can be transformed to a dissimilarity, we will here focus on dissimilaities. --&gt;

&lt;!-- Dissimilarities between all pairs of objects can be described in a dissimilarity matrix. Most algorithms are based on symmetric dissimilarities, i.e. when the dissimilarity between a and b is the same as between b and a. Also, most algorithm require non-negative dissimilarities. --&gt;

K-means uses the squared Euclidean distance as a dissimilarity measure, but there  of course other ways to measure the dissimilarity between two objects (data points).

&lt;!-- An objects can usually be described by a set of `\(p\)` measurements, for `\(n\)` objects we have the measurements `\(x_{ij},\)` where `\(i=1,\dots,n\)` and `\(j=1,\dots,p\)`. --&gt;

---

Common dissimilarity measures include;

*Euclidean distance*
`$$d_{euc} (x, y) = \sqrt{\sum_{j=1}^{p} (x_j - y_j)^2}$$`
--

*Squared Euclidean distance*
`$$d_{eucsq} (x, y) = \sum_{j=1}^{p} (x_j - y_j)^2$$`
---

*Manhattan distance*
`$$d_{man} (x, y) = \sqrt{\sum_{j=1}^{p} |x_j - y_j|}$$`
---

*Pearson correlation distance*

Pearson's correlation is a similarity measure

`$$r = \frac{\sum_{j=1}^p(x_j-\bar x)(y_i-\bar y)}{\sqrt{\sum_{j=1}^p(x_j-\bar x)^2\sum_{j=1}^p(y_j-\bar y)^2}}$$`

Using a transformation we can compute a Pearson's correlation distance

`$$d_{pear}(x,y) = \sqrt{1-r}$$`
---
layout: false
layout: true

# Hierarchical clustering

---

Hierarchical clustering does not require the number of clusters to be specified. 
Instead of creating a single set of clusters it creates a hierarchy of clusterings based on pairwise dissimilarities.

&lt;img src="clustering_files/figure-html/hclust0-1.png" width="80%" /&gt;
---

There are two strategies for hierarchical clustering *agglomerative* (bottom-up) and *divisive* (top-down).

---

## Agglomerative

&lt;img src="clustering_files/figure-html/agglomerative-1.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/agglomerative-2.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/agglomerative-3.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/agglomerative-4.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/agglomerative-5.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/agglomerative-6.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/agglomerative-7.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/agglomerative-8.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/agglomerative-9.png" width="20%" /&gt;

---

## Divisive

&lt;img src="clustering_files/figure-html/divisive-1.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-2.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-3.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-4.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-5.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-6.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-7.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-8.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-9.png" width="20%" /&gt;&lt;img src="clustering_files/figure-html/divisive-10.png" width="20%" /&gt;

---
layout: false
name: agglo
layout: true

# Agglomerative clustering

---

* Starts with all objects in separate clusters
* Merge based on smallest dissmiliarity
* Dissimilarity between clusters is defined using a **linkage method**

The dissimilarity between two clusters A and B with objects `\(a_1, \dots, a_{n_A}\)` and `\(b_1, \dots, b_{n_B}\)`, respectively, can be computed using one of several linkage methods.





---

### Single linkage

.pull-left[
Single linkage takes as a cluster dissimilarity the distance between the two closest objects in the two clusters.]
.pull-right[
`$$d_{sl}(A, B) = \min_{i,j} d(a_i, b_j)$$`
]

![](clustering_files/figure-html/single-1.png)&lt;!-- --&gt;


---
### Complete linkage

.pull-left[Complete linkage takes as a cluster dissimilarity the distance between the two objects furthest apart in the two clusters.]
.pull-right[
`$$d_{cl}(A, B) = \max_{i,j} d(a_i, b_j)$$`
]

![](clustering_files/figure-html/complete-1.png)&lt;!-- --&gt;
---

### Average linkage

.pull-left[Average linkage takes as a cluster dissimilarity the average distance between the objects in the two clusters.]
.pull-right[
`$$d_{al}(A, B) = \frac{1}{n_A n_B}\sum_i\sum_j d(a_i, b_j)$$`
]

![](clustering_files/figure-html/average-1.png)&lt;!-- --&gt;
---

### Ward's linkage 

Ward's linkage method minimize the within variance, by merging clusters with the minimum increase in within sum of squares.

`$$d_{wl}(A, B) = \sum_{i=1}^{n_A} (a_i - m_{A \cup B})^2 + \sum_{i=1}^{n_B} (b_i - m_{A \cup B})^2 - \sum_{i=1}^{n_A} (a_i - m_{A})^2 - \sum_{i=1}^{n_B} (b_i - m_{B})^2$$`
where `\(m_A, m_B, m_{A \cup B}\)` are the center of the clusters `\(A\)`, `\(B\)` and `\(A \cup B\)`, respectively. 

Note that Ward's linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function `hclust` either the Euclidean or squared Euclidean distance can be used in combination with the linkage `method='ward.D'` or `method='ward.D2`, respectively.

---
#### Ward's linkage
![](clustering_files/figure-html/ward-1.png)&lt;!-- --&gt;



---
layout: false

# Heatmap

&lt;img src="clustering_files/figure-html/heatmap-1.png" width="65%" style="display: block; margin: auto;" /&gt;
This heatmap was created using the `pheatmap` package in R.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "4:3",
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
