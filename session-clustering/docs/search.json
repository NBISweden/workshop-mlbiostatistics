[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clustering",
    "section": "",
    "text": "Preface\nAims\n\nto introduce cluster analysis with focus on partitioning and hierarchical methods\n\nLearning outcomes\n\nto be able to explain what clustering is\nto be able to run k-means, PAM and HCL cluster analysis\nto be able to evaluate cluster solutions\n\nDo you see a mistake or a typo? We would be grateful if you let us know via edu.ml-biostats@nbis.se\nThis repository contains teaching and learning materials prepared for and used during “Introduction to biostatistics and Machine Learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use the basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Clustering: art of finding groups",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "intro.html#introduction",
    "href": "intro.html#introduction",
    "title": "1  Clustering: art of finding groups",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\nClustering is about finding structure in the data by identifying natural groupings (clusters). In other words, clustering aims to group object according to similarity. As a results, a cluster is a collection of objects (samples, genes etc.) that are more “similar” to each other that they are to cases in other clusters.\nClustering for a set of \\(n\\) objects is usually performed based on a selection of \\(p\\) variables. We can for instance cluster \\(n\\) blood samples based on \\(p\\) gene expression measurements or we can cluster \\(n\\) genes based on the \\(p\\) samples blood profiles.\nClustering is unsupervised learning, where we do not use any data labels to perform grouping. It is commonly used in EDA to reveal structures in the data.\n\n\n\nCode\ninclude_graphics(\"images/fruits.png\")\n\n\n\n\n\nFigure 1.1: Grouping fruits (A) can be done in more than one natural way. For instance, we can group by fruit type (B) or by fruit color (C). Often, the usefulness of the grouping is in the eye of the beholder, something that should not be forgotten when promoting a particular grouping over other possibilities."
  },
  {
    "objectID": "intro.html#common-usage",
    "href": "intro.html#common-usage",
    "title": "1  Clustering: art of finding groups",
    "section": "1.2 Common usage",
    "text": "1.2 Common usage\n\nGrouping samples based on clinical data\nGrouping samples based on gene expression profiles\nGrouping cells by cell types, e.g. in scRNA-seq\nBiomarker studies\nDisease subtyping and stratification\nClustering metagenomics contigs\nSpecies classification and biodiversity studies"
  },
  {
    "objectID": "intro.html#partition-methods-vs.-hierarchical",
    "href": "intro.html#partition-methods-vs.-hierarchical",
    "title": "1  Clustering: art of finding groups",
    "section": "1.3 Partition methods vs. hierarchical",
    "text": "1.3 Partition methods vs. hierarchical\n\n\n\n\nflowchart LR\n  A([Type of splitting]) -.-&gt; B([Partition])\n  A([Type of splitting]) -.-&gt; C([Hierarchical])\n  C([Hierarchical]) -.-&gt; D([Agglomerative])\n  C([Hierarchical]) -.-&gt; E([Divisive])\n\n\n\n\n\n\n\n\nThe partitioning and hierarchical methods are two general classes of clustering methods.\nIn partitioning methods address the problem of dividing \\(n\\) objects, described by \\(p\\) variables, into a small number (\\(k\\)) of discrete classes.\nFor instance, considering only two genes A and B (\\(p=2\\)) we can group 70 blood samples (\\(n=70\\)) into three clusters (\\(k=3\\)).\n\n\n\nCode\n# simulate data\nset.seed(190)\ndf &lt;- tibble(class = rep(c(\"cl1\", \"cl2\", \"cl3\"), c(30, 15, 25))) |&gt;\n  mutate(\n    x = rnorm(n(), c(cl1 = 1, cl2 = 3, cl3 = 2)[class], sd = .35), \n    y = rnorm(n(), c(cl1 = 1, cl2 = 1, cl3 = 2)[class], sd = .35),\n  )\n\n# plot data\ntext_size &lt;- 12\nmytheme &lt;- theme(legend.title = element_blank(),\n                 legend.position = \"top\", \n                 legend.text = element_text(size = text_size),\n                 panel.grid.major = element_blank(),\n                 panel.grid.minor = element_blank(),\n                 axis.text = element_text(size = text_size),\n                 axis.title = element_text(size = text_size))\n\nmycols &lt;- brewer.pal(6, \"Set1\")\n\np1 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(size = 2) + \n  theme_bw() +\n  xlab(\"gene A\") + \n  ylab(\"gene B\") + \n  mytheme \n  \n\n# show clusters\np2 &lt;- ggplot(df, aes(x, y, color = class)) +\n  geom_point(size = 2) + \n  theme_bw() +\n  xlab(\"gene A\") + \n  ylab(\"gene B\") + \n  mytheme + \n  scale_color_brewer(palette = \"Set1\")\n\np1 + p2\n\n\n\n\n\nFigure 1.2: Illustration of partition data splitting. A) Mock-up data for 70 blood samples for which expression of two genes were measured (left). B) Following cluster analysis, samples are grouped into three clusters (right).\n\n\n\n\n\n\nDivisive hierarchical methods begin with all of the objects in one cluster which is broken down into sub-cluster until all objects are separated. The agglomerative approach works in the opposite direction as single-member clusters are increasingly fused (joint) until all objects are in one cluster.\n\n\n\nCode\ninclude_graphics(\"images/hclust-ann.png\")\n\n\n\n\n\nFigure 1.3: A dendrogram representation for hierarchical clustering of data objects."
  },
  {
    "objectID": "intro.html#distance-and-similarity-measures",
    "href": "intro.html#distance-and-similarity-measures",
    "title": "1  Clustering: art of finding groups",
    "section": "1.4 Distance and similarity measures",
    "text": "1.4 Distance and similarity measures\n\nAll clustering algorithms start by measuring the similarity between the objects to be clustered. Similar cases will be placed into the same cluster.\nIt is also possible to view similarity by its inverse, the distance between the objects, with distance declining as similarity increases.\nThere are large number of distance metrics by which distance can be measured. Some are restricted to particular data types.\nAll distance metrics share the property that distance declines with increasing similarity. However, they do not share a common distance between the same two cases, i.e. distance changes with the similarity measure, leading to potential alternative clustering results.\nThree general classes of distance measures are Euclidean metrics, Non-Euclidean metrics and Semi-metrics.\n\nEuclidean metrics\n\nThese metrics measure true straight-line distances in Euclidean space.\nIn a univariate example the Euclidean distance between two values is the arithmetic difference. In a bivariate case the minimum distance between two points is the hypotenuse of a triangle formed from the points (Pythagoras theorem). For three variables the hypotenuse extends through three-dimensional space. In n-dimensional space, the Euclidean distance between two points \\(A = (a_1, a_2, ..., a_n)\\) and \\(B = (b_1, b_2, ..., b_n)\\)\n\n\\[distance(A,B) = \\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2}\\]\n\nFor instance, given two objects, red circle at \\((5, 3)\\) and yellow cylinder at \\((3, 1)\\) the Euclidean distance is \\(d = \\sqrt{(3 - 5)^2 + (1 - 3)^2} \\approx 2.82\\)\n\n\n\nCode\ninclude_graphics(\"images/distances.png\")\n\n\n\n\n\nFigure 1.4: Ilustration of distance metrics for a bivariate case. A) Euclidean metric, a distance between two point is sthe hypotenuse of a triangle formed from the points B) Non-Euclidean Manhatan metric.\n\n\n\n\nNon-Euclidean metrics\n\nThese are distances that are not straight lines, but which obey four rules. Let \\(d_{ij}\\) be the distance between two objects, \\(i\\) and \\(j\\).\n\n\n\\(d_{ij}\\) must be 0 or positive (objects are identical, \\(d_{ij} = 0\\), or they are different, \\(d_{ij} &gt; 0\\))\n\\(d_{ij} = d_{ji}\\), distance from A to B is the same as from B to A\n\\(d_{jj} = 0\\), an object is identical to itself\n\\(d_{ik} \\le d_{ij} + d_{jk}\\), when considering three objects the distance between any two of them cannot exceed the sum of the distances between the other two pairs. In other words, the distances can be constructed a a triangule.\n\n\nThe Manhattan or City Block metric is an example of this type of distance metrics. \\[distance (A,B) = \\sum|a_i - b_i|\\]\nIn our red circle and yellow cylinder, we get: \\(d = |3 - 5| + |1 - 3| = 4\\)\n\nSemi metric\n\nSemi metric distances obey the first three aforementioned rules, but may not obey the last “triangle rule”.\nThe cosine measure is an example of semi metric distance class. It is often used to compare vectors in text mining, where documents are often represented as vectors of numbers that represent word/term frequencies.\n\n\n\nCode\ninclude_graphics(\"images/cosine.png\")\n\n\n\n\n\nFigure 1.5: Ilustration of cosine distance metric, used for comparing vectors and text queries.\n\n\n\n\n\n1.4.1 Distances for interval variables\n\nEuclidean distance\n\nAlready mentioned above, normal Pythagoras theorem extended to the appropriate number of dimensions.\nCan be used when measuring similarity between points in Euclidean space, e.g. working with spatial data such as geographic coordinates or points on a map.\nIt may not be the best choice for high-dimensional data due to “distance concentration” phenomena. In high-dimensional space, the pair-wise distances between different data points converge to the same value as the dimensionality increases, making it hard to distinguish points that are near and far away.\n\n\n\nSquared Euclidean distance\n\nThe sum of the squared differences between scores for two cases on all variables, i.e. the squared length of the hypotenuse. This measure magnifies distances between cases that are further apart.\n\n\n\nChebychev\n\nThe absolute maximum difference, on one variable, between two cases. \\[distance(A, B) = max |a_i - b_i|\\]\nThis measures examines distances across all of the variables, but only uses the maximum distance. This one dimensional difference need not be constant across cases. For example, age could be used for one pair and height for another.\nInteresting application of the distance is by Ghaderyan and Beyrami (2020) who tried many distance metric in analysis of gait rhythm fluctuations for automatic neurodegenerative disease detection. They achieved good results using Chebyshev and cosine distance to distinguish patients with Amyotrophic Lateral Sclerosis, Parkinson’s disease (PD) and and Huntington’s disease (HD).\n\n\n\nCity block or Manhattan distance\n\nA distance that follows a route along the non-hypotenuse sides of a triangle. The name refers to the grid-like layout of most American cities which makes it impossible to go directly between two points.\nThis metric is hence less affected by outliers than the Euclidean and squared Euclidean metrics. \\[distance (A,B) = \\sum|a_i - b_i|\\]\nIs preferred over Euclidean distance when there is a high dimensionality in the data.\nMay not be the best choice, if one wants to perform space rotation or wants a smooth and differentiable function (converge more easily).\n\n\n\nMahalanobis distance\n\nIt is a generalized version of a Euclidean distance which weights variables using the sample variance-covariance matrix. Because the covariance matrix is used this also means that correlations between variables are taken into account. \\[distance(A,B) = [(a_i - b_i)^t S^{-1}(a_i - b_i)]^{1/2}\\]\n\nwhere \\(S^{-1}\\) is the inverse covariance matrix$\n\n\nMinkowski\n\nMinkowski distance is ageneralization of both the Euclidean distance and the Manhattan distance in a normed vector space.\nIt can be thought of as a way of measuring the length of the path between two points when moving along axes aligned with the coordinate system.\nThe formula for the Minkowski distance of order \\(p\\) between two points \\(A = (a_1, a_2, ..., a_n)\\) and \\(B = (b_1, b_2, ..., b_n)\\) in an \\(n\\)-dimensional space is given by \\[distance(A,B) = \\left(\\sum_{i=1}^{n} |a_i - b_i|^p\\right)^{1/p}\\] where \\(p\\) determines the form of the distance: for \\(p=1\\), it becomes the Manhattan distance (sum of the absolute differences of their coordinates); for \\(p=2\\), it yields the Euclidean distance (the shortest direct line between two points); and for \\(p=\\infty\\), it approaches the Chebyshev distance (the maximum difference along any coordinate dimension).\nThe versatility of the Minkowski distance makes it applicable across various fields and data types, allowing adjustments for different spatial concepts and scales.\n\n\n\nCanberra\n\nThe Canberra is a weighted version of the Manhattan distance, as it calculates the absolute difference between two vectors and normalizes it by dividing it by the absolute sum of their values \\[distance(A, B) = \\sum_{i=1}^{n} \\frac{|a_i - b_i|}{|a_i| + |b_i|}\\]\nBlanco-Mallo et al. (2023) compared a series of distance metrics for machine learning purposes and showed that Canberra distance had the best overall performance and the highest tolerance to noise.\n\n\n\n\n1.4.2 Distances for binary data\n\nThere is large number of metrics available for binary data, and many have more than one name, with Dice and Jaccard measures used probably most often in biology. All of the distance measures for binary data use two or three values obtained from a simple two by two matrix of agreement.\n\n\n\nCode\nx &lt;- matrix(data = c(\"\", \"\", \"Case\", \"i\", \n                     \"\", \"\", \"+\", \"-\", \n                     \"Case j\", \"+\", \"a\", \"b\", \n                     \"\", \"-\", \"c\", \"d\"), ncol = 4, nrow = 4, byrow = TRUE)\n\nx %&gt;%\n  as_tibble() %&gt;% #print()\n  kbl(col.names = NULL, row.names = FALSE, align = c(\"c\", \"c\", \"c\", \"c\")) %&gt;%\n  kable_paper(full_width = F)\n\n\n\n\nTable 1.1: Binary metrics use two or more of the values obtained from a two by two matrix of agrreement.\n\n\n\n\nCase\ni\n\n\n\n\n+\n-\n\n\nCase j\n+\na\nb\n\n\n\n-\nc\nd\n\n\n\n\n\n\n\n\nwhere:\n\n\\(a\\)) is the number of cases which both share the attribute\n\\(d\\)) number of cases which neither have the attribute\n\\(b\\) and \\(c\\)) are the number of cases in which only one of the pair has the attribute.\nNote: \\(a + b + c + d = n\\), the sample size\n\n\nDice\n\nA similarity measure in which joint absences \\(d\\) are excluded from consideration, and matches \\(a\\) are weighted double.\nAlso known as Czekanowski or Sorensen measure \\[distance(A, B) = 2a / (2a + b + c)\\]\n\n\n\nCode\ninclude_graphics(\"images/dice.png\")\n\n\n\n\n\nFigure 1.6: Ilustration of dice distance metric, used for comparing binary data.\n\n\n\n\n\n\nJaccard\n\nA similarity measure in which joint absences \\(d\\) are excluded from consideration.\nEqual weight is given to matches and mismatches.\n\n\\[distance(A, B) = a / (a + b + c)\\]"
  },
  {
    "objectID": "intro.html#partition-methods-k-means-pam",
    "href": "intro.html#partition-methods-k-means-pam",
    "title": "1  Clustering: art of finding groups",
    "section": "1.5 Partition methods: k-means & PAM",
    "text": "1.5 Partition methods: k-means & PAM\n\n1.5.1 k-means\n\nThe k-means clustering aims to divide all \\(n\\) objects, described by \\(p\\) variables, into exactly \\(k\\) clusters of discrete classes.\nThe value of \\(k\\), i.e. number of clusters, has to be given to the algorithm.\nThe algorithm minimizes the variance within clusters, by iteratively assigning each object to the cluster with the closest centroid, where centroid of cluster \\(k\\)th is the arithmetic mean of all \\(n_k\\) objects in the cluster.\n\n\\[{\\mathbf m}_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} {\\mathbf x}_{i}\\]\n\nThe simplest version of k-means algorithm is by Lloyd and Forgy’s Lloyd (1982). In R, kmeans functions implements a modified more effective version of this algorithm by Hartigan and Wong (1979).\n\nThe Lloyd and Forgy’s algorithm\n\nInitialization. Select \\(k\\) initial centroids. The initial centroids can be selected in several different ways. Two common methods are\n\nSelect \\(k\\) data points as initial centroids.\nRandomly assign each data point to one out of \\(k\\) clusters and compute the centroids for these initial clusters.\n\nAssign each object to the closest centroid (in terms of squared Euclidean distance). The squared Euclidean distance between an object (a data point) and a cluster centroid \\(m_k\\) is \\(d_i = \\sum_{j=1}^p (x_{ij} - m_{kj})^2\\). By assigning each object to closest centroid the total within cluster sum of squares (WSS) is minimized. \\[WSS = \\sum_{k=1}^K\\sum_{i \\in C_k}\\sum_{j=1}^p (x_{ij} - m_{kj})^2\\]\nUpdate the centroids for each of the \\(k\\) clusters by computing the centroid for all the objects in each of the clusters.\nRepeat steps 2 and 3 until convergence.\n\n\n\n\n\n\nkmeans\n\n\nFigure 1.7: Interactive visualization of k-means algorithm can be found here.\n\n\nWhen to use and not to use k-means\n\nk-means is relatively fast and scalable, making it a good choice for large data sets. It works well with continuous, numeric data since it relies on Euclidean distances. Results are easy to interpret due to the simple assingment of data points to clusters. It works good if one knows the number of clusters or can get that the number of clusters for internal or external validation.\nOn the downside, k-means is sensitive to the initial placement of cluster centroids. It can sometimes converge to local minima, resulting in suboptimal cluster assignments. Outliers can also significantly impact the position of centroids and lead to poor cluster assignments.\n\n\n\n1.5.2 PAM, partition around medoids\n\nPAM is very similar to k-means, but instead of using centroids we use medoids to represent clusters.\nMedoid is centrally located objects within the cluster.\nThis makes PAM more roboust when compare to k-means, but with higher computational complexity.\n\nThe algorithm can be described in few steps:\n\nInitialization: randomly select \\(k\\) objects as the initial medoids.\nAssignment: assign each object to the nearest medoid, forming \\(k\\) clusters.\nUpdate: for each cluster, find the new medoid by selecting the object that minimizes the sum of distances to all other objects in the cluster.\nCheck for convergence: if none of the medoids have changed, the algorithm has converged. Otherwise, return to step 2 (Assignment) and repeat the process.\n\n\n\nCode\n# example of running k-means and PAM in R (k = 3)\n\n# simulate data\nset.seed(190)\ndf &lt;- tibble(class = rep(c(\"cl1\", \"cl2\", \"cl3\"), c(30, 15, 25))) |&gt;\n  mutate(\n    x = rnorm(n(), c(cl1 = 1, cl2 = 3, cl3 = 2)[class], sd = .35), \n    y = rnorm(n(), c(cl1 = 1, cl2 = 1, cl3 = 2)[class], sd = .35),\n  )\n\n# extract x and y coordinates, convert to matrix\nx &lt;- df %&gt;%\n  dplyr::select(x, y) %&gt;%\n  as.matrix() %&gt;%\n  round(2)\n  \n# run k-means and PAM\nres_kmeans &lt;- kmeans(x, centers = 3)\nres_pam &lt;- pam(x, k = 3)\n\n# preprae data frames for plotting\ndf_kmeans &lt;- df %&gt;%\n  mutate(cluster = res_kmeans$cluster) %&gt;%\n  mutate(cluster = as.factor(cluster))\n\ndf_pam &lt;- df %&gt;%\n  mutate(cluster = res_pam$clustering) %&gt;% \n  mutate(cluster = as.factor(cluster))\n\ndf_centroids &lt;- res_kmeans$centers %&gt;% \n  as_tibble() %&gt;% \n  mutate(cluster = 1:3) %&gt;% \n  mutate(cluster = as.factor(cluster))\n\ndf_medoids &lt;- res_pam$medoids %&gt;%\n  as_tibble() %&gt;% \n  mutate(cluster = 1:3) %&gt;% \n  mutate(cluster = as.factor(cluster))\n\n# points with k-means clusters and centroids\np1 &lt;- df_kmeans %&gt;%\n  ggplot(aes(x = x, y = y, color = cluster)) + \n  geom_point(size = 2) + \n  geom_point(data = df_centroids, color = \"black\", size = 5, pch = 10) + \n  theme_bw() + \n  mytheme + \n  xlab(\"gene A\") + \n  ylab(\"gene B\") + \n  scale_color_brewer(palette = \"Set1\")\n\n# points with pam clusters and medoids\np2 &lt;- df_pam %&gt;%\n  ggplot(aes(x = x, y = y, color = cluster)) + \n  geom_point(size = 2) + \n  geom_point(data = df_medoids, color = \"black\", size = 5, pch = 21) + \n  theme_bw() + \n  mytheme + \n  xlab(\"gene A\") + \n  ylab(\"gene B\") + \n  scale_color_brewer(palette = \"Set1\")\n\np1 + p2\n\n\n\n\n\nFigure 1.8: Ilustration of k-means clusters with centroids marked (left) and PAM clusters with medoids marked (right) for k = 3."
  },
  {
    "objectID": "intro.html#hcl-hierarchical-clustering",
    "href": "intro.html#hcl-hierarchical-clustering",
    "title": "1  Clustering: art of finding groups",
    "section": "1.6 HCL, hierarchical clustering",
    "text": "1.6 HCL, hierarchical clustering\n\nHierarchical clustering does not require the number of clusters to be specified. Instead of creating a single set of clusters it creates a hierarchy of clusterings based on pairwise dissimilarities.\nThese hierarchy of clusteres is often represented as dendrogram.\n\n\n\nCode\nset.seed(19)\ndf &lt;- tibble(x = rnorm(10), y = rnorm(10))\nh &lt;- hclust(dist(df))\n\ndend &lt;- as.dendrogram(h) %&gt;% set(\"branches_lwd\", 3)\nplot(dend)\n\n\n\n\n\nFigure 1.9: Hierarchical clustering creates a hierarchy of clustering based on pairwise dissimilarites, that are often represented as dendrograms.\n\n\n\n\n\nThere are two strategies for hierarchical clustering agglomerative (bottom-up) and divisive (top-down).\nThe agglomerative strategy starts at the bottom with all objects in separate clusters and at each level merge a pair of clusters.\nThe merged pair of clusters are those with the smallest dissimilarity.\n\n\n\nCode\n# illustrate HCL agglomerative\n\ndf_cl &lt;- map_dfr(\n  1:10,   # step-by-step tree cuts\n  \\(ii) df |&gt; \n    mutate(step = ii, \n           Cluster = as.character(cutree(h, 11 - ii))) |&gt; \n    group_by(Cluster) |&gt; \n    mutate(is_dup = n() &gt; 1) |&gt; \n    ungroup()\n)\n  \nggplot(df_cl, aes(x, y, color = Cluster)) +\n  geom_point(size = 2) +\n  ggalt::geom_encircle(data = filter(df_cl, is_dup)) +\n  facet_wrap(~step, nrow = 2) +\n  guides(color = \"none\") +\n  theme_bw() + \n  mytheme + \n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\nFigure 1.10: Ilustration of hierarchical clustering agglomerative (bottom-up) approach.\n\n\n\n\n\nThe divisive strategy starts at the top with all objects in a single cluster and at each level one cluster is split into two.\nThe split is chosen to produce the two groups with the largest possible dissimilarity.\n\n\n\nCode\n# illustrate HCL divisive\nd &lt;- cluster::diana(dist(df))\ndf_cl &lt;- list()\ndf_cl0 &lt;- pre &lt;- mutate(df, step = 10, Cluster = -c(1:10), is_dup = FALSE)\n\nfor(ii in 1:9) {\n  i_merge &lt;- d$merge[ii, ]  # output from divisive h-clustering\n  cl &lt;- pre$Cluster   # copy previous\n  cl[cl %in% i_merge] &lt;- ii\n  \n  nxt &lt;- mutate(df, step = 10 - ii, Cluster = cl)\n  df_cl &lt;- c(df_cl, list(pre &lt;- nxt))\n}\n  \ndf_cl &lt;- c(list(df_cl0), df_cl) |&gt; \n  map_dfr(\n    ~.x |&gt; \n      mutate(Cluster = as.integer(factor(Cluster))) |&gt;  # drop disappeared cluster number\n      # encircle\n      group_by(Cluster) |&gt; \n      mutate(is_dup = n() &gt; 1) |&gt; \n      ungroup()\n  ) |&gt; \n  mutate(Cluster = as.character(Cluster))\n\nggplot(df_cl, aes(x, y, color = Cluster)) +\n  geom_point(size = 2) +\n  ggalt::geom_encircle(data = filter(df_cl, is_dup)) +\n  facet_wrap(~step, nrow = 2) +\n  guides(color = \"none\") +\n  theme_bw() + \n  mytheme + \n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\nFigure 1.11: Ilustration of hierarchical clustering divsive (top-down) approach.\n\n\n\n\n\nWith \\(n\\) objects to cluster both strategies will produce a dendrogram representing the \\(n-1\\) levels in the hierarchy.\nEach level represent a specific clustering of the objects into disjoint clusters.\nThe heights of the branches in the dendrogram are proportional to the dissimilarity of the merged/split clusters.\nThe divisive approach is used less often due to the computational complexity. Here, we fill focus more on the commonly used agglomerative approach.\n\n\n1.6.1 Linkage & Agglomerative clustering\n\nAgglomerative clustering starts with all objects in separate clusters and at each level merge the pair of clusters with the smallest dissimilarity. The pairwise dissilimarities between objects can be computed according to the distance measures discussed above.\nWe need one more ingredient for hierarchical clustering, that is a method for computing dissimilarity between clusters, known as linkage method. There are several linkage methods, as illustrated below.\n\n\n\nCode\ninclude_graphics(\"images/linkage.png\")\n\n\n\n\n\nFigure 1.12: Ilustration of linkage methods to calcualte dissimilarities between clusters.\n\n\n\n\nMore formally, the dissimilarity between two clusters A and B with objects \\(a_1, \\dots, a_{n_A}\\) and \\(b_1, \\dots, b_{n_B}\\) are defined as follows.\nSingle linkage\n\nSingle linkage takes as a cluster dissimilarity the distance between the two closest objects in the two clusters. \\[d_{sl}(A, B) = \\min_{i,j} d(a_i, b_j)\\]\n\nComplete linkage\n\nComplete linkage takes as a cluster dissimilarity the distance between the two objects furthest apart in the two clusters. \\[d_{cl}(A, B) = \\max_{i,j} d(a_i, b_j)\\]\n\nAverage linkage\n\nAverage linkage takes as a cluster dissimilarity the average distance between the objects in the two clusters. \\[d_{al}(A, B) = \\frac{1}{n_A n_B}\\sum_i\\sum_j d(a_i, b_j)\\]\n\nWard’s linkage\n\nWard’s linkage method minimize the within variance, by merging clusters with the minimum increase in within sum of squares. \\[d_{wl}(A, B) = \\sum_{i=1}^{n_A} (a_i - m_{A \\cup B})^2 + \\sum_{i=1}^{n_B} (b_i - m_{A \\cup B})^2 - \\sum_{i=1}^{n_A} (a_i - m_{A})^2 - \\sum_{i=1}^{n_B} (b_i - m_{B})^2\\] where \\(m_A, m_B, m_{A \\cup B}\\) are the center of the clusters \\(A\\), \\(B\\) and \\(A \\cup B\\), respectively.\nNote that Ward’s linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function hclust either the Euclidean or squared Euclidean distance can be used in combination with the linkage method='ward.D' or method='ward.D2, respectively.\n\n\n\nCode\nset.seed(19)\ndf &lt;- tibble(x = rnorm(10), y = rnorm(10)) # simulate data\nd &lt;- dist(df) # calculate distance\n\nh_single &lt;- hclust(d, method = \"single\")\nh_average &lt;- hclust(d, method = \"average\")\nh_complete &lt;- hclust(d, method = \"complete\")\nh_ward &lt;- hclust(d, method = \"ward.D\")\n\ndend_single &lt;- as.dendrogram(h_single) %&gt;% set(\"branches_lwd\", 3)\ndend_average &lt;- as.dendrogram(h_average) %&gt;% set(\"branches_lwd\", 3)\ndend_complete &lt;- as.dendrogram(h_complete) %&gt;% set(\"branches_lwd\", 3)\ndend_ward &lt;- as.dendrogram(h_ward) %&gt;% set(\"branches_lwd\", 3)\n\npar(mfrow=c(2,2))\nplot(dend_single, main = \"single\")\nplot(dend_average, main = \"average\")\nplot(dend_complete, main = \"complete\")\nplot(dend_ward, main = \"Ward's\")\n\n\n\n\n\nFigure 1.13: HCL agglomerative clustering with different linkage methods used.\n\n\n\n\n\n\nCode\n# compare dendrograms side-by-side and highlight differences\ndend_diff(dend_single, dend_complete)\n\n\n\n\n\nFigure 1.14: Side-by-side comparison of dendrograms with single (left) and complete linkage (right).\n\n\n\n\n\n\nCode\n# compare dendrograms side-by-side and highlight differences\ndend_diff(dend_average, dend_ward)\n\n\n\n\n\nFigure 1.15: Side-by-side comparison of dendrograms with average (left) and Ward’s linkage (right).\n\n\n\n\n\n\n1.6.2 Getting clusters\nTo find concrete clusters based on HCL we can cut the tree either using height or using number of clusters.\n\n\nCode\npar(mfrow = c(1,2))\n\ndend_complete %&gt;% set(\"labels_cex\", 2) %&gt;% set(\"labels_col\", value = c(3,4), k=2) %&gt;% \n   plot(main = \"Two clusters\")\nabline(h = 2, lty = 2)\n\ndend_complete %&gt;% set(\"labels_cex\", 2) %&gt;% set(\"labels_col\", value = c(3,4,5), k=3) %&gt;% \n   plot(main = \"Three clusters\")\nabline(h = 1.6, lty = 2)\n\n\n\n\n\nFigure 1.16: Example of cutting dendrogram, complete linkage, using height into two and three clusters."
  },
  {
    "objectID": "intro.html#how-many-clusters",
    "href": "intro.html#how-many-clusters",
    "title": "1  Clustering: art of finding groups",
    "section": "1.7 How many clusters?",
    "text": "1.7 How many clusters?\n\n\n\n\n\n\nAs we have seen above clustering is subjective, and there is more than one way to think about groupings of objects. In addition, there are many clustering algorithms to choose from, each with its own set of parameters and assumptions. We need some ways to be able to say something about how good our clustering, or how many clusters we have.\nWe will focus on internal validation that evaluates clustering quality and estimates cluster count without external information. In external validation one compares clustering outcomes to known results, measuring label match to select the best algorithm for a dataset.\nInternal validation methods evaluate clustering often by measuring the compactness and separation of the clusters - objects within a cluster exhibit maximal similarity (compactness), while those in separate clusters maintain a high degree of distinction (separation).\nThere are many ways to measure this, and some common methods include elbow method based on the total sum of squares \\(WSS\\), Silhouette method or Gap statistic.\nNbClust package in R implements 30 indexes for determining the optimal number of clusters in a data set to offer the best clustering scheme.\n\n\n1.7.1 Elbow method\n\nThe Elbow method is based on the total within sum of squares, \\(WSS\\), that the k-means algorithm tries to minimize.\nBy running the algorithm with several different values of \\(k\\), e.g. from 1 to 10, we can plot \\(WSS\\) as a function of \\(k\\).\nThe inflection (bend, elbow) on the curve indicates an optimal number of clusters.\n\n\n\nCode\n# simulate data\nset.seed(190)\ndf &lt;- tibble(class = rep(c(\"cl1\", \"cl2\", \"cl3\"), c(30, 15, 25))) |&gt;\n  mutate(\n    x = rnorm(n(), c(cl1 = 1, cl2 = 3, cl3 = 2)[class], sd = .35), \n    y = rnorm(n(), c(cl1 = 1, cl2 = 1, cl3 = 2)[class], sd = .35),\n  )\n\nfactoextra::fviz_nbclust(df[, c(\"x\", \"y\")], FUNcluster = kmeans, method=\"wss\")\n\n\n\n\n\nFigure 1.17: WSS as a function of the number of clusters. The inflection for k = 3 indicates that this is the best number of clusters for the data set.\n\n\n\n\n\n\n1.7.2 Silhouette method\n\n\n\n\n\n\nThe silhouette value for a single object \\(i\\) is a value between -1 and 1 that measures how similar the object is to other objects in the same cluster as compared to how similar it is to objects in other clusters.\nThe average silhouette over all objects is a measure of how good the clustering is, the higher the value the better is the clustering.\nThe silhouette value, \\(s(i)\\), for an object \\(i\\) in cluster \\(C_a\\) is calculated as follows:\n\nAverage distance between \\(i\\) and all other objects in the same cluster \\(C_a\\) \\[a(i) = \\frac{1}{|C_a|-1} \\sum_{j \\neq i, j \\in C_a} d(i, j)\\]\nAverage distance between \\(i\\) and all objects in another cluster \\(C_b\\), \\(d(i,C_b)\\) and define the minimum; \\[b(i) = \\min_{b \\neq a} d(i, C_b)\\]\nThe Silhouette index is defined as; \\[s(i) = \\frac{b(i) - a(i)}{max(a(i), b(i))}\\]\n\nA silhouette value close to 1 means that the object is very well clustered, a value close to 0 means that the object lies between two clusters. A negative silhouette value means that the object is incorrectly clustered.\nThe average silhouette over all objects is a measure of how good the clustering is, the higher the value the better is the clustering.\n\n\n\nCode\nfactoextra::fviz_nbclust(df[, c(\"x\", \"y\")], FUNcluster = kmeans, method=\"silhouette\")\n\n\n\n\n\nFigure 1.18: Average silhouette width as a function of the number of clusters. The maxium value for k = 3 indicates that this is the best number of clusters for the data set.\n\n\n\n\n\n\n1.7.3 Gap\n\nThe gap statistic compares the total within intra-cluster variation for different values of \\(k\\) with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\n\n\n\nCode\nfactoextra::fviz_nbclust(df[, c(\"x\", \"y\")], FUNcluster = kmeans, method=\"gap_stat\")\n\n\n\n\n\nFigure 1.19: Gap statistics as a function of the number of clusters. The maxium value for k = 3 indicates that this is the best number of clusters for the data set.\n\n\n\n\n\n\n1.7.4 pvclust\n\nPvclust R package to assess the uncertainty in hierarchical cluster analysis (Suzuki and Shimodaira 2006)\nPvclust calculates probability values (p-values) for each cluster using bootstrap resampling techniques.\nTwo types of p-values are available: approximately unbiased (AU) p-value and bootstrap probability (BP) value. Multiscale bootstrap resampling is used for the calculation of AU p-value, which has superiority in bias over BP value calculated by the ordinary bootstrap resampling.\n\n\n\nCode\ninclude_graphics(\"images/pvclust.png\")\n\n\n\n\n\nFigure 1.20: Bootstrapping for hierarchical clustering with pvclust. Clusters with AU &gt; 95% indicated by by the rectangles are considered to be strongly supported by data."
  },
  {
    "objectID": "intro.html#two-way-clustering",
    "href": "intro.html#two-way-clustering",
    "title": "1  Clustering: art of finding groups",
    "section": "1.8 Two-way clustering",
    "text": "1.8 Two-way clustering\n\nWhen dealing with omics data we often perform two-way clustering, where there is simultaneous clustering, typically hierarchical, of the rows (e.g. gene expression) and columns (e.g. samples).\nThis is often a default option when showing data on the heatmaps.\nWe suggest checking out ComplexHeatmap package that provides rich functionalities for customizing, including splitting columns and rows by clustering solutions\n\n\n\nCode\nload(\"data/tissuesGeneExpression.rda\")\n\nset.seed(123)\nn &lt;- 80\nidx.keep &lt;- sample(1:ncol(e), n, replace = FALSE)\n\ndata_genes &lt;- e[, idx.keep]\ndata_samples &lt;- tab[idx.keep, ]\n\n# calculate variance\ngenes_var &lt;- apply(data_genes, 1, var)\no &lt;- order(genes_var, decreasing = T)\n\n# select top genes with highest variance\nn_ft &lt;- 40\nx &lt;- data_genes[o[1:n_ft], ]\n\n# define function to calculate z scores\nfunc_cal_z_score &lt;- function(x){\n  (x - mean(x)) / sd(x)\n}\n\ndata_zscore &lt;- t(apply(x, 1, func_cal_z_score))\ndata_heatmap &lt;- data_zscore\ncols_wong &lt;- c(\"#0072b2\", \"#e69f00\")\n\n# set color scale\ncol_fun = colorRamp2(c(-1, 0, 2), c(cols_wong[1], \"white\", cols_wong[2]))\n\n# annotate columns\n# annotations: columns\nha_col &lt;- HeatmapAnnotation(tissue = data_samples$Tissue,\n                            col = list(tissue = c(\"kidney\" = \"darkgreen\",\n                                               \"hippocampus\" = \"red\",\n                                               \"colon\" = \"blue\",\n                                               \"cerebellum\" = \"orange\", \n                                               \"liver\" = \"yellow\", \n                                               \"placenta\" = \"pink\", \n                                               \"endometrium\" = \"black\")))\n\nHeatmap(data_heatmap,\n        col = col_fun,\n        top_annotation = ha_col,\n        name = \"h1\",\n        show_row_names = FALSE,\n        show_column_names = FALSE,\n        show_row_dend = TRUE,\n        show_column_dend = TRUE, \n        column_dend_height = unit(3, \"cm\"), \n    row_dend_width = unit(3, \"cm\"))\n\n\n\n\n\nFigure 1.21: Example of a heatmap based on gene expression data collected for seven tissues. Two-way clustering of both genes and samples shows a good seperation between the tissues."
  },
  {
    "objectID": "intro.html#additional-comments-resources",
    "href": "intro.html#additional-comments-resources",
    "title": "1  Clustering: art of finding groups",
    "section": "1.9 Additional comments & resources",
    "text": "1.9 Additional comments & resources\n\nIn addition to partitioning and hierarchical clustering methods there is a wide range of other algorithms. Common ones include model-, density, and grid-based methods. Some of them are explained in this presentation. Here, you can also find more details about external validation of cluster analysis.\nAlthough historically it was not recommended to apply cluster analysis to mixed data types (e.g. both binary and continuous), some distance metrics have been developed, e.g. Gower’s general coefficient to calculate similarity between different data types. Lately, the principles of decision trees have been also used for computing yet anther metric for mixed data types, unsupervised extra trees dissimilarity, UET. For an interesting study comparing clustering methods for heterogeneous data, where more details about Grower’s and UET can be found, see Preud’homme et al. (2021).\nFor a more applied reading in health research, reviewing distance metrics and clustering algorithms see Gao et al. (2023).\nIn scRNA-seq analysis it is common to use clustering based on the graph data. In particular, more details about Leiden algorithms can be found here or in the (Traag, Waltman, and Eck 2019)."
  },
  {
    "objectID": "exercises.html#r-notebook-with-questions-and-answers-kmeans-pam-hcl-complexheatmaps",
    "href": "exercises.html#r-notebook-with-questions-and-answers-kmeans-pam-hcl-complexheatmaps",
    "title": "Exercises",
    "section": "R notebook with questions and answers: kmeans, PAM, hcl, complexHeatmaps",
    "text": "R notebook with questions and answers: kmeans, PAM, hcl, complexHeatmaps"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dendextend)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(cluster)\nlibrary(RColorBrewer)\nlibrary(ggalt)\nlibrary(factoextra)\nlibrary(NbClust)\nlibrary(pvclust)\nlibrary(ComplexHeatmap)\nlibrary(circlize)\n\n\nYou have found some older gene expression data, based on the microarray technology. They contains measurements for 22215 genes for 189 samples, across 7 tissue (kidney, hippocampus, cerebellum, colon, liver, endometrium and placenta).\nData can be loaded and preview:\n\nload(\"data/tissuesGeneExpression.rda\")\n\n# expression data\nprint(dim(e))\n\n[1] 22215   189\n\nprint(head(e[1:4, 1:5]))\n\n          GSM11805.CEL.gz GSM11814.CEL.gz GSM11823.CEL.gz GSM11830.CEL.gz\n1007_s_at       10.191267       10.509167       10.272027       10.252952\n1053_at          6.040463        6.696075        6.144663        6.575153\n117_at           7.447409        7.775354        7.696235        8.478135\n121_at          12.025042       12.007817       11.633279       11.075286\n          GSM12067.CEL.gz\n1007_s_at       10.157605\n1053_at          6.606701\n117_at           8.116336\n121_at          10.832528\n\n# corresponding sample information\nstr(tab)\n\n'data.frame':   189 obs. of  6 variables:\n $ filename     : chr  \"GSM11805.CEL.gz\" \"GSM11814.CEL.gz\" \"GSM11823.CEL.gz\" \"GSM11830.CEL.gz\" ...\n $ DB_ID        : chr  \"GSM11805\" \"GSM11814\" \"GSM11823\" \"GSM11830\" ...\n $ ExperimentID : chr  \"GSE781\" \"GSE781\" \"GSE781\" \"GSE781\" ...\n $ Tissue       : chr  \"kidney\" \"kidney\" \"kidney\" \"kidney\" ...\n $ SubType      : chr  \"normal\" \"cancer\" \"normal\" \"cancer\" ...\n $ ClinicalGroup: chr  NA NA NA NA ...\n\n\n\nExercise 1 (Partition methods) Use first two genes only and run k-means clustering.\n\nFind optimal number of \\(k\\) using \\(WWS\\).\nVisualize your cluster results on one a scatter plot.\nUse first 10 genes now with the same value of \\(k\\). Is the clustering better or worse now? How can you tell?\n\n\n\nExercise 2 (HCL) Select samples corresponding to four tissues of your choice. Run HCL and compare dendrograms:\n\nwith complete and ward linkage, distance measure Euclidean\nwith complete and ward linkage, distance measure Canberra\n\n\n\nExercise 3 (Pvclust) Try running pvclust on the samples you’ve chosen above. Which clusters are supported by bootstrapping?\n\n\nExercise 4 (Heatmap) Select top 100 genes based on variance (with highest variance). Make a heatmap using ComplexHeatmap package. Group columns (samples) by tissue and split rows (genes) using k-means (k = 7).\n\nDo you see any interesting patterns?\nHow would you go about extracting genes belonging to a specific cluster, if you were interested in running functional annotations on those?"
  },
  {
    "objectID": "exercises.html#answers-to-exercises",
    "href": "exercises.html#answers-to-exercises",
    "title": "Exercises",
    "section": "Answers to exercises",
    "text": "Answers to exercises\n\nSolution. Exercise 1\n\n\n# load data\nload(\"data/tissuesGeneExpression.rda\")\n\n# subset gene expression data, two first genes\nx2 &lt;- t(e[1:2, ]) \n\n# find optimal number of cluster using WSS\nfactoextra::fviz_nbclust(x2, FUNcluster = kmeans, method=\"sil\")\n\n\n\n# decide on the best cluster\nk_best &lt;- 3\n\n# run k-means with best k\nres_kmeans2 &lt;- kmeans(x2, centers = k_best)\n\n# prepare data for plotting, add cluster solutions\ndf &lt;- data.frame(x2, clusters = res_kmeans2$cluster) %&gt;%\n  mutate(clusters = as.factor(clusters))\n\n# plot data\ndf %&gt;%\n  ggplot(aes(x = X1007_s_at, y = X1053_at, color = clusters)) + \n  geom_point() + \n  theme_bw()\n\n\n\n# run for 1000 genes\nn &lt;- nrow(e)\nn &lt;- 1000\nx_n &lt;- t(e[1:n, ]) \n\n# run k-means with best k\nres_kmeans_n &lt;- kmeans(x_n, centers = k_best)\n\n# To compare clustering results, we can compare average Silhouette for k = 3. \n# one way to do so is to plot average Silhouette as a function of different k again\nfactoextra::fviz_nbclust(x_n, FUNcluster = kmeans, method=\"sil\")\n\n\n\n# from the plot we can see now that the best value for k is 7\n# Looking at the average Silhouette value at k = 3, it is just above 0.3 when using 1000 genes\n# whereas it was above 0.5 when using 2 genes. \n# This means that choosing k = 3 for 1000 genes would results in worse clustering solution than when choosing k = 3 for 2 genes. \n# We can also see that the best number of clusters for 1000 genes is now 7. \n# Since we have some prior knowledge here, i.e. we know that the samples originate from 7 tissues and gene expression has been shown tissue specific, most likely using only two first genes is not enough to cluster samples into relevant groups. \n\n\nSolution. Exercise 2\n\n\n# see how many samples of each tissue we have\nprint(summary(as.factor(tab$Tissue)))\n\n cerebellum       colon endometrium hippocampus      kidney       liver \n         38          34          15          31          39          26 \n   placenta \n          6 \n\n# let's pick the fours tissues with smallest amount of samples\n# so the dendrograms are smaller and easier to read\n\ndata_samples &lt;- tab %&gt;%\n  as_tibble() %&gt;% \n  filter(Tissue %in% c(\"placenta\", \"endometrium\"))\n\n# subset gene expression data\nx &lt;- t(e[, data_samples$filename]) \n\n# make dendrograms\ndend_euclidean_complete &lt;- as.dendrogram(hclust(dist(x, method = \"euclidean\"), method = \"complete\"))\ndend_euclidean_ward &lt;- as.dendrogram(hclust(dist(x, method = \"euclidean\"), method = \"ward.D\"))\ndend_canberra_complete &lt;- as.dendrogram(hclust(dist(x, method = \"canberra\"), method = \"complete\"))\ndend_canberra_ward &lt;- as.dendrogram(hclust(dist(x, method = \"canberra\"), method = \"ward.D\"))\n\n# plot all dendrograms\npar(mfrow=c(2,2))\nplot(dend_euclidean_complete, main = \"euclidean, complete\")\nplot(dend_euclidean_ward, main = \"euclidean, Ward's\")\nplot(dend_canberra_complete, main = \"canberra, complete\")\nplot(dend_canberra_ward, main = \"canberra, Ward's\")\n\n\n\n# compare 2 side-by-side\ndend_diff(dend_euclidean_complete, dend_euclidean_ward)\n\n\n\ndend_diff(dend_canberra_complete, dend_canberra_ward)"
  }
]