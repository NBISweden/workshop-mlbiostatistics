<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Eva Freyhult, Mun-Gwan Hong" />

<meta name="date" content="2022-09-15" />

<title>Clustering</title>

<script src="chapters_files/header-attrs-2.16/header-attrs.js"></script>
<link href="chapters_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="chapters_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>





</head>

<body>




<h1 class="title toc-ignore">Clustering</h1>
<h4 class="author">Eva Freyhult, Mun-Gwan Hong</h4>
<h4 class="date">2022-09-15</h4>



<div id="learning-outcomes" class="section level2 unnumbered">
<h2 class="unnumbered">Learning outcomes</h2>
<ul>
<li>Understand the concept clustering</li>
<li>Understand and be able to perform k-means clustering</li>
<li>Understand and be able to perform hierarchical clustering</li>
<li>Understand the difference between different linkage methods</li>
</ul>
</div>
<div id="clust-01intro" class="section level1">
<h1>Introduction to clustering</h1>
<p>Clustering is about grouping objects together according to
similarity. The objects are grouped into clusters so that objects within
the same cluster are more similar to one another than to objects in
other clusters.</p>
<p>Clustering for a set of <span class="math inline">\(n\)</span>
objects (or observations) is usually performed based on a selection of
<span class="math inline">\(p\)</span> variables (or measurements). The
variables are chosen on the basis of prior knowledge.</p>
<p><span class="citation">Ahlqvist et al. (2018)</span></p>
<blockquote>
<p>Model variables were selected on the premise that patients develop
diabetes when they can no longer increase their insulin secretion…</p>
</blockquote>
<p>A single observation <span class="math inline">\(i\)</span> can thus
be described by the vector <span class="math inline">\({\mathbf x}_i =
[x_{i1}, x_{i2}, \dots, x_{ip}]\)</span>.</p>
<p>With only <span class="math inline">\(p=2\)</span> measurements these
measurements can easily be plotted and we could illustrate clusters by
colors.</p>
<p><img src="chapters_files/figure-html/clusters-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Clustering is commonly used for <strong>exploratory data
analysis</strong> and to identify <strong>sub-structures</strong> in a
data set. There are many types of clustering algorithms, here we will
only discuss two of them, <em>K-means</em> and <em>hierarchical</em>
clustering.</p>
</div>
<div id="k-means" class="section level1">
<h1>K-means</h1>
<p>The K-means clustering aims to divide all objects into exactly <span
class="math inline">\(K\)</span> clusters. <span
class="math inline">\(K\)</span> has to be given to the algorithm. The
algorithm minimize the variance within clusters, by iteratively
assigning each object to the cluster with the closest centroid.</p>
<p>The centroid of cluster <span class="math inline">\(k\)</span> is the
arithmetic mean of all <span class="math inline">\(n_k\)</span> objects
in the cluster.</p>
<p><span class="math display">\[{\mathbf m}_k = \frac{1}{n_k}
\sum_{i=1}^{n_k} {\mathbf x}_{i}\]</span></p>
<p>The simplest algorithm is Lloyd and Forgy’s (<span
class="citation">Forgy (1965)</span>, <span class="citation">Lloyd
(1957)</span>). Other algorithms, e.g. Hartigan and Wong’s (<span
class="citation">Hartigan and Wong (1979)</span>), the default of the
function <code>kmeans</code>, are modified versions of this to get the
results efficiently. The Lloyd and Forgy’s algorithm is performed as
follows;</p>
<ol style="list-style-type: decimal">
<li><p>Initialization. Select <span class="math inline">\(k\)</span>
initial centroids. The initial centroids can be selected in several
different ways. Two common methods are</p>
<ul>
<li>Select <span class="math inline">\(k\)</span> data points as initial
centroids</li>
<li>Randomly assign each data point to one out of <span
class="math inline">\(k\)</span> clusters and compute the centroids for
these initial clusters.</li>
</ul></li>
<li><p>Assign each object to the closest centroid (in terms of squared
Euclidean distance). The squared Euclidean distance between an object (a
data point) and a cluster centroid <span
class="math inline">\(m_k\)</span> is <span class="math inline">\(d_i =
\sum_{j=1}^p (x_{ij} - m_{kj})^2\)</span>. By assigning each object to
closest centroid the total within cluster sum of squares (WSS) is
minimized. <span class="math display">\[WSS = \sum_{k=1}^K\sum_{i \in
C_k}\sum_{j=1}^p (x_{ij} - m_{kj})^2\]</span></p></li>
<li><p>Update the centroids for each of the <span
class="math inline">\(k\)</span> clusters by computing the centroid for
all the objects in each of the clusters.</p></li>
<li><p>Repeat 2-3 until convergence</p></li>
</ol>
<div id="choosing-the-number-of-clusters" class="section level2">
<h2>Choosing the number of clusters</h2>
<p>K-means clustering requires that we specify the number of clusters,
<span class="math inline">\(k\)</span>. How do we select such a <span
class="math inline">\(k\)</span>?</p>
<div id="elbow-method" class="section level3">
<h3>Elbow method</h3>
<p>The Elbow method is based on the total within sum of squares, <span
class="math inline">\(WSS\)</span>, that the K-means algorithm tries to
minimize. By running the algorithm with several different values of
<span class="math inline">\(k\)</span>, e.g. 1–10, we can plot WSS as a
function of <span class="math inline">\(k\)</span>.</p>
<p><img src="chapters_files/figure-html/elbow-1.png" width="80%" /></p>
<p>WSS is constantly decreasing as <span
class="math inline">\(k\)</span> increases. The elbow methods suggests
that the inflection (bend, elbow) on the curve indicate an optimal
number of clusters. In this case, the elbow method suggests that the
optimal number of clusters is <span
class="math inline">\(k=3\)</span>.</p>
</div>
<div id="silhouette-method" class="section level3">
<h3>Silhouette method</h3>
<p>The silhouette value for a single object <span
class="math inline">\(i\)</span> is a value between -1 ans 1 that
measure how similar the object is to other objects in the same cluster
as compared to how similar it is to objects in other clusters.</p>
<p>The average silhouette over all objects is a measure of how good the
clustering is, the higher the value the better is the clustering.</p>
<p><img src="chapters_files/figure-html/silhouette-1.png" width="80%" /></p>
<p>The silhouette value, <span class="math inline">\(s(i)\)</span>, for
an object <span class="math inline">\(i\)</span> in cluster <span
class="math inline">\(C_a\)</span> is calculated as follows;</p>
<ol style="list-style-type: decimal">
<li>Average distance between <span class="math inline">\(i\)</span> and
all other objects in the same cluster <span
class="math inline">\(C_a\)</span> <span class="math display">\[a(i) =
\frac{1}{|C_a|-1} \sum_{j \neq i, j \in C_a} d(i, j)\]</span></li>
<li>Average distance between <span class="math inline">\(i\)</span> and
all objects in another cluster <span class="math inline">\(C_b\)</span>,
<span class="math inline">\(d(i,C_b)\)</span> and define the minimum;
<span class="math display">\[b(i) = \min_{b \neq a} d(i,
C_b)\]</span></li>
<li>The Silhouette index is defined as; <span
class="math display">\[s(i) = \frac{b(i) - a(i)}{max(a(i),
b(i))}\]</span> A silhouette value close to 1 means that the object is
very well clustered, a value close to 0 means that the object lies
between two clusters. A negative silhouette value means that the object
is incorrectly clustered.</li>
</ol>
<p>The average silhouette over all objects is a measure of how good the
clustering is, the higher the value the better is the clustering.</p>
</div>
</div>
</div>
<div id="dissimilarity-matrix" class="section level1">
<h1>Dissimilarity matrix</h1>
<p>All clustering algorithms need a measure of dissimilarity between
objects, which is similar to distance but a more general form with less
restriction.</p>
<p>Dissimilarities between all pairs of objects can be described in a
dissimilarity matrix. Most algorithms are based on symmetric
dissimilarities, i.e. when the dissimilarity between a and b is the same
as between b and a. Also, most algorithm require non-negative
dissimilarities.</p>
<p>K-means uses the squared Euclidean distance as a dissimilarity
measure, but there of course other ways to measure the dissimilarity
between two objects (data points).</p>
<p>For <span class="math inline">\(p\)</span> measurements, common
dissimilarity measures include;</p>
<p><em>Euclidean distance</em> <span class="math display">\[d_{euc} (x,
y) = \sqrt{\sum_{j=1}^{p} (x_j - y_j)^2}\]</span> <em>Squared Euclidean
distance</em> <span class="math display">\[d_{eucsq} (x, y) =
\sum_{j=1}^{p} (x_j - y_j)^2\]</span> <em>Manhattan distance</em> <span
class="math display">\[d_{man} (x, y) = \sqrt{\sum_{j=1}^{p} |x_j -
y_j|}\]</span> <em>Pearson correlation distance</em></p>
<p>Pearson’s correlation is a similarity measure</p>
<p><span class="math display">\[r = \frac{\sum_{j=1}^p(x_j-\bar
x)(y_i-\bar y)}{\sqrt{\sum_{j=1}^p(x_j-\bar x)^2\sum_{j=1}^p(y_j-\bar
y)^2}}\]</span></p>
<p>Using a transformation we can compute a Pearson’s correlation
distance</p>
<p><span class="math display">\[d_{pear}(x,y) = \sqrt{1-r}\]</span></p>
</div>
<div id="hierarchical-clustering" class="section level1">
<h1>Hierarchical clustering</h1>
<p>Hierarchical clustering does not require the number of clusters to be
specified. Instead of creating a single set of clusters it creates a
hierarchy of clusterings based on pairwise dissimilarities.</p>
<p><img src="chapters_files/figure-html/hclust0-1.png" width="80%" /></p>
<p>There are two strategies for hierarchical clustering
<em>agglomerative</em> (bottom-up) and <em>divisive</em> (top-down). The
agglomerative strategy starts at the bottom with all objects in separate
clusters and at each level merge a pair of clusters. The merged pair of
clusters are those with the smallest dissimilarity.</p>
<p><img src="chapters_files/figure-html/agglomerative-1.png" width="100%" /></p>
<p>The divisive strategy starts at the top with all objects in a single
cluster and at each level one cluster is split into two. The split is
chosen to produce the two groups with the largest possible
dissimilarity.</p>
<p><img src="chapters_files/figure-html/divisive-1.png" width="100%" /></p>
<p>With <span class="math inline">\(n\)</span> objects to cluster both
strategies will produce a dendrogram representing the <span
class="math inline">\(n-1\)</span> levels in the hierarchy. Each level
represent a specific clustering of the objects into disjoint clusters.
The heights of the branches in the dendrogram are proportional to the
dissimilarity of the merged/split clusters.</p>
<div id="agglomerative-clustering" class="section level2">
<h2>Agglomerative clustering</h2>
<p>Agglomerative clustering starts with all objects in separate clusters
and at each level merge the pair of clusters with the smallest
dissimilarity. The pairwise dissilimarities between objects are known,
but a method for computing dissimilarity between clusters is needed, as
so called <em>linkage method</em>.</p>
<p>The dissimilarity between two clusters A and B with objects <span
class="math inline">\(a_1, \dots, a_{n_A}\)</span> and <span
class="math inline">\(b_1, \dots, b_{n_B}\)</span>, respectively, can be
computed using one of several linkage methods.</p>
<div id="single-linkage" class="section level3">
<h3>Single linkage</h3>
<p>Single linkage takes as a cluster dissimilarity the distance between
the two closest objects in the two clusters. <span
class="math display">\[d_{sl}(A, B) = \min_{i,j} d(a_i,
b_j)\]</span></p>
<p><img src="chapters_files/figure-html/single-1.png" width="80%" /></p>
</div>
<div id="complete-linkage" class="section level3">
<h3>Complete linkage</h3>
<p>Complete linkage takes as a cluster dissimilarity the distance
between the two objects furthest apart in the two clusters. <span
class="math display">\[d_{cl}(A, B) = \max_{i,j} d(a_i,
b_j)\]</span></p>
<p><img src="chapters_files/figure-html/complete-1.png" width="80%" /></p>
</div>
<div id="average-linkage" class="section level3">
<h3>Average linkage</h3>
<p>Average linkage takes as a cluster dissimilarity the average distance
between the objects in the two clusters. <span
class="math display">\[d_{al}(A, B) = \frac{1}{n_A n_B}\sum_i\sum_j
d(a_i, b_j)\]</span></p>
<p><img src="chapters_files/figure-html/average-1.png" width="80%" /></p>
</div>
<div id="wards-linkage" class="section level3">
<h3>Ward’s linkage</h3>
<p>Ward’s linkage method minimize the within variance, by merging
clusters with the minimum increase in within sum of squares. <span
class="math display">\[d_{wl}(A, B) = \sum_{i=1}^{n_A} (a_i - m_{A \cup
B})^2 + \sum_{i=1}^{n_B} (b_i - m_{A \cup B})^2 - \sum_{i=1}^{n_A} (a_i
- m_{A})^2 - \sum_{i=1}^{n_B} (b_i - m_{B})^2\]</span> , where <span
class="math inline">\(m_A, m_B, m_{A \cup B}\)</span> are the center of
the clusters <span class="math inline">\(A\)</span>, <span
class="math inline">\(B\)</span> and <span class="math inline">\(A \cup
B\)</span>, respectively.</p>
<p>Note that Ward’s linkage method should not be combined with any
dissimilarity matrix as it is based on the squared Euclidean distance.
In the R function <code>hclust</code> either the Euclidean or squared
Euclidean distance can be used in combination with the linkage
<code>method='ward.D'</code> or <code>method='ward.D2</code>,
respectively.</p>
<p><img src="chapters_files/figure-html/ward-1.png" width="80%" /></p>
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Ahlqvist:2018td" class="csl-entry">
Ahlqvist, Emma, Petter Storm, Annemari Käräjämäki, Mats Martinell,
Mozhgan Dorkhan, Annelie Carlsson, Petter Vikman, et al. 2018.
<span>“Novel Subgroups of Adult-Onset Diabetes and Their Association
with Outcomes: A Data-Driven Cluster Analysis of Six Variables.”</span>
<em>The Lancet Diabetes &amp; Endocrinology</em> 6 (5): 361–69. <a
href="https://doi.org/10.1016/S2213-8587(18)30051-2">https://doi.org/10.1016/S2213-8587(18)30051-2</a>.
</div>
<div id="ref-forgy1965" class="csl-entry">
Forgy, Edward W. 1965. <span>“Cluster Analysis of Multivariate Data:
Efficiency Vs. Interpretability of Classification.”</span>
<em>Biometrics</em> 21 (3): 768–69.
</div>
<div id="ref-hartigan1979" class="csl-entry">
Hartigan, John A, and Manchek A Wong. 1979. <span>“Algorithm AS 136: A
k-Means Clustering Algorithm.”</span> <em>Journal of the Royal
Statistical Society. Series c (Applied Statistics)</em> 28 (1): 100–108.
</div>
<div id="ref-lloyd1957" class="csl-entry">
Lloyd, S. P. 1957. <span>“Least Square Quantization in PCM. Bell
Telephone Laboratories Paper. Published in Journal Much Later: Lloyd,
SP: Least Squares Quantization in PCM.”</span> <em>IEEE Trans. Inform.
Theor.(1957/1982)</em> 18: 5.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
