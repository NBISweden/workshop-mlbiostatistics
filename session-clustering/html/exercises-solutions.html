<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Eva Freyhult, Mun-Gwan Hong" />

<meta name="date" content="2022-09-15" />

<title>Exercises and solutions: Clustering</title>

<script src="exercises-solutions_files/header-attrs-2.16/header-attrs.js"></script>
<link href="exercises-solutions_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="exercises-solutions_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>










</head>

<body>




<h1 class="title toc-ignore">Exercises and solutions: Clustering</h1>
<h4 class="author">Eva Freyhult, Mun-Gwan Hong</h4>
<h4 class="date">2022-09-15</h4>



<div id="exercise-1" class="section level4">
<h4>Exercise 1</h4>
<p>Using Edgar Anderson’s iris data, we will investigate K-means
clustering. The data is stored in <code>iris</code> in R. Here we focus
on <code>Sepal.Width</code> and <code>Petal.Width</code>.</p>
<p><a
href="https://www.publicdomainpictures.net/en/view-image.php?image=342859&amp;picture=iris-flower"><img
src="images/iris-flower-1592163215Z1C.jpg" alt="Iris" /></a></p>
<p> </p>
<ol style="list-style-type: lower-alpha">
<li>Examine the contents and data types of the data set
<em>iris</em>.</li>
</ol>
<pre class="r"><code>str(iris)</code></pre>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<pre class="r"><code>summary(iris)</code></pre>
<pre><code>##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## </code></pre>
<pre class="r"><code>plot(Sepal.Width ~ Petal.Width, iris, col = Species)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-2-1.png" width="480" /></p>
<pre class="r"><code>df0 &lt;- iris[, c(&#39;Sepal.Width&#39;, &#39;Petal.Width&#39;)]</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li>Try to find three clusters in the data of sepal and petal widths
using a k-means algorithm. You can use the function <code>kmeans</code>
in R. It allows to choose an algorithm with the argument
<code>algorithm</code> (e.g. <code>algorithm = "Forgy"</code>).</li>
</ol>
<pre class="r"><code>## Number of clusters
n_clust &lt;- 3
## k-means with k = 3 and Forgy&#39;s algorithm
km &lt;- kmeans(df0, centers = n_clust, algorithm = &quot;Forgy&quot;)
## k-means with k = 3 and Hartigan and Wong&#39;s algorithm (as the default)
km &lt;- kmeans(df0, centers = n_clust)</code></pre>
<ol start="3" style="list-style-type: lower-alpha">
<li>Plot the data and color by cluster ID</li>
</ol>
<pre class="r"><code>df &lt;- df0
## The cluster identities are stored in km$cluster, same order as input 
df$Cluster &lt;- factor(km$cluster)
## Plot using base R graphics
plot(Sepal.Width ~ Petal.Width, df, col = Cluster)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>## alternatively, plot using ggplot
ggplot(df, aes(Petal.Width, Sepal.Width, color = Cluster)) + 
  geom_point()</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Repeat the same analysis a few times. Do you get the same clusters
all the time? Use <code>table</code> to check it allowing different
cluster numbers.</li>
</ol>
<pre class="r"><code>## the same analysis
km2 &lt;- kmeans(df0, centers = n_clust)
## Compare the results. Note cluster numbers can be changed.
table(km$cluster, km2$cluster)</code></pre>
<pre><code>##    
##      1  2  3
##   1  1 18  0
##   2  0 31  0
##   3 52  0 48</code></pre>
<pre class="r"><code>## Visualize the difference
if(sum(table(km$cluster, km2$cluster) &gt; 0) != n_clust) { # different
  df2 &lt;- df0
  df2$Cluster &lt;- factor(km2$cluster)
  df$model  &lt;- &quot;1st model&quot;
  df2$model &lt;- &quot;2nd model&quot;
  df3 &lt;- rbind(df, df2)
  ggplot(df3, aes(Petal.Width, Sepal.Width, color = Cluster)) + 
    geom_point() + 
    facet_grid(~ model)
}</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<ol start="5" style="list-style-type: lower-alpha">
<li>Try the argument <code>nstart</code> out and compare how stable two
runs are. By setting the argument to 4, the algorithm will automatically
try four different (random) starting points.</li>
</ol>
<pre class="r"><code>km3.1 &lt;- kmeans(df0, centers = n_clust, nstart = 4)
km3.2 &lt;- kmeans(df0, centers = n_clust, nstart = 4)
table(km3.1$cluster, km3.2$cluster)</code></pre>
<pre><code>##    
##      1  2  3
##   1  0 49  0
##   2  0  0 53
##   3 48  0  0</code></pre>
<ol start="6" style="list-style-type: lower-alpha">
<li>Try different values for <span class="math inline">\(k\)</span>, run
the k-means algorithm and collect the WSS (<code>tot.withinss</code>).
Plot WSS vs <span class="math inline">\(k\)</span>. The WSS is always
decreasing as <span class="math inline">\(k\)</span> increases, but the
curve can still give you a hint of which <span
class="math inline">\(k\)</span> to choose. The Elbow method for
selecting <span class="math inline">\(k\)</span> is to look at this
curve and choose the <span class="math inline">\(k\)</span> that you
find at the bend of the curve, at the ‘elbow’. Which <span
class="math inline">\(k\)</span> would you pick based on this?</li>
</ol>
<pre class="r"><code>wss &lt;- sapply(1:10, function(k) kmeans(df0, centers = k, nstart = 4)$tot.withinss)
plot(1:10, wss, type = &quot;b&quot;)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<ol start="7" style="list-style-type: lower-alpha">
<li>Compute the Silhouette width of the first object (or observation)
when <span class="math inline">\(k\)</span> is 2.</li>
</ol>
<pre class="r"><code>k &lt;- 2
cl &lt;- kmeans(df0, k)$cluster
## Euclidean distances
d &lt;- sapply(1:nrow(df0), function(ii) sqrt(sum((df0[1, ] - df0[ii, ])^2)))
## a_i = 1 / (|Ca| - 1) * sum(d(i, j))  in the same cluster Ca
ai &lt;- sum(d[cl == cl[1]]) / (sum(cl == cl[1]) - 1)
## other cluster IDs
other_cl &lt;- unique(cl[cl != cl[1]])
## b_i = min( mean(d(i, Cb)) )  average distance to another cluster Cb
bi &lt;- min(sapply(other_cl, function(ocl) mean(d[cl == ocl])))
si &lt;- (bi - ai) / max(ai, bi)
print(si)</code></pre>
<pre><code>## [1] 0.8001702</code></pre>
<ol start="8" style="list-style-type: lower-alpha">
<li>(Optional) Compute average Silhouette width of all objects adjusting
<span class="math inline">\(k\)</span> from 2 to 10 and choose optimal
<span class="math inline">\(k\)</span>. Euclidean distance can be
computed by <code>dist</code>. Check the help of the function using
<code>?dist</code>.</li>
</ol>
<pre class="r"><code>n &lt;- nrow(df0)
dm &lt;- as.matrix(dist(df0))   # matrix output for row wise analysis
sil &lt;- sapply(2:10, function(k) {
  cl &lt;- kmeans(df0, k)$cluster
  si &lt;- sapply(1:n, function(ii) {
    ## a_i = 1 / (|Ca| - 1) * sum(d(i, j))  in the same cluster Ca
    ai &lt;- sum(dm[ii, cl == cl[ii]]) / (sum(cl == cl[ii]) - 1)
    ## other cluster IDs
    other_cl &lt;- unique(cl[cl != cl[ii]]) 
    ## b_i = min( mean(d(i, Cb)) )  average distance to another cluster Cb
    bi &lt;- min(sapply(other_cl, function(ocl) mean(dm[ii, cl == ocl])))
    ## s_i
    (bi - ai) / max(ai, bi)
  })
  mean(si)
})
print(sil)</code></pre>
<pre><code>## [1] 0.6273425 0.5286392 0.4659839 0.4515611 0.4769964 0.3993389 0.4133418
## [8] 0.3924118       NaN</code></pre>
<pre class="r"><code>plot(2:10, sil)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<ol style="list-style-type: lower-roman">
<li>(Optional) Try <code>fviz_nbclust</code> of the R-package
<code>factoextra</code> and choose optimal <span
class="math inline">\(k\)</span> based on average Silhouette width.</li>
</ol>
<pre class="r"><code># install.packages(&quot;factoextra&quot;)  # if the package was not installed before
factoextra::fviz_nbclust(df0, kmeans, method=&quot;sil&quot;)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p> </p>
</div>
<div id="exercise-2" class="section level4">
<h4>Exercise 2</h4>
<p>The NCI60 data set consists of gene expression values for 6830 genes
for 64 cell lines. Using this data set, we investigate a few
hierarchical clustering distances and linkage methods. The data can be
downloaded in R using the following command</p>
<pre><code>nci_data &lt;- read.table(
  url(&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv&quot;),
  sep = &quot;,&quot;,
  row.names = 1,
  header = TRUE
)
nci_label &lt;- scan(
  url(&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label.txt&quot;),
  what = &quot;&quot;
)</code></pre>
<p> </p>
<ol style="list-style-type: lower-alpha">
<li>What is the size of the data matrix? Do every column represent a
gene or a cell line?</li>
</ol>
<pre class="r"><code>dim(nci_data)</code></pre>
<pre><code>## [1] 6830   64</code></pre>
<pre class="r"><code>nci_data[1:5, 1:5]</code></pre>
<pre><code>##        s1       s2     s3     s4     s5
## g1  0.300 0.679961  0.940  0.280  0.485
## g2  1.180 1.289961 -0.040 -0.310 -0.465
## g3  0.550 0.169961 -0.170  0.680  0.395
## g4  1.140 0.379961 -0.040 -0.810  0.905
## g5 -0.265 0.464961 -0.605  0.625  0.200</code></pre>
<pre class="r"><code>## ANS : Rows are genes, and columns are cell lines</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li>Compute the Euclidean distance <strong>between cell lines</strong>.
This can be accomplished using the function <code>dist</code>. Read the
help text <code>?dist</code>. This function computes the distance
between rows of the input data matrix. If the rows represent cell lines,
you can run <code>dist(nci_data)</code>. But, if your cell lines are
represented by columns, you need to transpose the data matrix first
<code>t(nci_data)</code>.</li>
</ol>
<pre class="r"><code>d &lt;- dist(t(nci_data))</code></pre>
<ol start="3" style="list-style-type: lower-alpha">
<li>Cluster the cell lines using complete linkage hierarchical
clustering, use the function <code>hclust</code>.</li>
</ol>
<pre class="r"><code>hc &lt;- hclust(d, method = &quot;complete&quot;)   # no need to specify the method</code></pre>
<ol start="4" style="list-style-type: lower-alpha">
<li>Plot the dendrogram of the clustering result (The help for
<code>hclust</code> includes the function for the plot)
<ul>
<li>Try changing the labels to something more informative using
<code>nci_label</code>.</li>
<li>Investigate the argument <code>hang</code>, what happens if you set
it to -1?</li>
</ul></li>
</ol>
<pre class="r"><code>plot(hc)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>plot(hc, labels = nci_label)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<pre class="r"><code>plot(hc, labels = nci_label, hang = -1)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-15-3.png" width="672" /></p>
<ol start="5" style="list-style-type: lower-alpha">
<li>Try the linkage methods “single”, “average” and “ward.D” in addition
to “complete”. Compare the results. Which method is ‘best’?</li>
</ol>
<pre class="r"><code>hc_s &lt;- hclust(d, method = &quot;single&quot;)
plot(hc_s, labels = nci_label, main = &quot;Single&quot;)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>hc_a &lt;- hclust(d, method = &quot;average&quot;)
plot(hc_a, labels = nci_label, main = &quot;Average&quot;)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-16-2.png" width="672" /></p>
<pre class="r"><code>hc_w &lt;- hclust(d, method = &quot;ward.D&quot;)
plot(hc_w, labels = nci_label, main = &quot;ward.D&quot;)</code></pre>
<p><img src="exercises-solutions_files/figure-html/unnamed-chunk-16-3.png" width="672" /></p>
<p>Pick the tree resulting from the method you think is ‘best’. How many
clusters are there?</p>
<p>You can cut the tree on any level to get between 1 and 64 clusters.
The function <code>cutree</code> either on a specific height
(dissimilarity) or to get a specific number of clusters.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
