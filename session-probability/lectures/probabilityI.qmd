---
editor: source
title: "Probability theory"
subtitle: "part I"
author: "Eva Freyhult"
date: "2023-04-24"
date-format: long
institute: NBIS, SciLifeLab
format: 
  revealjs:
    slide-number: true
    self-contained: true
    theme: [default]
---

```{r, include=FALSE}
require(tidyverse)
require(ggplot2)
require(reshape2)
require(knitr)
require(kableExtra)
library(ggthemes)
knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, cache=TRUE, error=FALSE, warnings=FALSE, dpi=600)
options(digits=2)
```

## Probability {auto-animate="true"}

Probability describes how likely an event, $E$, is to happen.

## Probability {auto-animate="true"}

Probability describes how likely an event, $E$, is to happen.

1.  $0 \leq P(E) \leq 1$

A probability is always between 0 and 1, where 1 means that the event
always happens, and 0 that it never happens.

## Probability {auto-animate="true"}

Probability describes how likely an event, $E$, is to happen.

1.  $0 \leq P(E) \leq 1$
2.  $P(S) = 1$

The total probability of all possible events is always 1.

The **sample space**, $S$, is the set of all possible events.

## Probability {auto-animate="true"}

Probability describes how likely an event, $E$, is to happen.

1.  $0 \leq P(E) \leq 1$
2.  $P(S) = 1$
3.  If $E$, $F$ are disjoint events, then $P(E \cup F) = P(E) + P(F)$

The probability of two disjoint (non overlapping) events, is the sum of
the probability of each event separately.

## Probability {auto-animate="true"}

Probability describes how likely an event, $E$, is to happen.

### Axioms of probability

1.  $0 \leq P(E) \leq 1$
2.  $P(S) = 1$
3.  If $E$, $F$ are disjoint events, then $P(E \cup F) = P(E) + P(F)$

::: notes
Show how sets can be used to express probabilies on white board.
:::

## Common rules of probability

Based on the axioms the following rules of probability can be proved.

:::{.incremental}
-   **Complement rule**: let $E'$ be the complement of $E$, then $P(E') = 1 - P(E)$
-   **Impossible event**: $P(\emptyset)=0$
-   **Probability of a subset**: If $E \subseteq F$ then $P(F) \geq P(E)$
-   **Addition rule**: $P(E \cup F) = P(E) + P(F) - P(E \cap F)$
:::

:::{.aside}
Let $E,F \subseteq S$ be any two events.
:::

## The urn model

::: {layout=[-15,10,10,10,-15] fig-align="center" height="50%"}
![A fair coin](figures/coinurn.png)

![Age](figures/ageurn.png){fig-align="center"}

![Pollen allergy](figures/pollenageurn.png){fig-align="center"}
:::

By drawing balls from the urn with (or without) replacement
probabilities and other properties of the model can be inferred.


## Random variables 

A **random variable** describes the outcome of a random experiment.

::: {.fragment .fade-in}
::: {.smaller style="font-size: 25px"}
-   The weight of a random newborn baby, $W$, $P(W>4.0kg)$
-   The smoking status of a random mother, $S$, $P(S=1)$
-   The hemoglobin concentration in blood, $Hb$, $P(Hb<125 g/L)$
-   The number of mutations in a gene, $M$
-   BMI of a random man, $B$
-   Weight status of a random man (underweight, normal weight,
    overweight, obese), $W$
-   The result of throwing a dice, $X$
:::

![](figures/dice.jpg){fig-align="center" width="30%"}
:::

::: notes
Whenever chance is involved
:::



## Random variables 

A **random variable** describes the outcome of a random experiment.

::: {.smaller style="font-size: 25px" .incremental}

* Random variables: $X, Y, Z, \dots$, in general denoted by a capital letter.

* Probability: $P(X=5)$, $P(Z>0.34)$, $P(W \geq 3.5 | S = 1)$

* **Observations** of the random variable, $x, y, z, \dots$

* The **sample space** is the collection of all possible observation
values.

* The **population** is the collection of all possible observations.

* A **sample** is a subset of the population.
:::

::: {.notes}
A random variable can not be predicted exactly, but the probability of
all possible outcomes can be described.

Whenever chance is involved in the outcome of an experiment the
outcome is a random variable.

Note, the population is not always countable.

the probability that $X \geq 3.5$ if $S = 1$

the probability that a smoking mother has a baby with birth weight of
3.5 kg or more"
:::

## Discrete random variables

<!-- ::: {.smaller style="font-size: 30px"} -->
A categorical random variable has nominal or ordinal outcomes such as; {red, blue, green} or {tiny, small, average, large, huge}.

:::{.fragment}
A discrete random number has countable number of outcome values, such as {1,2,3,4,5,6}; {0,2,4,6,8} or all integers.
:::

:::{.fragment}
A discrete or categorical random variable can be described by its **probability mass function (PMF)**.
:::

:::{.fragment}
The probability that the random variable, $X$, takes the value $x$ is
denoted $P(X=x) = p(x)$.
:::

<!-- ::: -->

## Example: a fair six-sided dice
::: {fig-align="center" layout=[-1,1,-1]}
![](figures/dice.jpg)
:::

Possible outcomes: $\{1, 2, 3, 4, 5, 6\}$

## Example: a fair six-sided dice

The probability mass function;

```{r}
kable(matrix(c(as.character(1:6), format(rep(1/6,6), digits=3)),ncol=6, byrow=TRUE, dimnames=list(c('x','p(x)'), c()))) %>% kable_styling(full_width = FALSE)
```

```{r dice, fig.height=3, fig.width=7, out.width="45%", fig.align='center'}
plot(data.frame(x=1:6, p=1/6) %>% ggplot(aes(x=x, y=p)) + geom_bar(stat="identity") + theme_bw() + ylim(c(0,.25)) + scale_x_continuous(breaks = 1:6))
```

## Example: Nucleotide at a given site

```{r}
#| label: tbl-nucl
#| tbl-cap: "Probability mass function of a nucleotide site."
p <- c(A=0.4, C=0.2, T=0.1, G=0.3)
kable(matrix(c(names(p), p), ncol=4, byrow=TRUE, dimnames=list(c('x','p(x)'), c()))) %>% kable_styling()
```

## Example: Number of bacterial colonies

```{r CFU, fig.height=3, fig.width=7}
x=1:50
ggplot(data.frame(x=x, fx=dpois(x, lambda=25)), aes(x,fx)) + geom_bar(stat="identity") + theme_bw() + ylab("p(x)")
```

## Expected value
:::{.smaller  style="font-size: 30px"}
The **expected value** is the average outcome of a random variable over many trials and is denoted $E[X]$ or $\mu$.

When the probability mass function is know, $E(X)$ can be computed as follow;

$$E[X] = \mu = \sum_{i=1}^n x_i p(x_i),$$ where $n$ is the number of outcomes.

Alternatively, $E(X)$ can be computed as the **population mean**, by summing over all $N$ objects in the population;

$$E[X] = \mu = \frac{1}{N}\sum_{i=1}^N x_i$$


:::

<!-- ## Expected value -->
<!-- **Linear transformations and combinations** -->
<!-- $$E(aX) = a E(X)$$ -->
<!-- $$E(X + Y) = E(X) + E(Y)$$ -->
<!-- $$E[aX + bY] = aE[X] + bE[Y]$$ -->

## Variance

The variance is a measure of spread and is defined as the expected value
of the squared distance from the population mean;

$$var(X) = \sigma^2 = E[(X-\mu)^2] = \sum_{i=1}^n (x_i-\mu)^2 p(x_i)$$

## Standard deviation

The **standard deviation** is the square root of the **variance** and is usually denoted $\sigma$

$$\sigma = \sqrt{E[(X-\mu)^2]} = \sqrt{\sum_{i=1}^n (x_i-\mu)^2 p(x_i)}$$
or by summing over all objects in the population; 

$$\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^N (x_i-\mu)^2}$$

The standard deviation is always positive and on the same scale as the outcome values.

<!-- ## Variance -->
<!-- **Linear transformations and combinations** -->
<!-- $$var(aX) = a^2 var(X)$$ -->
<!-- For independent random variables X and Y -->
<!-- $$var(aX + bY) = a^2var(X) + b^2var(Y)$$ -->
<!-- ::: {.notes} -->
<!-- X and Y are independent if $p(X|Y)=P(X)$ and/or $p(X \cap Y)=P(X)*P(Y)$ -->


## Simulate distributions

Once a random variables probability distribution is known, properties of interest can be computed, such as;

* probabilities, e.g. $P(X=a), P(X<a)$ and $P(X \geq a)$ 
* expected value, $E(X)
* variance, $\sigma^2$
* standard deviation, $\sigma$

If the distribution is not known, simulation might be the solution.

## Simulate distributions

When rolling a single dice the probabity of *six* is 1/6.

When rolling 20 dice, what is the probability of at least 15 sixes?

:::: {.columns}

::: {.column width="40%"}
![](figures/diceurn.png)
:::

::: {.column width="60%"}
The outcome of a single dice roll is a
random variable, $X$, that can be described using an urn model.

:::{.fragment}
*Simulation in R!*
:::
:::

::::


:::{.notes}
In R we can simulate random draws from an urn model using the
function `sample`.

```{r coin, echo=TRUE}
# A single coin toss
sample(c("H", "T"), size=1)
# Another coin toss
sample(c("H", "T"), size=1)
```

Every time you run the sample a new coin toss is simulated.

The argument `size` tells the function how many balls we want to draw
from the urn. To draw 20 balls from the urn, set `size=20,` remember to
replace the ball after each draw!

```{r coins, echo=TRUE}
# 20 independent coin tosses
(coins <- sample(c("H", "T"), size=20, replace=TRUE))
```

How many heads did we get in the 20 random draws?

```{r, echo=TRUE}
# How many heads?
sum(coins == "H")
```

We can repeat this experiment (toss 20 coins and count the number of
heads) several times to estimate the distribution of number of heads in
20 coin tosses.

To do the same thing several times we use the function `replicate.`

To simulate tossing 20 coins and counting the number of heads 10000
times, do the following;

```{r Nheads, echo=TRUE}
Nheads <- replicate(10000, {
  coins <- sample(c("H", "T"), size=20, replace=TRUE)
  sum(coins == "H")
})
```

Plot distribution of the number of heads in a histogram.

```{r histNheads, out.width="70%", echo=TRUE}
hist(Nheads, breaks=0:20)
```

Now, let's get back to the question; when tossing 20 coins, what is the
probability of at least 15 heads?

$P(X \geq 15)$

Count how many times out of our `r length(Nheads)` exeriments the number
is 15 or greater

```{r, echo=TRUE}
sum(Nheads >= 15)
```

From this we conclude that

$P(X \geq 15) =$ `r sum(Nheads>=15)`/`r length(Nheads)` =
`r sum(Nheads>=15)/length(Nheads)`
:::

## Parametric discrete distributions

* Uniform
* Bernoilli
* Binomial
* Poisson
* Negative binomial
* Geometric
* Hypergeometric


## Uniform

In a uniform distribution every possible outcome has the same probability.

With $n$ different outcomes, the probability for each outcome is $1/n$.

```{r dicerep, fig.height=3, fig.width=7, out.width="45%", fig.align='center'}
plot(data.frame(x=1:6, p=1/6) %>% ggplot(aes(x=x, y=p)) + geom_bar(stat="identity") + theme_bw() + ylim(c(0,.25)) + scale_x_continuous(breaks = 1:6))
```

## Bernoulli

::: {.smaller style="font-size: 35px"}
A Bernoulli trial is a random experiment with two outcomes; *success*
(1) and *failure* (0).

The outcome of a Bernoulli trial is a discrete random variable, $X$.

$$P(X=x) = p(x) = \left\{
\begin{array}{ll}
p & \mathrm{if}\,x=1\mathrm,\,success\\
1-p & \mathrm{if}\,x=0\mathrm,\,failure
\end{array}
\right.$$

Using the definitions of expected value and variance it can be shown
that;

$$E[X] = p\\
var(X) = p(1-p)$$
:::

## Binomial

The number of successes in a series of $n$ independent and identical
Bernoulli trials ($Z_i$, with probability $p$ for *success*) is a discrete random variable, $X$.

$$X = \sum_{i=0}^n Z_i,$$

## Binomial

::: {.smaller style="font-size: 35px"}
:::: {.columns}
::: {.column width="60%"}
The probability mass function of $X$, called the binomial distribution,
is

$$P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$$
:::
::: {.column width="40%"}
```{r}
#| fig-width: 5
x=1:50
ggplot(data.frame(x=x, fx=dbinom(x, size=70, prob=0.3)), aes(x,fx)) + geom_bar(stat="identity") + theme_bw() + ylab("p(x)")
```
:::
::::

The expected value and variance;

$$E[X] = np\\
var(X) = np(1-p)$$

In R: `pbinom` to compute $P(X \leq k)$ and `dbinom` to compute the **pmf** $P(X=k)$. 
:::

:::{.notes}
Examples fo binomial random variables;

-   The number of patients responding to a treatment out of $n$ patients in a study, if the probability of a patient responding to treatment is $p$.
-   The number of patients experiencing a side effect out of $n$ patients in a study, if the probability of a side effect is $p$.
-   the number of mutations in a gene of length $n$, if the mutations are independent and identically distributed and the probability of a mutation at every single position is $p$.
:::

## Hypergeometric distribution

::: {.smaller style="font-size: 35px"}
The hypergeometric distribution describe the number of successes in a
series of $n$ draws *without* replacement, from a population of size $N$
with $Np$ objects of interest (successes).

The probability density function

$$P(X=k) = \frac{\binom{Np}{k}\binom{N-Np}{n-k}}{\binom{N}{n}}$$
In R: `phyper` to compute $P(X \leq k)$ and `dhyper` to compute the **pmf** $P(X=k)$. 
:::

::: {.notes}
*Example:* Overrepresentation of iron metabolism genes in a list of
significant genes.

You have a population of $N$ genes, of which $Np$ belong to the pathway
*iron metabolism*. A statistiction gave you a list of $n$ interesting
genes of which $k$ genes belong to the *iron metabolism*. If the
statistician just selected the top list at random, the hypergeometric
distribution can be used to compute the probability of getting \$k4
successes, i.e. genes that belong to the *iron metabolism*.
:::


## Poisson distribution

The Poisson distribution describes the number of times a rare event (probability $p$)
occurs in a large number ($n$) of trials.

### Examples

- A rare disease has a very low probability for a single individual. The
number of individuals in a large population that catch the disease in a
certain time period can be modelled using the Poisson distribution.
- Number of reads aligned to a gene region.

## Poisson distribution

The probability mass function;

$$P(X=k) = \frac{\lambda}{k!}e^{-\lambda},$$

$$E[X] = var(X) = \lambda = n \pi$$

The Poisson distribution can approximate the binomial distribution if
$n$ is large and $\pi$ is small, $n>10, \pi < 0.1$.

In R: `ppois` to compute $P(X \leq k)$ and `dpois` to compute the **pmf** $P(X=k)$. 

## Negative binomial

A negative binomial distribution describes the number of failures that occur before a specified number of successes ($r$) has occurred, in a sequence of independent and identically distributed Bernoilli trials.

$r$ is also called the dispersion parameter.
 
In R: `dnbinm, pnbinom, qnbinom`

## Geometric

The geometric distribution is a special case of the negative binomial distribution, where $r=1$.

In R: `dgeom, pgeom, qgeom`

## Example PMFs

```{r}
#| label: fig-distr
#| fig-cap: "Probability mass functions for the binomial distribution (n=20, p=0.1, 0.3 or 0.5), hypergeometric distribution (N=100, n=20, p=0.1, 0.3 or 0.5), negative binomial distribution (n=20, r=n*p, p=0.1, 0.3 or 0.5) and Poisson distribution (n=20, p=0.1, 0.3 or 0.5)."
#| fig-width: 7
N=100
n=20
x <- 0:n
probs <- c(0.1, 0.3, 0.5)
df <- rbind(data.frame(prob=probs) |> group_by(prob) |> summarize(data.frame(x=x) |> mutate(px=dbinom(x, n, prob)), .groups="drop") |> mutate(distribution="Binomial"),
data.frame(prob=probs) |> group_by(prob) |> summarize(data.frame(x=x) |> mutate(px=dpois(x, n*prob)), .groups="drop") |> mutate(distribution="Poisson"),
data.frame(prob=probs) |> group_by(prob) |> summarize(data.frame(x=x) |> mutate(px=dnbinom(x, n*(1-prob), 1-prob)), .groups="drop") |> mutate(distribution="Negative binomial"),
data.frame(prob=probs) |> group_by(prob) |> summarize(data.frame(x=x) |> mutate(px=dhyper(x, round(N*prob), round(N*(1-prob)), n)), .groups="drop") |> mutate(distribution="Hypergeometric"))

df|>ggplot(aes(x=x, y=px, color=distribution, shape=factor(prob))) + geom_point(size=5) + geom_line() + theme_bw() + scale_color_colorblind() + ylab("p(x)") + scale_shape_discrete("")
```

## In R

Probability mass functions, $P(X=x)$; `dbinom`, `dhyper`, `dpois`, `dnbinom` and `dgeom`.

Cumulative distribution functions, $P(X \leq x)$;  `pbinom`, `phyper`, `ppois`, `pnbinom` and `pgeom`.

Also, functions for computing an $x$ such that $P(X \leq x) = q$, where $q$ is a probability of interest are available using; `qbinom`, `qhyper`, `qpois`, `qnbinom` and `qgeom`.
