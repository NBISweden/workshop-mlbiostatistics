[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Preface\nThis repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://uppsala.instructure.com/courses/93277."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Probability Theory",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nunderstand the concept of random variables\nunderstand the concept of probability\nunderstand and learn to use resampling to compute probabilities\nunderstand the concept probability mass function\nunderstand the concept probability density function\nunderstand the concept cumulative distribution functions\nuse normal distribution\nunderstand the central limit theorem\nto understand and define sampling distribution and standard error\nto compute standard error of mean and proportions"
  },
  {
    "objectID": "prob_01intro.html#axioms-of-probability",
    "href": "prob_01intro.html#axioms-of-probability",
    "title": "1  Introduction to probability",
    "section": "1.1 Axioms of probability",
    "text": "1.1 Axioms of probability\n\n\\(0 \\leq P(E) \\leq 1\\) for any event \\(E \\subseteq S\\)\n\\(P(S) = 1\\)\nIf \\(E\\), \\(F\\) are disjoint events, then \\(P(E \\cup F) = P(E) + P(F)\\)"
  },
  {
    "objectID": "prob_01intro.html#common-rules-of-probability",
    "href": "prob_01intro.html#common-rules-of-probability",
    "title": "1  Introduction to probability",
    "section": "1.2 Common rules of probability",
    "text": "1.2 Common rules of probability\nBased on the axioms the following rules of probability can be proved.\n\nComplement rule: let \\(E \\subseteq S\\) be any event, then \\(P(E') = 1 - P(E)\\)\nImpossible event: \\(P(\\emptyset)=0\\)\nProbability of a subset: let \\(E,F \\subseteq S\\) be events such that \\(E \\subseteq F\\) then \\(P(F) \\geq P(E)\\)\nAddition rule: let \\(E,F \\subseteq S\\) be any two events, then \\(P(E \\cup F) = P(E) + P(F) - P(E \\cap F)\\)"
  },
  {
    "objectID": "prob_01intro.html#conditional-probability",
    "href": "prob_01intro.html#conditional-probability",
    "title": "1  Introduction to probability",
    "section": "1.3 Conditional probability",
    "text": "1.3 Conditional probability\nLet \\(E,F \\subseteq S\\) be two events that \\(P(E)&gt;0\\) then the conditional probability of \\(F\\) given that \\(E\\) occurs is defined to be: \\[P(F|E) = \\frac{P(E\\cap F)}{P(E)}\\]\nProduct rule follows conditional probability: let \\(E,F \\subseteq S\\) be events such that \\(P(E)&gt;0\\) then: \\[P(E \\cap F) = P(F|E)P(E) = P(E|F)P(F)\\]"
  },
  {
    "objectID": "prob_01intro.html#the-urn-model",
    "href": "prob_01intro.html#the-urn-model",
    "title": "1  Introduction to probability",
    "section": "1.4 The urn model",
    "text": "1.4 The urn model\nThe urn model is a simple mathematical model commonly used in statistics and probability. In the urn model, objects (such as people, mice, cells, genes, molecules, etc) are represented by balls of different colors or labels. A fair coin can be represented by an urn with two balls representing the coin’s two sides; H and T. A group of people can be modeled in an urn model, if age is the variable of interest, we write the age of each person on the balls. If we instead are interested in if the people are allergic to pollen or not, we color the balls according to allergy status.\n\n\n\n\n\n\n\n(a) Fair coin\n\n\n\n\n\n\n\n(b) Age\n\n\n\n\n\n\n\n(c) Pollen allergy status\n\n\n\n\nFigure 1.1: Urn models of a fair coin, age of a group of people, and pollen allergy status of a group of people.\n\n\nIn the urn model every unit (ball) is equally likely of being selected. This means that the urn model is well suited to represent flipping a fair coin. However, a biased coin can also be modelled using an urn model, by changing the number of balls that represent each side of the coin.\nBy drawing balls from the urn with (or without) replacement, probabilities and other properties of the model can be inferred. For example, with an urn representinga population of people who are or are not allergic, the probability of randomly selecting three people who are all allergic can be determined."
  },
  {
    "objectID": "prob_01intro.html#random-variables",
    "href": "prob_01intro.html#random-variables",
    "title": "1  Introduction to probability",
    "section": "1.5 Random variables",
    "text": "1.5 Random variables\nThe outcome of a random experiment can be described by a random variable. Whenever chance is involved in the outcome of an experiment the outcome is a random variable.\nA random variable can not be predicted exactly, but the probability of all possible outcomes can be described. The sample space is the set of all possible outcomes of a random variable. Note, the sample space is not always countable.\nA random variable is usually denoted by a capital letter, \\(X, Y, Z, \\dots\\). Values collected in an experiment are observations of the random variable, usually denoted by lowercase letters \\(x, y, z, \\dots\\).\nThe sample space is the collection of all possible observation values.\nThe population is the collection of all possible observations.\nA sample is a subset of the population.\n\nExample 1.1 Paint a fair six sided dice black on one side and white on the other five sides. The outcome of throwing this dice is a random variable, \\(X\\). The possible outcomes, the sample space, is black and white. The population is all possible observations, i.e. the six sides.\n\nExample random variables and probabilites:\n\nThe weight of a random newborn baby, \\(W\\). \\(P(W&gt;4.0kg)\\)\nThe smoking status of a random mother, \\(S\\). \\(P(S=1)\\)\nThe hemoglobin concentration in blood, \\(Hb\\). \\(P(Hb&lt;125 g/L)\\)\nThe number of mutations in a gene\nBMI of a random man\nWeight status of a random man (underweight, normal weight, overweight, obese)\nThe result of throwing a\n\nConditional probability can be written for example \\(P(W \\geq 3.5 | S = 1)\\), which is the probability that \\(X \\geq 3.5\\) if \\(S = 1\\), in words “the probability that a smoking mother has a baby with birth weight of 3.5 kg or more”.\nRandom variables are divided based on their data type, categorical (nominal or ordinal) or numeric (discrete or continuous)."
  },
  {
    "objectID": "prob_02discrv.html#expected-value-and-variance",
    "href": "prob_02discrv.html#expected-value-and-variance",
    "title": "2  Discrete random variables",
    "section": "2.1 Expected value and variance",
    "text": "2.1 Expected value and variance\nTwo common properties of a probability distribution, such as a probability mass function, are the expected value and the variance.\n\nThe expected value is the average outcome of a random variable over many trials and is denoted \\(E[X]\\) or \\(\\mu\\) and can be computed by summing up all possible outcome values weighted by their probability;\n\\[E[X] = \\mu = \\sum_{i=1}^n x_i p(x_i),\\] where \\(n\\) is the number of outcomes.\nFor a uniform distribution, where every outcome has the same probability , the expected value can be computed as the sum of all outcome values divided by the total number of outcome values.\nThe expected value can also be computed as the population mean, i.e. by summing over outcome value for every object in the population (this is of course only possible if the population is countable);\n\\[E[X] = \\mu = \\frac{1}{N}\\sum_{i=1}^N x_i,\\] where \\(N\\) is the number of objects in the population.\nThe variance is a measure of spread defined as the expected value of the squared difference of the random variable and its expected value;\n\\[var(X) = \\sigma^2 = E[(X-E[X])^2] = \\sum_{i=1}^n (x_i-\\mu)^2 p(x_i).\\] The variance can also be computed by summing over the outcome value for every object in the population;\n\\[var(X) = \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^2,\\] where \\(N\\) is the number of objects in the population.\nThe standard deviation is the square root of the variance. The standard deviation of a random variable is usually denoted \\(\\sigma\\). The standard deviation is always positive and on the same scale as the outcome values.\n\n2.1.1 Linear transformations and combinations\n\\[E(aX) = a E(X)\\]\n\\[E(X + Y) = E(X) + E(Y)\\]\n\\[E[aX + bY] = aE[X] + bE[Y],\\] where \\(a\\) and \\(b\\) are constants.\n\\[var(aX) = a^2 var(X)\\]\nFor independent random variables X and Y\n\\[var(aX + bY) = a^2var(X) + b^2var(Y)\\]"
  },
  {
    "objectID": "prob_02discrv.html#simulate-distributions",
    "href": "prob_02discrv.html#simulate-distributions",
    "title": "2  Discrete random variables",
    "section": "2.2 Simulate distributions",
    "text": "2.2 Simulate distributions\nOnce a random variable’s probability distribution is known, probabilities, such as \\(P(X=x), P(X&lt;x)\\) and \\(P(X \\geq x)\\), and properties, such as expected value and variance, of the random variable can be computed. If the distribution is not known, simulation might be the solution.\n\nExample 2.4 (Simulate coin toss) In a single coin toss the probabity of heads is 0.5. In 20 coin tosses, what is the probability of at least 15 heads?\n\nThe outcome of a single coin toss is a random variable, \\(X\\), with two possible outcomes \\(\\{H, T\\}\\). We know that \\(P(X=H) = 0.5\\). The random variable of interest is the number of heads in 20 coin tosses, \\(Y\\). The probability that we need to compute is \\(P(Y \\geq 15)\\).\n\n\n\n\n\nFigure 2.4: A coin toss. Urn model with one black ball (heads) and one white ball (tails).\n\n\n\n\nA single coin toss can be modelled by an urn with two balls. When a ball is drawn randomly from the urn, the probability to get the black ball (heads) is \\(P(X=H) = 0.5\\).\nIn R we can simulate random draws from an urn model using the function sample.\n\n## A single coin toss\nsample(c(\"H\", \"T\"), size=1)\n\n[1] \"H\"\n\n## Another coin toss\nsample(c(\"H\", \"T\"), size=1)\n\n[1] \"T\"\n\n\nEvery time you run sample a new coin toss is simulated.\nIf we want to simulate tossing 20 coins (or one coin 20 times) we can use the same urn model, if the ball is replaced after each draw.\nThe argument size tells the function how many balls we want to draw from the urn. To draw 20 balls from the urn, set size=20, remember to replace the ball after each draw!\n\n## 20 independent coin tosses\n(coins &lt;- sample(c(\"H\", \"T\"), size=20, replace=TRUE))\n\n [1] \"T\" \"H\" \"T\" \"T\" \"T\" \"T\" \"T\" \"H\" \"H\" \"H\" \"T\" \"H\" \"H\" \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n[20] \"H\"\n\n\nHow many heads did we get in the 20 random draws?\n\n## How many heads?\nsum(coins == \"H\")\n\n[1] 10\n\n\nWe can repeat this experiment (toss 20 coins and count the number of heads) several times to estimate the distribution of number of heads in 20 coin tosses.\nTo do the same thing several times we use the function replicate.\nTo simulate tossing 20 coins and counting the number of heads 10000 times, do the following;\n\nNheads &lt;- replicate(10000, {\n  coins &lt;- sample(c(\"H\", \"T\"), size=20, replace=TRUE)\n  sum(coins == \"H\")\n})\n\nPlot the distribution of the number of heads in a histogram.\n\nhist(Nheads, breaks=0:20-0.5)\n\n\n\n\nNow, let’s get back to the question; when tossing 20 coins, what is the probability of at least 15 heads?\n\\(P(Y \\geq 15)\\)\nCount how many times out of our 10000 experiments the number is 15 or greater\n\nsum(Nheads &gt;= 15)\n\n[1] 245\n\n\nFrom this we conclude that\n\\(P(Y \\geq 15) =\\) 245/10000 = 0.0245\n\nResampling can also be used to compute other properties of a random variable, such as the expected value.\nThe law of large numbers states that if the same experiment is performed many times the average of the result will be close to the expected value.\nThe coin flip is a common example in statistics, but many situations with two outcomes can be modeled using similar models. If we consider the outcome of interest succes (like heads in the coin example) and the alternative outcome failure we can for example model;\n\nDrug effect: A patient can respond to drug treatment (success) or not (failure)\nSide effect: After a treatment a patient might experience a side effect (success) or not (failure)\nTreatment or placebo: Randomly assign a study participant into treatment or placebo group\nAntibiotic resistance: A bacteria is either resistant to an antibiotic or not\n\nThe probability of success and failure can be equal, like in the coin example, but they don’t have to be equal. If the probability of a patient responding to a treatment is \\(p=0.80\\), we know that the probability of the patient not responding is \\(1-p=0.20\\), this can be modelled using an urn model with 4 black balls (cured) and 1 white ball (not cured)."
  },
  {
    "objectID": "prob_02discrv.html#parametric-discrete-distributions",
    "href": "prob_02discrv.html#parametric-discrete-distributions",
    "title": "2  Discrete random variables",
    "section": "2.3 Parametric discrete distributions",
    "text": "2.3 Parametric discrete distributions\nA discrete parametric distribution is a probability distribution described by a set of parameters. In many situations data can be assumed to follow a parametric distribution.\n\n2.3.1 Uniform\nIn a uniform distribution every possible outcome has the same probability. With \\(n\\) different outcomes, the probability for each outcome is \\(1/n\\).\n\n\n2.3.2 Bernoulli\nA Bernoulli trial is a random experiment with two outcomes; success and failure. The probability of success, \\(P(success) = p\\), is constant. The probability of failure is \\(P(failure) = 1-p\\).\nWhen coding it is convenient to code success as 1 and failure as 0.\nThe outcome of a Bernoulli trial is a discrete random variable, \\(X\\).\n\\[P(X=x) = p(x) = \\left\\{\n\\begin{array}{ll}\np & \\mathrm{if}\\,x=1\\mathrm,\\,success\\\\\n1-p & \\mathrm{if}\\,x=0\\mathrm,\\,failure\n\\end{array}\n\\right.\\]\nUsing the definitions of expected value and variance it can be shown that;\n\\[E[X] = p\\] \\[var(X) = p(1-p)\\]\nA Bernoulli trial can be simulated, as seen in previous section, using the function sample.\n\n\n2.3.3 Binomial\nThe number of successes in a series of independent and identical Bernoulli trials is a binomial random variable, \\(X\\).\n\\(X = \\sum_{i=0}^n Z_i,\\)\nwhere all \\(Z_i\\) describe the outcome of independent and identical Bernoulli trials with probability \\(p\\) for success (\\(P(Z_i=1) = p\\)).\nThe probability mass function of \\(X\\) is called the binomial distribution. In short we use the notation;\n\\[X \\sim Bin(n, p)\\]\nThe probability mass function is\n\\[P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\\] It can be shown that\n\\[E[X] = np\\] \\[var(X) = np(1-p)\\]\nA binomial random variable is the number of successes when sampling \\(n\\) objects with replacement from an urn with objects of two types, of which the interesting type (success) has probability \\(p\\).\nThe probability mass function, \\(P(X=k)\\) can be computed using the R function dbinom and the cumulative distribution function \\(P(X \\leq k)\\) can be computed using pbinom.\nExamples fo binomial random variables;\n\nThe number of patients responding to a treatment out of \\(n\\) patients in a study, if the probability of a patient responding to treatment is \\(p\\).\nThe number of patients experiencing a side effect out of \\(n\\) patients in a study, if the probability of a side effect is \\(p\\).\nthe number of mutations in a gene of length \\(n\\), if the mutations are independent and identically distributed and the probability of a mutation at every single position is \\(p\\).\n\n\n\n2.3.4 Poisson\nThe Poisson distribution describe the number of times a rare event occurs in a large number of trials. Commonly used to describe the number of events during a given time period.\nA rare disease has a very low probability for a single individual. The number of individuals in a large population that catch the disease in a certain time period can be modelled using the Poisson distribution.\nThe probability mass function has a single parameter, \\(\\lambda\\), the expected value, and can be described as;\n\\[P(X=k) = \\frac{\\lambda}{k!}e^{-\\lambda}\\]\nThe expected value \\(\\lambda = n \\pi\\), where \\(n\\) is the number of objects sampled from the population and \\(\\pi\\) is the probability of a single object.\nThe variance is equal to the expected value;\n\\[var(X) = E[X] = \\lambda = n \\pi\\]\nThe Poisson distribution can approximate the binomial distribution if \\(n\\) is large (\\(n&gt;20\\)) and \\(\\pi\\) is small (\\(\\pi&lt;0.05\\) and \\(n\\pi &lt; 10\\)).\nExamples of Poisson random variables;\n\nA rare disease has a very low probability for a single individual. The number of individuals in a large population that catch the disease in a certain time period is a Poisson random variable.\nNumber of reads aligned to a gene region\n\n\n\n2.3.5 Negative binomial\nA negative binomial distribution describes the number of failures that occur before a specified number of successes (\\(r\\)) has occurred, in a sequence of independent and identically distributed Bernoilli trials. \\(r\\) is also called the dispersion parameter.\nIn R: dnbinm, pnbinom, qnbinom\n\n\n2.3.6 Geometric\nThe geometric distribution is a special case of the negative binomial distribution, where \\(r=1\\).\nIn R: dgeom, pgeom, qgeom\n\n\n2.3.7 Hypergeometric distribution\nThe hypergeometric distribution occurs when sampling \\(n\\) objects without replacement from an urn with \\(N\\) objects of two types, of which the interesting type has probability \\(p\\).\nThe probability mass function\n\\[P(X=k) = \\frac{\\binom{Np}{k}\\binom{N-Np}{n-k}}{\\binom{N}{n}}\\] can be computed in R using dhyper and the cumulative distribution function \\(P(X \\leq k)\\) can be computed using phyper.\nExamples of hypergeometric random variables;\n\nIn a student group of 25 individuals, 10 are R beginners. If 5 individuals are randomly choosen to belong to group A, the number of R beginners in group A is a hypergeometric random variable.\nA drug company is producing 1000 pills per day, 5% have an amount of active substance below an acceptable threshold. In a random sample of 10 pills, how many contain to little active substance? The number of pills with to little active substance is a hypergeometric random variable.\nYou investigate the differential expression of 10000 genes, 150 of these genes belong to the super interesting pathway XXX. You run a black-box algorithm that report 80 differentially expressed (DE) genes. If the black-box algorithm choose DE genes at random, the number of DE genes that belong to pathway XXX is a hypergeometric random variable.\n\n\n\n2.3.8 Summary of discrete distributions\nThe binomial, hypoergeometric, negative binomial and poisson distributions have similarities. For large N (large population) the hypergeometric and binomial distributions are very similar. The Poisson distribution has equal variance and mean, whereas the binomial has a variance less than the mean and the negative binomial has a variance greater than the mean.\n\n\n\n\n\nFigure 2.5: Probability mass functions for the binomial distribution (n=20, p=0.1, 0.3 or 0.5), hypergeometric distribution (N=100, n=20, p=0.1, 0.3 or 0.5), negative binomial distribution (n=20, r=n*p, p=0.1, 0.3 or 0.5) and Poisson distribution (n=20, p=0.1, 0.3 or 0.5).\n\n\n\n\nProbability mass functions, \\(P(X=x)\\), for the binomial, hypergeometric, negative binomial and Poisson distributions can in R can be computed using functions dbinom, dhyper, dnbinom and dpois, respectively.\nCumulative distribution functions, \\(P(X \\leq x)\\) can be computed using pbinom, phyper, pnbinom and ppois.\nAlso, functions for computing an \\(x\\) such that \\(P(X \\leq x) = q\\), where \\(q\\) is a probability of interest are available using qbinom, qhyper, qnbinom and qpois."
  },
  {
    "objectID": "prob_exr1_discrv_solutions.html#introduction-to-probability",
    "href": "prob_exr1_discrv_solutions.html#introduction-to-probability",
    "title": "Exercises: Discrete random variables",
    "section": "Introduction to probability",
    "text": "Introduction to probability\n\nExercise 1 (BRCA) The probability of carrying mutations (one or more) in the breast cancer gene BRCA1 is 0.01. What is the probability of not carrying any mutations in BRCA1?\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the rule of complement.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAccording to the rule of complement \\(P(no\\,mutation) = 1 - P(mutations) = 1 - 0.01 = 0.99\\)\n\n\n\n\n\nExercise 2 (A coin toss) When tossing a fair coin\n\nwhat is the probability of heads?\nwhat is the probability of tails?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFair = equal probabilities\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor a fair coin the probability of heads and tails are equal; \\(P(H) = P(T)\\). According to the rule of complement \\(p(H) = 1 - P(T)\\)\nIt follows that\n\n\\(P(H) = 0.5\\)\n\\(P(T) = 0.5\\)\n\n\n\n\n\n\n\nExercise 3 (Number of children) In a region in Sweden with many children the number of children per household is between 0 and 6. The probability mass function is as follows;\n\n\n\nx\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\np(x)\n0.14\n0.20\n0.27\n0.19\n0.13\n0.05\n0.02\n\n\n\nIn a randomly choosen household\n\nwhat is the probability of exactly 3 children?\nwhat is the probability of less than 3 children?\nwhat is the probability of 3 or less children?\nwhat is the probability of an even number of children?\n\nIn your answers, denote the probability with a mathematical expression (such as \\(P(X&gt;4)\\)) and calculate its value.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of children in a random household is a random variable. Let \\(X\\) (a random variable) denote the number of children in a household in the studied region.\n\n\\(P(X=3) = 0.19\\)\n\\(P(X&lt;3) = P(X=0) + P(X=1) + P(X=2) = 0.14 + 0.20 + 0.27 = 0.61\\)\n\\(P(X \\leq 3) = P(X=3) + P(X&lt;3) = 0.19 + 0.61 = 0.80\\)\n\\(P(even\\,X) = P(X=0) + P(X=2) + P(X=4) + P(X=6) = 0.14 + 0.27 + 0.13 + 0.02 = 0.56\\)\n\n\n\n\n\n\nExercise 4 (Rolling dice) When tossing a fair six-sided dice\n\nwhat is the probability of getting 6?\nwhat is the probability of an even number?\nwhat is the probability of getting 3 or more?\nwhat is the expected value of dots on the dice?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nOn fair sided dice, all six sides have equal pobability.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe random variable, \\(X\\), describe the number of dots on the upper face of a dice.\n\n\\(P(X=6) = \\frac{1}{6}\\)\n\\(P(even\\, X) = \\frac{3}{6} = \\frac{1}{2}\\)\n\\(P(X \\geq 3) = \\frac{4}{6} = \\frac{2}{3}\\)\n\\(E[X] = 1*\\frac{1}{6} + 2*\\frac{1}{6} + 3*\\frac{1}{6} + 4*\\frac{1}{6} + 5*\\frac{1}{6} + 6*\\frac{1}{6} = 3.5\\)"
  },
  {
    "objectID": "prob_exr1_discrv_solutions.html#simulation",
    "href": "prob_exr1_discrv_solutions.html#simulation",
    "title": "Exercises: Discrete random variables",
    "section": "Simulation",
    "text": "Simulation\n\n\n\nExercise 5 (Randomization) In a clinical trial, enrolled patients are randomly assigned to treatment or control group with equal probability.\nFor a single patient, what is the probability of being assigned to\n\nthe treatment group?\nthe control group?\n\nIf 20 patients are enrolled in the study;\n\nwhat is the probability of exactly 15 in the treatment group?\nwhat is the probability of less than 7 in the treatment group?\nWhat is the most probable number of patients in the treatment group?\nwhat is the probability of 5 or less patients in the control group?\nwhat is the probability of 2 or less patients in the treatment group?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(P(T)=0.5\\)\n\\(P(C) = 0.5\\)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe probability of assigning to control (C) and treatment (T) group are equal and sum up to 1. Hence \\(P(T) = 0.5\\) and;\n\\(P(C) = 0.5\\)\n\nSimulate the assignment of patients into T or C groups.\n\n## Randomization for a single patient\nsample(c(\"T\", \"C\"), size=1)\n\n[1] \"T\"\n\n## Randomize 20 independent patients\n(patients &lt;- sample(c(\"T\", \"c\"), size=20, replace=TRUE))\n\n [1] \"c\" \"T\" \"T\" \"c\" \"T\" \"T\" \"T\" \"T\" \"c\" \"c\" \"c\" \"T\" \"c\" \"c\" \"T\" \"c\" \"c\" \"c\" \"T\"\n[20] \"T\"\n\n## How many patients are assigned to treatment group?\nsum(patients == \"T\")\n\n[1] 10\n\n## Simulate by repeating 10000 times\nNtreat &lt;- replicate(10000, {\n  patients &lt;- sample(c(\"T\", \"C\"), size=20, replace=TRUE)\n  sum(patients == \"T\")\n})\n\n\nProbability of exactly 15 T\n\n\n## Proportion of the 10000 repeats with exactly 15 T\nmean(Ntreat==15)\n\n[1] 0.016\n\n\n\nProbability of less than 7 T\n\n\nmean(Ntreat&lt;7)\n\n[1] 0.057\n\n\n\nWhat is the most probable number of T patients?\n\n\n## plot the distribution and read the graph\nhist(Ntreat, breaks=0:21-0.5)\n\n\n\n## or tabulate\ntable(Ntreat)\n\nNtreat\n   2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17 \n   1   10   51  143  367  762 1192 1623 1714 1624 1218  721  352  161   45   11 \n  18 \n   5 \n\n\n\nWhat is the probability of 5 C or less?\n\nTo get five or less C out of 20 throws is equal to getting 15 or more T out of 20.\n\n## probability of 15 T or more\nmean(Ntreat&gt;=15)\n\n[1] 0.022\n\n\n\nwhat is the probability of 2 T or less?\n\n\nmean(Ntreat&lt;=2)\n\n[1] 1e-04\n\nsum(Ntreat&lt;=2)\n\n[1] 1\n\n## with this low number of observations, more repeats is required to get a more accurate answer\n  \nNtreat &lt;- replicate(1000000, {\n  patients &lt;- sample(c(\"T\", \"C\"), size=20, replace=TRUE)\n  sum(patients == \"T\")\n})\nsum(Ntreat&lt;=2)\n\n[1] 188\n\nmean(Ntreat&lt;=2)\n\n[1] 0.00019\n\n\n\n\n\n\n\n\n\nExercise 6 (Bacterial colonies) In a bacterial sample, 1/6 are antibiotic resistant. From bacterial colonies on an agar plate, you randomly pick 10 colonies and investigate how many that are antibiotic resistant.\n\nDefine the random variable of interest\nWhat are the possible outcomes?\nUsing simulation, estimate the probability mass function\nwhat is the probability to get at least 5 antibiotic resistant colonies?\nWhich is the most likely number of antibioitic colonies?\nWhat is the probability to get exactly 2 antibiotic resistant colonies?\nOn average how many antibiotic resistant colonies would you get if the experiment is repeated many time? \n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThink of the dice example, where the probability of getting ‘six’ on one dice is 1/6.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(X\\), the number of antibiotic resistant colonies out of 10.\n\\({0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\\)\n\n\n\n##Simulate picking 10 colonies and counting the number of antibiotic resistant ones.\nN &lt;- replicate(100000, sum(sample(1:6, size=10, replace=TRUE)==6))\ntable(N)\n\nN\n    0     1     2     3     4     5     6     7     8     9 \n15986 32327 29148 15521  5466  1306   223    21     1     1 \n\n##The probability mass function\ntable(N)/length(N)\n\nN\n      0       1       2       3       4       5       6       7       8       9 \n0.15986 0.32327 0.29148 0.15521 0.05466 0.01306 0.00223 0.00021 0.00001 0.00001 \n\nhist(N, breaks=(0:11)-0.5)\n\n\n\n\n\n0.015\n\n\n\n[1] 1552\n\n\n[1] 0.016\n\n\n[1] 0.016\n\n\n\n1 (Use the PMF to answer this question)\n0.29\n\n\nmean(N==2)\n\n[1] 0.29\n\n\n\n1.7\n\n\nmean(N)\n\n[1] 1.7\n\n10*1/6\n\n[1] 1.7\n\n\n\n\n\n\n\n\nExercise 7 (Pollen allergy)  \n\n30% of a large population is allergic to pollen. If you randomly select 3 people to participate in your study, what is the probability than none of them will be allergic to pollen?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Solution using 100 replicates\nx &lt;- replicate(100, sum(sample(c(0,0,0,0,0,0,0,1,1,1), size=3, replace=TRUE)))\ntable(x)\n\nx\n 0  1  2  3 \n35 46 16  3 \n\nmean(x==0)\n\n[1] 0.35\n\n## Solution using 1000 replicates\nx &lt;- replicate(1000, sum(sample(c(0,0,0,0,0,0,0,1,1,1), size=3, replace=TRUE)))\ntable(x)\n\nx\n  0   1   2   3 \n335 445 203  17 \n\nmean(x==0)\n\n[1] 0.34\n\n## Solution using 100000 replicates\nx &lt;- replicate(100000, sum(sample(c(0,0,0,0,0,0,0,1,1,1), size=3, replace=TRUE)))\ntable(x)\n\nx\n    0     1     2     3 \n34488 44012 18698  2802 \n\nmean(x==0)\n\n[1] 0.34\n\n\n\n\n\n\nIn a class of 20 students, 6 are allergic to pollen. If you randomly select 3 of the students to participate in your study, what is the probability than none of them will be allergic to pollen?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Solution using 100000 replicates\nx &lt;- replicate(100000, sum(sample(rep(c(0, 1), c(14, 6)), size=3, replace=FALSE)))\ntable(x)\n\nx\n    0     1     2     3 \n31965 47947 18288  1800 \n\nmean(x==0)\n\n[1] 0.32\n\n\n\n\n\n\nOf the 200 persons working at a company, 60 are allergic to pollen. If you randomly select 3 people to participate in your study, what is the probability that none of them are allergic to pollen?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Solution using 100000 replicates\nx &lt;- replicate(100000, sum(sample(rep(c(0, 1), c(140, 60)), size=3, replace=FALSE)))\ntable(x)\n\nx\n    0     1     2     3 \n33774 44726 18798  2702 \n\nmean(x==0)\n\n[1] 0.34\n\n\n\n\n\n\nCompare your results in a, b and c. Did you get the same results? Why/why not?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe results differ. In a the probability of selecting an allergic person is constant, regardless of the status of previously selected persons. On the other hand in the situations in b and c, the probability of selecting an allergic person changes depending on the persons selected before."
  },
  {
    "objectID": "prob_exr1_discrv_solutions.html#parametric-discrete-distribution",
    "href": "prob_exr1_discrv_solutions.html#parametric-discrete-distribution",
    "title": "Exercises: Discrete random variables",
    "section": "Parametric discrete distribution",
    "text": "Parametric discrete distribution\n\nExercise 8 (Pollen) Do Exercise 7 again, but using parametric distributions. Compare your results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## 1.6 Solution using the Binomial distribution\npbinom(0, 3, 0.3)\n\n[1] 0.34\n\n## 1.7 Solution using the hypergeometric distribution\nphyper(0, 6, 20-6, 3)\n\n[1] 0.32\n\n## 1.8 Solution using the hypergeometric distribution\nphyper(0, 60, 200-60, 3)\n\n[1] 0.34\n\n\n\n\n\n\n\nExercise 9 (Gene set enrichment analysis) You have analyzed 20000 genes and a bioinformatician you are collaborating with has sent you a list of 1000 genes that she says are important. You are interested in a particular pathway A. 200 genes in pathway A are represented among the 20000 genes, 20 of these are in the bioinformaticians important list.\nIf the bioinformatician selected the 1000 genes at random, what is the probability to see 20 or more genes from pathway A in this list?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nphyper(20, 200, 20000-200, 1000, lower.tail=FALSE)\n\n[1] 0.0011\n\n\n\n\n\n\n\nExercise 10 (Chance of meeting boss) Your boss comes in to the office three days per week. You do also come in to work three days per week. If you both choose which days to come in to work at random, what is the probability that a particular week you are in the office at the same time 0, 1, 2 or 3 days, respectively?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nx\n    1     2     3 \n29799 60165 10036 \n\n\n[1] 0.0 0.3 0.9 1.0"
  },
  {
    "objectID": "prob_03contrv.html#parametric-continuous-distributions",
    "href": "prob_03contrv.html#parametric-continuous-distributions",
    "title": "3  Continuous random variable",
    "section": "3.1 Parametric continuous distributions",
    "text": "3.1 Parametric continuous distributions\nTwo important parameters of a distribution is the expected value, \\(\\mu\\), that describe the distributions location and the variance, \\(\\sigma^2\\), that describe the spread.\nThe expected value, or population mean, is defined as;\n\\[E[X] = \\mu = \\int_{-\\infty}^\\infty x f(x) dx\\] We will learn more about the expected value and how to estimate a population mean from a sample later in the course.\nThe population variance is defined as the expected value of the squared distance from the population mean;\n\\[\\sigma^2 = E[(X-\\mu)^2] = \\int_{-\\infty}^\\infty (x-\\mu)^2 f(x) dx\\]\nThe square root of the variance is the standard deviation, \\(\\sigma\\)."
  },
  {
    "objectID": "prob_03contrv.html#normal-distribution",
    "href": "prob_03contrv.html#normal-distribution",
    "title": "3  Continuous random variable",
    "section": "3.2 Normal distribution",
    "text": "3.2 Normal distribution\nThe normal distribution (sometimes referred to as the Gaussian distribution) is a common bell-shaped probability distribution. Many continuous random variables can be described by the normal distribution or be approximated by the normal distribution.\nThe normal probability density function\n\\[f(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\]\ndescribes the distribution of a normal random variable, \\(X\\), with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\), \\(e\\) and \\(\\pi\\) are two common mathematical constants, \\(e \\approx 2.71828\\) and \\(\\pi \\approx 3.14159\\).\n\nThe bell-shaped normal distributions is symmetric around \\(\\mu\\) and \\(f(x) \\rightarrow 0\\) as \\(x \\rightarrow \\infty\\) and as \\(x \\rightarrow -\\infty\\).\nAs \\(f(x)\\) is well defined, values for the cumulative distribution function \\(F(x) = \\int_{- \\infty}^x f(x) dx\\) can be computed.\n\n\n\n\n\n\nFigure 3.4: Normal probability density function and cumulative distribution functions.\n\n\n\n\n\n\n\nFigure 3.5: Normal probability density function and cumulative distribution functions.\n\n\n\n\n\nIf \\(X\\) is normally distributed with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\) we write:\n\\[X \\sim N(\\mu, \\sigma)\\]\nUsing transformation rules we can define \\(Z\\), a random variable that is standard normal distributed (mean 0 and standard deviation 1).\n\\[Z = \\frac{X-\\mu}{\\sigma}, \\, Z \\sim N(0,1)\\]\nValues for the cumulative standard normal distribution, \\(F(z)\\), are tabulated and easy to compute in R using the function pnorm.\n\n\n\n\n\nFigure 3.6: The shaded area under the curve is the tabulated value \\(P(Z \\leq z) = F(z)\\).\n\n\n\n\n\n\n\n\nTable 3.1: Cumulative distribution function for the standard normal distribution. The table gives F(z) = P(Z &lt; z) for standard normal Z.\n\n\n\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n\n\n\n\n0.0\n0.5000\n0.5040\n0.5080\n0.5120\n0.5160\n0.5199\n0.5239\n0.5279\n0.5319\n0.5359\n\n\n0.1\n0.5398\n0.5438\n0.5478\n0.5517\n0.5557\n0.5596\n0.5636\n0.5675\n0.5714\n0.5753\n\n\n0.2\n0.5793\n0.5832\n0.5871\n0.5910\n0.5948\n0.5987\n0.6026\n0.6064\n0.6103\n0.6141\n\n\n0.3\n0.6179\n0.6217\n0.6255\n0.6293\n0.6331\n0.6368\n0.6406\n0.6443\n0.6480\n0.6517\n\n\n0.4\n0.6554\n0.6591\n0.6628\n0.6664\n0.6700\n0.6736\n0.6772\n0.6808\n0.6844\n0.6879\n\n\n0.5\n0.6915\n0.6950\n0.6985\n0.7019\n0.7054\n0.7088\n0.7123\n0.7157\n0.7190\n0.7224\n\n\n0.6\n0.7257\n0.7291\n0.7324\n0.7357\n0.7389\n0.7422\n0.7454\n0.7486\n0.7517\n0.7549\n\n\n0.7\n0.7580\n0.7611\n0.7642\n0.7673\n0.7704\n0.7734\n0.7764\n0.7794\n0.7823\n0.7852\n\n\n0.8\n0.7881\n0.7910\n0.7939\n0.7967\n0.7995\n0.8023\n0.8051\n0.8078\n0.8106\n0.8133\n\n\n0.9\n0.8159\n0.8186\n0.8212\n0.8238\n0.8264\n0.8289\n0.8315\n0.8340\n0.8365\n0.8389\n\n\n1.0\n0.8413\n0.8438\n0.8461\n0.8485\n0.8508\n0.8531\n0.8554\n0.8577\n0.8599\n0.8621\n\n\n1.1\n0.8643\n0.8665\n0.8686\n0.8708\n0.8729\n0.8749\n0.8770\n0.8790\n0.8810\n0.8830\n\n\n1.2\n0.8849\n0.8869\n0.8888\n0.8907\n0.8925\n0.8944\n0.8962\n0.8980\n0.8997\n0.9015\n\n\n1.3\n0.9032\n0.9049\n0.9066\n0.9082\n0.9099\n0.9115\n0.9131\n0.9147\n0.9162\n0.9177\n\n\n1.4\n0.9192\n0.9207\n0.9222\n0.9236\n0.9251\n0.9265\n0.9279\n0.9292\n0.9306\n0.9319\n\n\n1.5\n0.9332\n0.9345\n0.9357\n0.9370\n0.9382\n0.9394\n0.9406\n0.9418\n0.9429\n0.9441\n\n\n1.6\n0.9452\n0.9463\n0.9474\n0.9484\n0.9495\n0.9505\n0.9515\n0.9525\n0.9535\n0.9545\n\n\n1.7\n0.9554\n0.9564\n0.9573\n0.9582\n0.9591\n0.9599\n0.9608\n0.9616\n0.9625\n0.9633\n\n\n1.8\n0.9641\n0.9649\n0.9656\n0.9664\n0.9671\n0.9678\n0.9686\n0.9693\n0.9699\n0.9706\n\n\n1.9\n0.9713\n0.9719\n0.9726\n0.9732\n0.9738\n0.9744\n0.9750\n0.9756\n0.9761\n0.9767\n\n\n2.0\n0.9772\n0.9778\n0.9783\n0.9788\n0.9793\n0.9798\n0.9803\n0.9808\n0.9812\n0.9817\n\n\n2.1\n0.9821\n0.9826\n0.9830\n0.9834\n0.9838\n0.9842\n0.9846\n0.9850\n0.9854\n0.9857\n\n\n2.2\n0.9861\n0.9864\n0.9868\n0.9871\n0.9875\n0.9878\n0.9881\n0.9884\n0.9887\n0.9890\n\n\n2.3\n0.9893\n0.9896\n0.9898\n0.9901\n0.9904\n0.9906\n0.9909\n0.9911\n0.9913\n0.9916\n\n\n2.4\n0.9918\n0.9920\n0.9922\n0.9925\n0.9927\n0.9929\n0.9931\n0.9932\n0.9934\n0.9936\n\n\n2.5\n0.9938\n0.9940\n0.9941\n0.9943\n0.9945\n0.9946\n0.9948\n0.9949\n0.9951\n0.9952\n\n\n2.6\n0.9953\n0.9955\n0.9956\n0.9957\n0.9959\n0.9960\n0.9961\n0.9962\n0.9963\n0.9964\n\n\n2.7\n0.9965\n0.9966\n0.9967\n0.9968\n0.9969\n0.9970\n0.9971\n0.9972\n0.9973\n0.9974\n\n\n2.8\n0.9974\n0.9975\n0.9976\n0.9977\n0.9977\n0.9978\n0.9979\n0.9979\n0.9980\n0.9981\n\n\n2.9\n0.9981\n0.9982\n0.9982\n0.9983\n0.9984\n0.9984\n0.9985\n0.9985\n0.9986\n0.9986\n\n\n3.0\n0.9987\n0.9987\n0.9987\n0.9988\n0.9988\n0.9989\n0.9989\n0.9989\n0.9990\n0.9990\n\n\n3.1\n0.9990\n0.9991\n0.9991\n0.9991\n0.9992\n0.9992\n0.9992\n0.9992\n0.9993\n0.9993\n\n\n3.2\n0.9993\n0.9993\n0.9994\n0.9994\n0.9994\n0.9994\n0.9994\n0.9995\n0.9995\n0.9995\n\n\n3.3\n0.9995\n0.9995\n0.9995\n0.9996\n0.9996\n0.9996\n0.9996\n0.9996\n0.9996\n0.9997\n\n\n3.4\n0.9997\n0.9997\n0.9997\n0.9997\n0.9997\n0.9997\n0.9997\n0.9997\n0.9997\n0.9998\n\n\n\n\n\n\n\n\nSome values of particular interest:\n\\[F(1.64) = 0.95\\] \\[F(1.96) = 0.975\\]\nAs the normal distribution is symmetric F(-z) = 1 - F(z)\n\\[F(-1.64) = 0.05\\] \\[F(-1.96) = 0.025\\] \\[P(-1.96 &lt; Z &lt; 1.96) = 0.95\\]\n\n\n\n\n\n3.2.1 Sum of two normal random variables\nIf \\(X \\sim N(\\mu_1, \\sigma_1)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2)\\) are two independent normal random variables, then their sum is also a random variable:\n\\[X + Y \\sim N(\\mu_1 + \\mu_2, \\sqrt{\\sigma_1^2 + \\sigma_2^2})\\]\nand\n\\[X - Y \\sim N(\\mu_1 - \\mu_2, \\sqrt{\\sigma_1^2 + \\sigma_2^2})\\] This can be extended to the case with \\(n\\) independent and identically distributed random variables \\(X_i\\) (\\(i=1 \\dots n\\)). If all \\(X_i\\) are normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), \\(X_i \\in N(\\mu, \\sigma)\\), then the sum of all \\(n\\) random variables will also be normally distributed with mean \\(n\\mu\\) and standard deviation \\(\\sqrt{n} \\sigma\\)."
  },
  {
    "objectID": "prob_03contrv.html#central-limit-theorem",
    "href": "prob_03contrv.html#central-limit-theorem",
    "title": "3  Continuous random variable",
    "section": "3.3 Central limit theorem",
    "text": "3.3 Central limit theorem\n\nTheorem 3.1 (CLT) The sum of \\(n\\) independent and equally distributed random variables is normally distributed, if \\(n\\) is large enough.\n\nAs a result of the central limit theorem, the distribution of fractions or mean values of a sample follow the normal distribution, at least if the sample is large enough (a rule of thumb is that the sample size \\(n&gt;30\\)).\n\n\n\n\n\nExample 3.1 (A skewed distribution) A left skewed distribution has a heavier left tail than right tail. An example might be age at death of natural causes.\n\n\n\n\n\nFigure 3.7: A left skewed distribution. Can for example show the distribution of age of a mouse who died of natural causes.\n\n\n\n\nRandomly sample 3, 5, 10, 15, 20, 30 values and compute the mean value, \\(m\\). Repeat many times to get the distribution of mean values.\n\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n\n\n\nFigure 3.8: Distribution of sample means, where the means are computed based on random samples of sizes 3, 5, 10, 15, 20 and 30, respectively.\n\n\n\n\nNote, mean is just the sum divided by the number of samples \\(n\\)."
  },
  {
    "objectID": "prob_03contrv.html#chi2-distribution",
    "href": "prob_03contrv.html#chi2-distribution",
    "title": "3  Continuous random variable",
    "section": "3.4 \\(\\chi^2\\)-distribution",
    "text": "3.4 \\(\\chi^2\\)-distribution\nThe random variable \\(Y = \\sum_{i=1}^n X_i^2\\) is \\(\\chi^2\\) distributed with \\(n-1\\) degrees of freedom, if \\(X_i\\) are independent identically distributed random variables \\(X_i \\in N(0,1)\\).\nIn short \\(Y \\in \\chi^2(n-1)\\).\n\n\n\n\n\nFigure 3.9: The \\(\\chi^2\\)-distribution.\n\n\n\n\n\nExample 3.2 The sample variance \\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\) is such that \\(\\frac{(n-1)S^2}{\\sigma^2}\\) is \\(\\chi^2\\) distributed with \\(n-1\\) degrees of freedom."
  },
  {
    "objectID": "prob_03contrv.html#f-distribution",
    "href": "prob_03contrv.html#f-distribution",
    "title": "3  Continuous random variable",
    "section": "3.5 F-distribution",
    "text": "3.5 F-distribution\nThe ratio of two \\(\\chi^2\\)-distributed variables divided by their degrees of freedom is F-distributed\n\n\n\n\n\nFigure 3.10: The F-distribution\n\n\n\n\n\nExample 3.3 The ratio of two sample variances is F-distributed"
  },
  {
    "objectID": "prob_03contrv.html#t-distribution",
    "href": "prob_03contrv.html#t-distribution",
    "title": "3  Continuous random variable",
    "section": "3.6 t-distribution",
    "text": "3.6 t-distribution\nThe ratio of a normally distributed variable and a \\(\\chi^2\\)-distributed variable is t-distributed.\n\n\n\n\n\nFigure 3.11: The t-distribution.\n\n\n\n\n\nExample 3.4 The ratio between sample mean and sample variance is t-distributed."
  },
  {
    "objectID": "prob_03contrv.html#distributions-in-r",
    "href": "prob_03contrv.html#distributions-in-r",
    "title": "3  Continuous random variable",
    "section": "3.7 Distributions in R",
    "text": "3.7 Distributions in R\nProbability density functions for the normal, t, \\(\\chi^2\\) and F distributions can in R can be computed using functions dnorm, dt, dchisq, and df, respectively.\nCumulative distribution functions can be computed using pnorm, pt, pchisq and pf.\nAlso, functions for computing an \\(x\\) such that \\(P(X&lt;x) = q\\), where \\(q\\) is a probability of interest are available using qnorm, qt, qchisq and qf."
  },
  {
    "objectID": "prob_04sample.html#random-sampling",
    "href": "prob_04sample.html#random-sampling",
    "title": "4  Sampling and experimental design",
    "section": "4.1 Random sampling",
    "text": "4.1 Random sampling\nIn many (most) experiments it is not feasible (or even possible) to examine the entire population. Instead a random sample is studied.\nA random sample is a random subset of individuals from a population.\nThere are different techniques for performing random sampling, two common techniques are simple random sampling and stratified random sampling.\nA simple random sample is a random subset of individuals from a population, where every individual has the same probability of being choosen. Simple random sampling can be performed using the urn model. If every individual in the population is represented by one ball, a simple random sample of size \\(n\\) can be achieved by drawing \\(n\\) balls from the urn, without replacement.\n\n\n\n\n\nIn stratified random sampling the population is first divided into subpopulations based on important attributes, e.g. sex (male/female), age (young/middle aged/old) or BMI (underweight/normal weight/overweight/obese). Simple random sampling is then performed within each subpopulation."
  },
  {
    "objectID": "prob_04sample.html#principles-of-experimental-design",
    "href": "prob_04sample.html#principles-of-experimental-design",
    "title": "4  Sampling and experimental design",
    "section": "4.2 Principles of experimental design",
    "text": "4.2 Principles of experimental design\nWhen you plan your experiment and assign experimental units to treatment/control group (or similar), it is important to consider extraneous variables, i.e. variables that are not your main interest but that might affect the studied experimental outcome or the variable of interest. These variables can be properties of the experimental units such as age, sex etc. but also variables introduced in the experiment, like batch, experiment date, laboratory personell etc.\nFundamental to experimental design are the three principles; replication, randomization and blocking.\nReplication. Replication is the repetition of the same experiment, with the same conditions. Biological replicates are measurements of different biological units under the same conditions, whereas technical replicates are repeated measurements of the same biological unit under the same conditions.\nRandomization. Experimental units are not identical, hence by assigning experimental units to treatment/control at random we can avoid unnecessary bias. It is also important to perform the measurements in random order.\n\n\nBlocking. Blocking is grouping experimental units into blocks consisting of units that are similar to one another and assigning units within a block to treatment/control at random. Blocking reduces known but irrelevant sources of variation between units and thus allows greater precision in the estimation of the source of variation under study.\nThe experimental units can be grouped into blocks according to their properies (e.g. age, sex, etc). Units within a block will be more similar than between blocks. By assigning units within a block to treatment/control at random, the variation due to differences between blocks (that are not relevant to the studied outcome) can be reduced.\nIn many retrospective studies it is not possible to assign patients into treatment/control or sick/healthy. Experimental design is still important for controling sources of variation introduced during the experiment.\nBlock what you can; randomize what you cannot."
  },
  {
    "objectID": "prob_04sample.html#sample-properties",
    "href": "prob_04sample.html#sample-properties",
    "title": "4  Sampling and experimental design",
    "section": "4.3 Sample properties",
    "text": "4.3 Sample properties\nSummary statistics can be computed for a sample, such as the sum, proportion, mean and variance.\nNotation: A random sample \\(x_1,x_2,\\dots,x_n\\) from a distribution, \\(D\\), consists of \\(n\\) observations of the independent random variables \\(X_1, X_2,\\dots,X_n\\) all with the probability distribution \\(D\\).\n\n4.3.1 Sample proportion\nThe proportion of a population with a particular property, the population proportion, is denoted \\(\\pi\\).\nThe number of individuals with the property in a simple random sample of size \\(n\\) is a random variable \\(X\\). The proportion of individuals in a sample with the property is also a random variable;\n\\[P = \\frac{X}{n}\\] with expected value \\[E[P] = \\frac{E[X]}{n} = \\frac{n\\pi}{n} = \\pi\\].\nThe sample proportion, \\(p\\), is said to be an unbiased estimate of the population proportion, as it’s expected value is the population proportion \\(\\pi\\).\n\n\n4.3.2 Sample mean\nFor a particular sample of size \\(n\\); \\(x_1, \\dots, x_n\\), the sample mean is denoted \\(m = \\bar x\\). The sample mean is calculated as;\n\\[m = \\bar x = \\frac{1}{n}\\displaystyle\\sum_{i=1}^n x_i.\\]\nNote that the mean of \\(n\\) independent identically distributed random variables, \\(X_i\\), is itself a random variable;\n\\[\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i.\\]\nIf \\(X_i\\) are normaly distributed \\(X_i \\sim N(\\mu, \\sigma)\\), then \\(\\bar X \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\).\nWhen we only have a sample of size \\(n\\), the sample mean \\(m\\) is our best estimate of the population mean. It is possible to show that the sample mean is an unbiased estimate of the population mean, i.e. the average (over many size \\(n\\) samples) of the sample mean is \\(\\mu\\).\n\\[E[\\bar X] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_i] = \\frac{1}{n} n \\mu = E[X] = \\mu\\]\n\n\n4.3.3 Sample variance\nThe sample variance is computed as;\n\\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-m)^2.\\]\nThe sample variance is an unbiased estimate of the population variance.\n\n\n4.3.4 Sampling distribution\nA sampling distribution is a probability distribution of a sample property. A sampling distribution is obtained by sampling many times from the studied population.\n\n\n4.3.5 Standard error\nEventhough the sample can be used to calculate unbiased estimates of the population property, the sample estimate will not be perfect. The standard deviation of the sampling distribution is called the standard error, \\(SE\\).\nFor the sample mean, \\(\\bar X\\), the variance is\n\\[E[(\\bar X - \\mu)] = var(\\bar X) = var(\\frac{1}{n}\\sum_i X_i) = \\frac{1}{n^2} \\sum_i var(X_i) = \\frac{1}{n^2} n var(X) = \\frac{\\sigma^2}{n}\\]\nThe standard error of the mean is thus;\n\\[SEM = \\frac{\\sigma}{\\sqrt{n}}\\]\nReplacing \\(\\sigma\\) with the sample standard deviation, \\(s\\), we get an estimate of the standard error of the mean;\n\\[SEM \\approx \\frac{s}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "prob_exr2_contrv_solutions.html#parametric-continuous-distribution",
    "href": "prob_exr2_contrv_solutions.html#parametric-continuous-distribution",
    "title": "Exercises: Continuous random variables",
    "section": "Parametric continuous distribution",
    "text": "Parametric continuous distribution\n\nExercise 1 (The normal table) Let \\(Z \\sim N(0,1)\\) be a standard normal random variable, and compute;\n\n\\(P(Z&lt;1.64)\\)\n\\(P(Z&gt;-1.64)\\)\n\\(P(-1.96&lt;Z)\\)\n\\(P(Z&lt;2.36)\\)\nAn \\(a\\) such that \\(P(Z&lt;a) = 0.95\\)\nA \\(b\\) such that \\(P(Z&gt;b) = 0.975\\)\n\nNote, this exercise can be solved using the standard normal table or using the R functions pnormand qnorm.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(P(Z&lt;1.64) = P(Z \\leq 1.64) = [\\textrm{from table}] = 0.9495\\)\n\\(P(Z&gt;-1.64) = [\\textrm{symmetry}] = P(Z&lt;1.64) = 0.9495\\)\n\\(P(-1.96&lt;Z) = [\\textrm{symmetry}] = P(Z&lt;1.96) = [\\textrm{from table}] = 0.975\\)\n\\(P(Z&lt;2.36) = [\\textrm{from table}] = 0.9909\\)\nStandard normal table gives that \\(a=1.64\\)\n\\(P(Z&gt;b) = 0.975\\), symmetry gives that \\(P(Z&lt;-b)=0.975\\). Look-up in the standard normal table gives that \\(-b=1.96\\), hence \\(b=-1.96\\).\n\n\n\n\n\n\nExercise 2 (Exercise in standardization/transformation) If \\(X \\sim N(3,2)\\), compute the probabilities\n\n\\(P(X&lt;5)\\)\n\\(P(3&lt;X&lt;5)\\)\n\\(P(X \\geq 7)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(P(X&lt;5) = P\\left(\\frac{X-3}{2} &lt; \\frac{5-3}{2}\\right) = P(Z&lt;1) = [\\textrm{from table}] = 0.8413\\)\n\\(P(3&lt;X&lt;5) = P\\left(\\frac{3-3}{2} &lt; \\frac{X-3}{2} &lt; \\frac{5-3}{2}\\right) = P(0&lt;Z&lt;1) = P(Z&lt;1) - P(Z&lt;0) = 0.8413 - 0.5000 = 0.3413\\)\n\\(P(X \\geq 7) = P\\left(\\frac{X-3}{2} \\geq \\frac{7-3}{2}\\right) = P(Z \\geq 2) = 1 - P(Z &lt; 2) = 1 - 0.9772 = 0.0228\\)\n\n\n\n\n\n\nExercise 3 (Hemoglobin) The hemoglobin (Hb) value in a male population is normally distributed with mean 188 g/L and standard deviation 14 g/L.\n\nMen with Hb below 158 g/L are considered anemic. What is the probability of a random man being anemic?\nWhen randomly selecting 10 men from the population, what is the probability that none of them are anemic?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(P(Hb&lt;158) = P(Z&lt;\\frac{158-188}{14}) = P(Z&lt;-2.14) = [table] = 0.016\\) or use R\n\n\npnorm(158, mean=188, sd=14)\n\n[1] 0.01606229\n\n\n\nProbability of one man not being anemic; \\(1-0.016 = 0.984\\). Probability of 10 selected men not being anemic (binomial distribution) \\(0.984^{10} = 0.95\\)\n\n\n\n\n\n\nExercise 4 (Rare disease) A rare disease affects 3 in 100000 in a large population. If 10000 people are randomly selected from the population, what is the probability\n\nthat no one in the sample is affected?\nthat at least two in the sample are affected?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nn &lt;- 10000\np &lt;- 3/100000\nppois(0, n*p)\n\n[1] 0.7408182\n\n\n\n\n\n\nppois(1, n*p, lower.tail=FALSE)\n\n[1] 0.03693631\n\n\n\n\n\n\n\nExercise 5 (Pill) A drug company is producing a pill, with on average 12 mg of active substance. The amount of active substance is normally distributed with mean 12 mg and standard deviation 0.5 mg, if the production is without problems. Sometimes there is a problem with the production and the amount of active substance will be too high or too low, in which case the pill has to be discarded. What should the upper and lower critical values (limits for when a pill is acceptable) be in order not to discard more than 1/20 pills from a problem free production?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(H_0: \\mu=12\\) \\(H_1: \\mu \\neq 12\\)\nSearch \\(x_{low}\\) and \\(x_{up}\\) such that \\(P(x_{low} &lt; X &lt; x_{up}) = 0.05\\)\nFrom table we know that \\(P(-1.96 &lt; Z &lt; 1.96) = 0.05\\)\n\\(Z = \\frac{X - \\mu_0}{\\sigma_0} = \\frac{X-12}{0.5}\\), hence\n\\(-1.96 &lt; Z &lt; 1.96 \\iff -1.96 &lt; \\frac{X-12}{0.5} &lt; 1.96 \\iff 12-1.96*0.5 &lt; X &lt; 12+1.96*0.5 \\iff 11.02 &lt; X &lt; 12.98\\)\nThe lower and upper critical values of active substance should be 11.02 and 12.98 mg.\n\n\n\n\n\nRandom sample\n\nExercise 6 (Exercise in distribution of sample mean) The total cholesterol in population (mg/dL) is normally distributed with \\(\\mu = 202\\) and \\(\\sigma = 40\\).\n\nHow is the sample mean of a sample of 4 persons distributed?\nWhat is the probability to see a sample mean of 260 mg/dL or higher?\nIs there reason to believe that the four persons with mean 260 mg/dL were sampled from another population with higher population mean?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\[\\bar X = \\frac{1}{4}\\sum_{i=1}^4 X_i \\\\\nX_i \\sim N(\\mu, \\sigma) \\\\\n\\bar X \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right) = N(202, 20)\n\\]\n0.0019\nYes\n\n\n\n\n\n\nExercise 7 (Amount of active substance) The amount of active substance in a pill is stated by the manufacturer to be normally distributed with mean 12 mg and standard deviation 0.5 mg. You take a sample of five pills and measure the amount of active substance to; 13.0, 12.3, 12.6, 12.5, 12.7 mg.\n[Note: a-c were already computed in the descriptive statistics session.]\n\nCompute the sample mean\nCompute the sample variance\nCompute the sample standard deviation\ncompute the standard error of mean, \\(SEM\\).\nIf the manufacturers claim is correct, what is the probability to see a sample mean as high as in a) or higher?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n12.62\n0.067\n0.26\n0.22\n0.0028\nSample mean\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- c(13.0, 12.3, 12.6, 12.5, 12.7)\nn &lt;- length(x)\n(m &lt;- sum(x)/n)\n\n[1] 12.62\n\n\n\nSample variance\n\n\n\n[1] 0.067\n\n\n\nSample standard deviation\n\n\n\n[1] 0.2588436\n\n\n\nStandard error of mean, SEM\n\n\n\n[1] 0.2236068\n\n\nNote, here the known standard deviation, \\(\\sigma=0.5\\) is used."
  }
]