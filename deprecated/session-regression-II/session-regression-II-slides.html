<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Regression session II: multiple linear regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Warren W. Kretzschmar" />
    <meta name="date" content="2019-05-22" />
    <link href="session-regression-II-slides_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="session-regression-II-slides_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Regression session II: multiple linear regression
### Warren W. Kretzschmar
### 2019-05-22

---





class: middle, center

# Warmup quiz: revisiting linear model specifications

https://forms.gle/z4qKdrcQj1wqUxBV7

![Revisiting linear model specefications form](./assets/qrcode.quiz-revisiting.png)

---

# Visualizing the Advertising dataset

Get it from the workshop directory on github: `./session-regression-II/data/Advertising.csv`

- Sales for 200 products
- Advertizing money spent on TV, radio, and newspaper ads

.center[How can we best spend advertizing money to maximize sales?]

--

## Live coding demo: Visualize the Advertising dataset and fit simple linear regressions

???


```r
# load the data
ads = read.csv('./data/Advertising.csv')
```

First, we check to see what columns were imported


```r
head(ads)
```

```
##   X    TV radio newspaper sales
## 1 1 230.1  37.8      69.2  22.1
## 2 2  44.5  39.3      45.1  10.4
## 3 3  17.2  45.9      69.3   9.3
## 4 4 151.5  41.3      58.5  18.5
## 5 5 180.8  10.8      58.4  12.9
## 6 6   8.7  48.9      75.0   7.2
```

It looks like a redundant column of row numbers, `X`, has made it into the table.

The other columns look like numbers. That's good.

Now, let us use the `pairs` function to get a quick overview of linear relationships within the dataset. 


```r
# visualize all pairwise relationships
pairs(ads)
```

![](session-regression-II-files/figures/unnamed-chunk-3-1.png)&lt;!-- --&gt;

The pairs plot creates a scatter plot of every pair of variables in a data frame.
First, to clear things up, the variable `X` is uncorrelated with the other columns and does not 
add anything to the dataset. We should probably remove it and replot:


```r
ads = ads[-1]
pairs(ads)
```

![](session-regression-II-files/figures/unnamed-chunk-4-1.png)&lt;!-- --&gt;

Ah, that's better.

From the pairs plot we can see that:

1. TV expenditure appears to be correlated with sales
2. As TV expenditure goes up, the variance associated with sales increases as well
3. radio expenditure appears to be correlated with sales
4. newspaper sales do not look very correlated to sales

It looks like more than one variable could be used to predict sales. 
How would we handle this in the simple regression? 

## Excercise: fitting simple linear regressions on multivariate data

We can fit a simple linear regression for TV vs Sales this way:

```r
lm(Sales ~ TV, data=ads)
```

```
## Error in eval(predvars, data, env): object 'Sales' not found
```

Oops that did not work! Let's see what's up:


```r
names(ads)
```

```
## [1] "TV"        "radio"     "newspaper" "sales"
```
Ah! sales is lower case:

```r
lm(sales ~ TV, data=ads)
```

```
## 
## Call:
## lm(formula = sales ~ TV, data = ads)
## 
## Coefficients:
## (Intercept)           TV  
##     7.03259      0.04754
```
For every \$1000 spent on TV ads, our average sales went up by five units.
Pretty sweet! What about radio?


```r
lm(sales ~ radio, data=ads)
```

```
## 
## Call:
## lm(formula = sales ~ radio, data = ads)
## 
## Coefficients:
## (Intercept)        radio  
##      9.3116       0.2025
```
radio appears to help even more!


```r
lm(sales ~ newspaper, data=ads)
```

```
## 
## Call:
## lm(formula = sales ~ newspaper, data = ads)
## 
## Coefficients:
## (Intercept)    newspaper  
##    12.35141      0.05469
```
newspaper appears to have a similar effect to TV. 
But there seemed to be much more noise between sales and newspaper in the pairs plot.  What's going on?


```r
summary(lm(sales ~ TV, data=ads))
```

```
## 
## Call:
## lm(formula = sales ~ TV, data = ads)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3860 -1.9545 -0.1913  2.0671  7.2124 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***
## TV          0.047537   0.002691   17.67   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.259 on 198 degrees of freedom
## Multiple R-squared:  0.6119,	Adjusted R-squared:  0.6099 
## F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16
```

```r
summary(lm(sales ~ newspaper, data=ads))
```

```
## 
## Call:
## lm(formula = sales ~ newspaper, data = ads)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.2272  -3.3873  -0.8392   3.5059  12.7751 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***
## newspaper    0.05469    0.01658    3.30  0.00115 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 5.092 on 198 degrees of freedom
## Multiple R-squared:  0.05212,	Adjusted R-squared:  0.04733 
## F-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148
```
The answer is in the `\(R^2\)` values: TV has a much higher `\(R^2\)` than newspaper.

---

# Fitting simple linear regressions wrapup

Two problems remain:

1. Process for combining three models for prediction? 
2. Inferrential problems when predictors are correlated.

Multiple linear regression solves these problems.

---

# Multiple linear regression model specifications

`$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_p + \epsilon$$`

- `\(Y\)`: response variable
- `\(X_j\)`: predictor variables
- `\(\beta_j\)`: model coefficients
- `\(\epsilon\)`: noise term

--

`\(\beta_j\)` can be interpreted as the average
increase in `\(Y\)` for one unit increase in `\(X_j\)` while holding all other predictors fixed.

--

An example regression model for the `Advertising` dataset:

`$$Sales = \beta_0 + \beta_1 newspaper + \beta_2 radio + \beta_3 TV + \epsilon$$`

---

# Estimating regression coefficients

`$$\hat{y} = \hat\beta_0 + \hat\beta_1x_1 + ... + \hat\beta_px_p$$`

- `\(\hat{y}\)`: estimated value
- `\(\hat{\beta_j}\)`: estimated regression coefficient

--

We choose `\(\beta_j\)` to minimize the residual sum of squares:

`$$\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$`

--

`$$\text{RSS} = \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_{i1} - ... - \hat\beta_px_{ip})^2$$`

???
The mathematical formulas for estimating the model coefficients in multiple linear regression
work similarly to the formulas for simple linear regression. However, they are more complex and 
require some linear algebra to understand, so we will skip those formulas here. 

---

class: middle, center

# Quiz: What was `\(y_i\)` again?

https://forms.gle/XUxpgxJbkTp1QfDG8

![What was y_i again?](./assets/qrcode.what_was_yi.png)

---

class: center, middle

# Live coding demo: Fitting a multiple linear regression model

???

Above, we fit a simple linear regression model to the Advertising dataset.
We had to fit each predictor separately. Here we will fit a joint
model:


```r
summary(lm(sales ~ TV + newspaper + radio, data=ads))
```

```
## 
## Call:
## lm(formula = sales ~ TV + newspaper + radio, data = ads)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.8277 -0.8908  0.2418  1.1893  2.8292 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***
## TV           0.045765   0.001395  32.809   &lt;2e-16 ***
## newspaper   -0.001037   0.005871  -0.177     0.86    
## radio        0.188530   0.008611  21.893   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.686 on 196 degrees of freedom
## Multiple R-squared:  0.8972,	Adjusted R-squared:  0.8956 
## F-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16
```

We note that the coefficient estimates have changed! TV looks similar 
to what we saw in simple linear regression, but newspaper and radio have
both decreased. This is expected when predictors are correlated.
The multiple linear regression fit takes the correlation between predictors 
into account. This fit states that once we take TV and radio into account, newspaper 
ads do not increase sales.

---

# Questions we can answer with a multiple linear regression

1. Can any of the predictors predict response?
2. If so, to what degree are the predictors important?
3. How well does the model explain the data?
4. What is the model's prediction accuracy?

---

# Can any of the predictors predict response?

## Null hypothesis

`$$H_0: \beta_0 = \beta_1 = ... = \beta_p = 0$$`

--

## Alternate hypothesis

`$$H_A: \text{at least one } \beta_j \text{ is not 0}$$`

---

# The `\(F\)`-test

- named in honor of R. A. Fisher
- uses the `\(F\)`-statistic:

`$$F = \frac{\text{explained variance}}{\text{unexplained variance}}$$`

???

In a model in which our assumptions hold, and for which our null hypothesis is true, 
we expect the explained and unexplained variances to be equal. Therefore, under the 
null hypothesis, the `\(F\)`-statistic should be around 1.

---

# Estimating explained variance

`$$\frac{TSS-RSS}{p}$$`

- TSS: total sum of squares
- RSS: residual sum of squares

--

# Estimating unexplained variance

`$$\frac{RSS}{(n-p-1)}$$`

???

For any model, as we increase the number of predictors the
amount of variance that we explain with our model increases. 
The denominator, `\(p\)`, corrects for this.

As with the explained variance above, `\(p\)` in this denominator corrects for 
how unexplained variance decreases as we add more predictors. We can also see
that as our sample size, `\(n\)`, increases, our RSS increases as well.

---

# The `\(F\)`-statistic for a linear model

`$$F = \left. {\color{cyan}{\frac{(TSS - RSS)}{p}}} \middle/ {\color{magenta}{\frac{RSS}{(n-p-1)}}} \right.$$`

- `\(\color{cyan}{\text{explained variance}}\)`
- `\(\color{magenta}{\text{unexplained variance}}\)`

???

If our null hypothesis is not true, then, assuming that the `\(TSS\)` remains constant,
we can expect the `\(RSS\)` to become smaller compared to the `\(TSS\)`, and
for the ratio to increase. Therefore, the larger the `\(F\)`-statistic, the more evidence there is for
rejecting the null hypothesis.

---

class: middle, center

# Live coding demo: Understanding the `\(F\)`-statistic 

???

`R` will happily calculate the `\(F\)`-statistic of a linear model fit for us. One way to get the
`\(F\)` statistic is using the `summary()` function:


```r
summary(lm(sales ~ TV + newspaper + radio, data=ads))
```

```
## 
## Call:
## lm(formula = sales ~ TV + newspaper + radio, data = ads)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.8277 -0.8908  0.2418  1.1893  2.8292 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***
## TV           0.045765   0.001395  32.809   &lt;2e-16 ***
## newspaper   -0.001037   0.005871  -0.177     0.86    
## radio        0.188530   0.008611  21.893   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.686 on 196 degrees of freedom
## Multiple R-squared:  0.8972,	Adjusted R-squared:  0.8956 
## F-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16
```

In this case, the `\(F\)`-statistic is 570. That looks like a lot, and the low
`\(p\)`-value confirms that it is.

### Excercise: playing around with `\(n\)`

We can get an idea of how sample size changes the `\(F\)`-statistic and the 
`\(p\)`-value associated with the statistic. 

We can use `sample()` to reduce the sample size:


```r
?sample
```

We want to be sure to sample without replacement, and the help page tells us that this is the default


```r
summary(lm(sales ~ TV + newspaper + radio, data=ads[sample(nrow(ads), 200),]))
```

```
## 
## Call:
## lm(formula = sales ~ TV + newspaper + radio, data = ads[sample(nrow(ads), 
##     200), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.8277 -0.8908  0.2418  1.1893  2.8292 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***
## TV           0.045765   0.001395  32.809   &lt;2e-16 ***
## newspaper   -0.001037   0.005871  -0.177     0.86    
## radio        0.188530   0.008611  21.893   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.686 on 196 degrees of freedom
## Multiple R-squared:  0.8972,	Adjusted R-squared:  0.8956 
## F-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16
```

```r
summary(lm(sales ~ TV + newspaper + radio, data=ads[sample(nrow(ads), 50),]))
```

```
## 
## Call:
## lm(formula = sales ~ TV + newspaper + radio, data = ads[sample(nrow(ads), 
##     50), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.2757 -0.7849  0.4184  1.2662  2.5785 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 2.247096   0.703314   3.195  0.00253 ** 
## TV          0.051863   0.003131  16.563  &lt; 2e-16 ***
## newspaper   0.006819   0.012470   0.547  0.58714    
## radio       0.164974   0.018727   8.809 1.95e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.855 on 46 degrees of freedom
## Multiple R-squared:  0.8882,	Adjusted R-squared:  0.8809 
## F-statistic: 121.8 on 3 and 46 DF,  p-value: &lt; 2.2e-16
```

```r
summary(lm(sales ~ TV + newspaper + radio, data=ads[sample(nrow(ads), 10),]))
```

```
## 
## Call:
## lm(formula = sales ~ TV + newspaper + radio, data = ads[sample(nrow(ads), 
##     10), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.3156 -0.5842  0.1551  0.6912  0.9508 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.39671    2.08503   3.068 0.022000 *  
## TV           0.02705    0.01032   2.621 0.039523 *  
## newspaper   -0.04501    0.02161  -2.083 0.082354 .  
## radio        0.25247    0.02842   8.883 0.000113 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.015 on 6 degrees of freedom
## Multiple R-squared:  0.9681,	Adjusted R-squared:  0.9521 
## F-statistic: 60.62 on 3 and 6 DF,  p-value: 7.04e-05
```

It looks like the signal is very strong, and we can still reject our null hypothesis 
with 10 data points.

The model summaries have p-values next to each model coefficient. 
These p-values are based on the `\(t\)`-statistic and are calculated in the same way as 
for simple linear regression (see previous section). 
Why can't we just use these p-values instead of the `\(F\)`-statistic to determine
if any coefficients are non-zero? 
Without correction for multiple testing we will get false positives at the probability of
`\(\alpha\)` per test.
If we do correct for multiple testing, then the `\(F\)`-test is more powerful and/or
requires fewer assumptions about the correlation structure of the `\(t\)`-statistics.

---

# Variable selection: Which predictors predict response?

--

- test all models? Not so fast.

1. Avoid multiple testing 
2. Number of possible models is `\(2^p\)`


???

- reduced set of predictors

Ideas?

- `\(t\)`-test? -&gt; false positives

Test all models!

Problems

1. Determining which model fits best while correcting for multiple testing and overfitting
2. The number of models that can be created from a subset of `\(p\)` predictors is
   `\(2^p\)`, and this number can get large very quickly.

---

# Avoiding multiple-testing with information criteria

- *Mallow's C*
- *Akaike's Information Criterion* (AIC) 
- *Bayesian Information Criterion* (BIC)

`$$AIC= 2p - 2 \ln(\hat{L})$$`

---

# Model search space reduction

- Stepwise selection through `stepAIC()`

???

- add
- remove
- add/remove

Not perfect but useful!

--

## Exercise: Let's select predictors in a model!


???

- `stepAIC()` default: AIC

We can start with the full model:

```r
library(MASS)
full = lm(sales ~ TV + newspaper + radio, data=ads)
summary(stepAIC(full))
```

```
## Start:  AIC=212.79
## sales ~ TV + newspaper + radio
## 
##             Df Sum of Sq    RSS    AIC
## - newspaper  1      0.09  556.9 210.82
## &lt;none&gt;                    556.8 212.79
## - radio      1   1361.74 1918.6 458.20
## - TV         1   3058.01 3614.8 584.90
## 
## Step:  AIC=210.82
## sales ~ TV + radio
## 
##         Df Sum of Sq    RSS    AIC
## &lt;none&gt;                556.9 210.82
## - radio  1    1545.6 2102.5 474.52
## - TV     1    3061.6 3618.5 583.10
```

```
## 
## Call:
## lm(formula = sales ~ TV + radio, data = ads)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.7977 -0.8752  0.2422  1.1708  2.8328 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***
## TV           0.04575    0.00139  32.909   &lt;2e-16 ***
## radio        0.18799    0.00804  23.382   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.681 on 197 degrees of freedom
## Multiple R-squared:  0.8972,	Adjusted R-squared:  0.8962 
## F-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16
```

This returns a model in which newspaper (unsurprisingly) has been removed.

Or we can start with an empty model:


```r
empty = lm(sales ~ 1, data=ads)
summary(stepAIC(empty, scope=sales ~ TV + newspaper + radio))
```

```
## Start:  AIC=661.8
## sales ~ 1
## 
##             Df Sum of Sq    RSS    AIC
## + TV         1    3314.6 2102.5 474.52
## + radio      1    1798.7 3618.5 583.10
## + newspaper  1     282.3 5134.8 653.10
## &lt;none&gt;                   5417.1 661.80
## 
## Step:  AIC=474.52
## sales ~ TV
## 
##             Df Sum of Sq    RSS    AIC
## + radio      1    1545.6  556.9 210.82
## + newspaper  1     184.0 1918.6 458.20
## &lt;none&gt;                   2102.5 474.52
## - TV         1    3314.6 5417.1 661.80
## 
## Step:  AIC=210.82
## sales ~ TV + radio
## 
##             Df Sum of Sq    RSS    AIC
## &lt;none&gt;                    556.9 210.82
## + newspaper  1      0.09  556.8 212.79
## - radio      1   1545.62 2102.5 474.52
## - TV         1   3061.57 3618.5 583.10
```

```
## 
## Call:
## lm(formula = sales ~ TV + radio, data = ads)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.7977 -0.8752  0.2422  1.1708  2.8328 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***
## TV           0.04575    0.00139  32.909   &lt;2e-16 ***
## radio        0.18799    0.00804  23.382   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.681 on 197 degrees of freedom
## Multiple R-squared:  0.8972,	Adjusted R-squared:  0.8962 
## F-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16
```

And in this case we get the same model as above.

---

# How well does the model explain the data?

Similarly to simple linear regression:

- `\(R^2\)`
- RSE

---

# `\(R^2\)`

- In simple linear linear regression: `\(R^2 = \text{Cor}(X, Y)^2\)`
- In multiple linear regression: `\(R^2 = \text{Cor}(Y, \hat{Y})\)`

## Live coding: Let's see what the `\(R^2\)` of our models look like


```r
summary(lm(sales ~ TV , data=ads))$r.squared
summary(lm(sales ~ TV + radio, data=ads))$r.squared
summary(lm(sales ~ TV + radio + newspaper, data=ads))$r.squared
```

???

- As we add predictors `\(R^2\)` always increases
- When the predictors explain no variance: `\(R^2 = 0\)`
- When they explain all variance: `\(R^2 = 1\)`.

---

# RSE

- In simple linear regression we learned: `\(\text{RSE} = \sqrt{RSS/(n-2)}\)`
- In multiple linear regression: `\(\text{RSE} = \sqrt{RSS/(n-p-1)}\)`

## Live coding: Let's see what the RSE of our models look like:


```r
summary(lm(sales ~ TV , data=ads))$sigma
summary(lm(sales ~ TV + radio, data=ads))$sigma
summary(lm(sales ~ TV + radio + newspaper, data=ads))$sigma
```

???

- The model that includes `newspaper` has a higher RSE than the model with only
`TV` and `radio`
- This is because RSE depends on `\(p\)`

---

# Visual inspection

- Plotting the data can reveal problems that are not evident from metrics

## Live coding: checking model fit

The model that includes `TV` and `radio` has some issues. Let's see if we can discover
them by visualization. 

???

- favorite: the quantiles plot:


```r
fit = lm(sales ~ TV + radio, data=ads)
plot(fit, which=2)
```

![](session-regression-II-files/figures/unnamed-chunk-19-1.png)&lt;!-- --&gt;

- looks bad!

- Let's get a better idea
- `\(\hat{Y}\)` (fitted values) vs residuals
- why?


```r
plot(fit, which=1)
```

![](session-regression-II-files/figures/unnamed-chunk-20-1.png)&lt;!-- --&gt;

- dip in residuals
- heteroskedasticity
- model fit not adequate

- In fact, there's synergy


```r
# Here we rely on resid() returning the residuals in the same order as the ads data.frame
plot_dat = data.frame(TV=ads$TV, radio=ads$radio, residual=resid(fit))

library(ggplot2)
ggplot(aes(x=TV, y=radio, color=residual), data=plot_dat) + geom_point() +
  scale_color_gradient2()
```

![](session-regression-II-files/figures/unnamed-chunk-21-1.png)&lt;!-- --&gt;

- The model overestimates sales generated from investment in a single ads platform 
  (top left and bottom right)
- The model underestimates sales generated from investment in both add platforms
  (top right and bottom left)

- can be modeled using "interaction term" -&gt; maybe time for later?  

---

# Prediction accuracy -- sources of error

1. *reducible error*: A result of the difference between the estimates 
   `$$\hat{Y} = \hat\beta_0 + \hat\beta_1 X_1 + ... + \hat\beta_p X_p$$`
   and the *true population regression plane*
   `$$f(x) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p$$`
   We use *confidence intervals* to estimate this error.

???

There are three sources of error when making predictions from a linear model

--

2. *model bias*: When our model differs from the true model 
   - for example, the missing interaction effect we just saw
   
--

3. *irreducible error*: `\(\epsilon\)`
   - the random noise that is part of our system
   - We can use *prediction intervals* to estimate this error.

---

## Live coding: confidence and prediction intervals around `\(\hat{Y}\)` of our (dubious) model fit

???

- Prediction intervals are always larger than confidence intervals
- because prediction intervals quantify both the **reducible** and **irreducible error**.

- Previously: we can make predictions from our model using the `predict()` function.


```r
# making a prediction on a new data point
predict(fit, newdata=data.frame(TV=100, radio=30))
```


```r
preds = as.data.frame(predict(fit, interval="confidence"))
head(preds)
preds2 = as.data.frame(predict(fit, interval="prediction"))
```

```
## Warning in predict.lm(fit, interval = "prediction"): predictions on current data refer to _future_ responses
```

```r
head(preds2)
preds$lwr_pred = preds2$lwr
preds$upr_pred = preds2$upr

preds = preds[order(preds$fit),]
preds$index = 1:nrow(preds)
head(preds)
```


```r
plot(preds$fit, type='l')
lines(preds$index, preds$upr, col='red')
lines(preds$index, preds$lwr, col='red')
lines(preds$index, preds$upr_pred, col='blue')
lines(preds$index, preds$lwr_pred, col='blue')
```

![](session-regression-II-files/figures/unnamed-chunk-24-1.png)&lt;!-- --&gt;

- prediction intervals are larger than the confidence intervals. 
- However, neither the confidence intervals nor the prediction intervals are valid here.

--

## Caveats

- Bad model fit -&gt; substantial model bias
- Prediction intervals are for future data
  
???

- Generally, we are interested in prediction intervals for **new data**
- For that we need to calculate prediction intervals on a separate (or held out) data set.

---

# Further reading

.pull-left[This lecture closely followed section 3.2 in 
*An Introduction to Statistical Learning, with applications in R*]

.pull-right[![An Introduction to Statistical Learning](./assets/qrcode.int_stat_learn.png)]

--

## But there's more!

- Section 3.3 is worth a read before you start 
fitting linear models to your data. That section discusses the following topics:

  - Qualitative predictors 
  - Interaction terms
  - Non-linear transformation of the predictors 
  - Potential problems: non-linearity, collinearity, outliers, heteroskedasticity
  - Logistic regression

--

- Check out `ggplot2`!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": ""
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
