---
title: "Exercises: Clustering"
author: "Eva Freyhult, Mun-Gwan Hong"
date: "2022-09-15"
output:
  html_document:
    theme: null
    self_contained: no
params:
  show_the_output: false
knit: (function(input, ...) rmarkdown::render(input, ..., output_dir= "html"))
---

```{r, setup, echo=FALSE, include=FALSE}
library(ggplot2)

knitr::opts_chunk$set(
  include = params$show_the_output, # control the output
  cache.path = "cache/",   # auto. created unless exists
  results = "asis",   # for HTML
  message = FALSE,    # no message from the code
  warning = FALSE    # no warning from the code
)
```


#### Exercise 1

Using Edgar Anderson's iris data, we will investigate K-means clustering.
The data is stored in `iris` in R.
Here we focus on `Sepal.Width` and `Petal.Width`.

[![Iris](images/iris-flower-1592163215Z1C.jpg)](https://www.publicdomainpictures.net/en/view-image.php?image=342859&picture=iris-flower)

&nbsp;

a) Examine the contents and data types of the data set *iris*.

```{r, results='markup', fig.dim=c(5, 5)}
str(iris)
summary(iris)
plot(Sepal.Width ~ Petal.Width, iris, col = Species)
df0 <- iris[, c('Sepal.Width', 'Petal.Width')]
```

b) Try to find three clusters in the data of sepal and petal widths using a k-means algorithm.
   You can use the function `kmeans` in R.
   It allows to choose an algorithm with the argument `algorithm` (e.g. `algorithm = "Forgy"`).

```{r}
## Number of clusters
n_clust <- 3
## k-means with k = 3 and Forgy's algorithm
km <- kmeans(df0, centers = n_clust, algorithm = "Forgy")
## k-means with k = 3 and Hartigan and Wong's algorithm (as the default)
km <- kmeans(df0, centers = n_clust)
```

c) Plot the data and color by cluster ID

```{r}
df <- df0
## The cluster identities are stored in km$cluster, same order as input 
df$Cluster <- factor(km$cluster)
## Plot using base R graphics
plot(Sepal.Width ~ Petal.Width, df, col = Cluster)
## alternatively, plot using ggplot
ggplot(df, aes(Petal.Width, Sepal.Width, color = Cluster)) + 
  geom_point()
```

d) Repeat the same analysis a few times. Do you get the same clusters all the time?
   Use `table` to check it allowing different cluster numbers.

```{r, results='markup', fig.dim=c(7, 4)}
## the same analysis
km2 <- kmeans(df0, centers = n_clust)
## Compare the results. Note cluster numbers can be changed.
table(km$cluster, km2$cluster)

## Visualize the difference
if(sum(table(km$cluster, km2$cluster) > 0) != n_clust) { # different
  df2 <- df0
  df2$Cluster <- factor(km2$cluster)
  df$model  <- "1st model"
  df2$model <- "2nd model"
  df3 <- rbind(df, df2)
  ggplot(df3, aes(Petal.Width, Sepal.Width, color = Cluster)) + 
    geom_point() + 
    facet_grid(~ model)
}
```

e) Try the argument `nstart` out and compare how stable two runs are.
   By setting the argument to 4, the algorithm will automatically try four different (random) starting points. 

```{r, results='markup'}
km3.1 <- kmeans(df0, centers = n_clust, nstart = 4)
km3.2 <- kmeans(df0, centers = n_clust, nstart = 4)
table(km3.1$cluster, km3.2$cluster)
```

f) Try different values for $k$, run the k-means algorithm and collect the WSS (`tot.withinss`).
   Plot WSS vs $k$. 
   The WSS is always decreasing as $k$ increases, but the curve can still give you a hint of which $k$ to choose.
   The Elbow method for selecting $k$ is to look at this curve and choose the $k$ that you find at the bend of the curve, at the 'elbow'.
   Which $k$ would you pick based on this?
  
```{r}
wss <- sapply(1:10, function(k) kmeans(df0, centers = k, nstart = 4)$tot.withinss)
plot(1:10, wss, type = "b")
```

g) Compute the Silhouette width of the first object (or observation) when $k$ is 2.

```{r, results='markup'}
k <- 2
cl <- kmeans(df0, k)$cluster
## Euclidean distances
d <- sapply(1:nrow(df0), function(ii) sqrt(sum((df0[1, ] - df0[ii, ])^2)))
## a_i = 1 / (|Ca| - 1) * sum(d(i, j))  in the same cluster Ca
ai <- sum(d[cl == cl[1]]) / (sum(cl == cl[1]) - 1)
## other cluster IDs
other_cl <- unique(cl[cl != cl[1]])
## b_i = min( mean(d(i, Cb)) )  average distance to another cluster Cb
bi <- min(sapply(other_cl, function(ocl) mean(d[cl == ocl])))
si <- (bi - ai) / max(ai, bi)
print(si)
```

h) (Optional) Compute average Silhouette width of all objects adjusting $k$ from 2 to 10 and choose optimal $k$.
   Euclidean distance can be computed by `dist`. 
   Check the help of the function using `?dist`.

```{r, results='markup'}
n <- nrow(df0)
dm <- as.matrix(dist(df0))   # matrix output for row wise analysis
sil <- sapply(2:10, function(k) {
  cl <- kmeans(df0, k)$cluster
  si <- sapply(1:n, function(ii) {
    ## a_i = 1 / (|Ca| - 1) * sum(d(i, j))  in the same cluster Ca
    ai <- sum(dm[ii, cl == cl[ii]]) / (sum(cl == cl[ii]) - 1)
    ## other cluster IDs
    other_cl <- unique(cl[cl != cl[ii]]) 
    ## b_i = min( mean(d(i, Cb)) )  average distance to another cluster Cb
    bi <- min(sapply(other_cl, function(ocl) mean(dm[ii, cl == ocl])))
    ## s_i
    (bi - ai) / max(ai, bi)
  })
  mean(si)
})
print(sil)
plot(2:10, sil)
```

i) (Optional) Try `fviz_nbclust` of the R-package `factoextra` and choose optimal $k$ based on average Silhouette width.

```{r}
# install.packages("factoextra")  # if the package was not installed before
factoextra::fviz_nbclust(df0, kmeans, method="sil")
```

&nbsp;

#### Exercise 2

The NCI60 data set consists of gene expression values for 6830 genes for 64 cell lines.
Using this data set, we investigate a few hierarchical clustering distances and linkage methods.
The data can be downloaded in R using the following command

```
nci_data <- read.table(
  url("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv"),
  sep = ",",
  row.names = 1,
  header = TRUE
)
nci_label <- scan(
  url("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label.txt"),
  what = ""
)
```

```{r, echo=FALSE}
nci_data <- read.table(
  url("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv"),
  sep = ",",
  row.names = 1,
  header = TRUE
)
nci_label <- scan(
  url("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label.txt"),
  what = ""
)
```

&nbsp;

a) What is the size of the data matrix? 
   Do every column represent a gene or a cell line?

```{r, results='markup'}
dim(nci_data)
nci_data[1:5, 1:5]
## ANS : Rows are genes, and columns are cell lines
```

b) Compute the Euclidean distance **between cell lines**.
   This can be accomplished using the function `dist`.
   Read the help text `?dist`.
   This function computes the distance between rows of the input data matrix.
   If the rows represent cell lines, you can run `dist(nci_data)`.
   But, if your cell lines are represented by columns, you need to transpose the data matrix first `t(nci_data)`.

```{r}
d <- dist(t(nci_data))
```

c) Cluster the cell lines using complete linkage hierarchical clustering, use the function `hclust`.

```{r}
hc <- hclust(d, method = "complete")   # no need to specify the method
```

d) Plot the dendrogram of the clustering result (The help for `hclust` includes the function for the plot)
   - Try changing the labels to something more informative using `nci_label`. 
   - Investigate the argument `hang`, what happens if you set it to -1?

```{r}
plot(hc)
plot(hc, labels = nci_label)
plot(hc, labels = nci_label, hang = -1)
```

e) Try the linkage methods "single", "average" and "ward.D" in addition to "complete". Compare the results. Which method is 'best'?

```{r}
hc_s <- hclust(d, method = "single")
plot(hc_s, labels = nci_label, main = "Single")
hc_a <- hclust(d, method = "average")
plot(hc_a, labels = nci_label, main = "Average")
hc_w <- hclust(d, method = "ward.D")
plot(hc_w, labels = nci_label, main = "ward.D")
```

Pick the tree resulting from the method you think is 'best'. How many clusters are there?

You can cut the tree on any level to get between 1 and 64 clusters. The function `cutree` either on a specific height (dissimilarity) or to get a specific number of clusters.

