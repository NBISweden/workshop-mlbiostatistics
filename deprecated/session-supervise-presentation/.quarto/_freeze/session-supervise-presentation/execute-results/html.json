{
  "hash": "1a88bb84ac7e51d88a0f577b1f8ccd57",
  "result": {
    "markdown": "---\ntitle: \"Introduction to supervised learning\"\n# author: Olga Dethlefsen\nformat: \n  revealjs:\n    slide-number: true\n    theme: [default, custom.scss]\n    chalkboard: \n      buttons: true\n  html:\n    code-fold: false\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n\n\n## What is supervised learning?\n\n::: incremental\n-   When we talked earlier about PCA and clustering, we were interested in finding patterns in the data. We treated data set as a whole, using all the samples, and we did not use samples labels in any way to find the components with the highest variables (PCA) or the number of clusters (k-means).\n-   In supervised learning, we are using sample **labels** to build (train) our models. When then use these trained models for interpretation and **prediction**.\n:::\n\n## Supervised classification\n\n::: incremental\n-   Classification methods are algorithms used to categorize (classify) objects based on their measurements.\n-   E.g. given a set of gene expression measurements we may want to be able to say whether a cancer patient falls into category of cancer stages I, II, III or IV.\n-   Classification methods belong under **supervised learning** as we usually start off with **labeled** data, i.e. observations with measurements for which we know the labels (class) of.\n:::\n\n## Supervised classification\n\n::: incremental\n-   Let's for each observations $i$ collect pair of information $\\{\\mathbf{x_i}, g_i\\}$\n-   where $\\{\\mathbf{x_i}\\}$ is a set of exploratory variables e.g. a gene expression data\n-   and $g_i \\in \\{1, \\dots, G\\}$ is the class label for each observation (known), e.g. cancer stage I, II, III or IV\n-   Then we want to find a **classification rule** $f(.)$ (model) such that $$f(\\mathbf{x_i})=g_i$$\n:::\n\n## KNN example\n*example of a classification algorithm*\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n:::{.r-stack}\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-00-1.png){.fragment width=\"700\" height=\"600\"}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-01-1.png){.fragment width=\"700\" height=\"600\"}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-02-1.png){.fragment width=\"700\" height=\"600\"}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-03-1.png){.fragment width=\"700\" height=\"600\"}\n:::\n\n\n## KNN example\n*example of a classification algorithm*\n\n\n\n\n\n\n\n\n\n\n\n:::{.r-stack}\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-10-1.png){.fragment width=\"700\" height=\"600\"}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-20-1.png){.fragment width=\"700\" height=\"600\"}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-30-1.png){.fragment width=\"700\" height=\"600\"}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-40-1.png){.fragment width=\"700\" height=\"600\"}\n:::\n\n## Data splitting\n\n::: incremental\n-   Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\n-   As a results the trained model may not be generalizable to future data due to the added complexity that only works for given unique data set, leading to so called **overfitting**.\n-   To deal with overconfident estimation of future performance we randomly split data into training data, validation data and test data.\n:::\n\n## Data splitting\n\n*train, validation & test sets*\n\n-   Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\n-   **Training data**: this is data used to fit (train) the classification model, i.e. derive the classification rule\n-   **Validation data**: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters\n-   **Test data**: this data is used to give an estimate of future prediction performance for the model and parameters chosen\n\n![](figures/split.png)\n\n## Data splitting {.smaller}\n\n*k-fold*\n\n-   It could happen that despite random splitting in train/validation/test dataset one of the subsets does not represent data. e.g. gets all the difficult observation to classify.\n-   Or that we do not have enough data in each subset after performing the split.\n-   In **K-fold cross-validation** we split data into $K$ roughly equal-sized parts.\n-   We start by setting the validation data to be the first set of data and the training data to be all other sets.\n-   We estimate the validation error rate / correct classification rate for the split.\n-   We then repeat the process $K-1$ times, each time with a different part of the data set to be the validation data and the remainder being the training data.\n\n![](figures/split-kfold.png)\n\n::: notes\n-   We finish with $K$ different error of correct classification rates.\n-   In this way, every data point has its class membership predicted once.\n-   The final reported error rate is usually the average of $K$ error rates.\n:::\n\n## Data splitting\n\n*Leave One Out Cross-Validation*\n\n![](figures/split-loocv.png)\n\n## Evaluating Classification Model Performance\n\n::: incremental\n-   To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of $k$ in KNN.\n-   There are few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\n-   Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n:::\n\n## Evaluating Classification Model Performance\n\n**Correct (miss)classification rate**\n\n-   A simple way to evaluate in which we count for all the $n$ predictions how many times we got the classification right. $$Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}$$ where:\n\n-   $1[]$ is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\n\n. . .\n\n<br>\n\nMissclassification Rate = 1 - Correct Classification Rate\n\n## Putting it all together with KNN\n\n*Live demo*\n\n::: incremental\n-   Let's train KNN model on the iris data set to be able to use it for prediction\n-   by training, we mean finding the best value of $k$\n-   and by prediction, we mean that when taking an unknown species of iris and measuring length and width of petals and sepals we can tell whether it is setosa, versicolor or virgnica.\n:::\n\n## Going back to regression\n\n*Could we use supervise learning for regression?*\n\n## Going back to regression\n\nYes! We do both supervised classification and regression when building predictive models.\n\n## Going back to regression\n\n-   The idea of using data splits to train the model holds for fitting (training) regression models.\n-   If we were to use regression in supervised learning context, we would use data splits to train and assess the regression model.\n-   For instance, given a number of variables of interest, we could try to find the best regression model using train data to fit the model and assess on the validation data; while keeping the test to assess the performance on the final model. Or we could use cross validation (We have seen before how to fit the model and assess the model fit, e.g. with $R^2$.)\n-   Other popular regression performance metrics include **RMSE**, root mean square error $$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y_i})^2}$$\n-   and **MAE**, mean absolute error, $$MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y_i}|$$\n\n## Note on exercise\n\nExercise 01: practice data splitting strategies when working with the Breast Cancer data. Try out and compare:\n\n-   \n\n    i)  splitting into train, validation and test\n\n-   \n\n    ii) k-folds\n\n-   \n\n    iii) LOOVC\n\n## Thank you for listening\n\n*Any questions?*\n",
    "supporting": [
      "session-supervise-presentation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}