# Matrices

**Aims**

- to introduce matrix and basic matrices operations

**Learning outcomes**

- to be able to write matrices using matrix notations
- to be able to perform simple matrix operations such as adding and multiplication
- to be able to find the reverse of the 2-dimensional matrix

## Matrix
A matrix is a rectangular array of numbers e.g. 

$$\mathbf{A}=\begin{bmatrix}
  x_{11} & x_{12} & x_{13} & \dots & x_{1n} \\
  x_{21} & x_{22} & x_{23} & \dots & x_{2n} \\
  \dots & \dots & \dots& \dots & \dots\\
  x_{m1} & x_{m2} & x_{1m3} & \dots & x_{mn} \\
\end{bmatrix}$$

where:

- the notional subscripts in the typical element $x_{ij}$ refers to its row and column location in the array, e.g. $x_{12}$ refers to element in the first row and second column
- we say that matrix has $m$ rows and $n$ columns and the **dimension** of a matrix is defined as $m \times n$
- a matrix can be viewed as a set of column vectors or a set of row vectors
- a vector can be viewed as a matrix with only one column or with only one row

## Special matrices
- A matrix with the same number of rows as columns, $m = n$, is said to be a **square matrix**
- A matrix that is not squared, $m \neq n$ is called **rectangular matrix**
- A **null matrix** is composed of all 0
- An **identity matrix**, denoted as $\mathbf{I}$ or $\mathbf{I_n}$, is a square matrix with 1's on the main diagonal and all other elements equal to 0, e.g. a three-dimensional identity matrix is $$\mathbf{I}=\begin{bmatrix}
  1 & 0 & 0  \\
  0 & 1 & 0  \\
  0 & 0 & 1
\end{bmatrix}$$
- A square matrix is said to be **symmetric** if $x_{ij} = x_{ji}$ e.g. 
$$\mathbf{A}=\begin{bmatrix}
  1 & 4 & 2  \\
  4 & 1 & 0  \\
  2 & 0 & 1
\end{bmatrix}$$
- A **diagonal matrix** is a square matrix whose non-diagonal entries are all zero, that is $x_{ij} = 0$ for $i \neq j$, e.g. 
$$\mathbf{A}=\begin{bmatrix}
  1 & 0 & 0  \\
  0 & 2 & 0  \\
  0 & 0 & 3
\end{bmatrix}$$
- An **upper-triangular matrix** is a square matrix in which all entries below the diagonal are 0, that is $x_{ij}=0$ for $i>j$ e.g. 
$$\mathbf{A}=\begin{bmatrix}
  1 & 3 & 4  \\
  0 & 2 & 5  \\
  0 & 0 & 3
\end{bmatrix}$$
- A **lower-triangular matrix** is a square matrix in which all entries above the diagonal are 0, that is hat is $x_{ij}=0$ for $i<j$ e.g. 
$$\mathbf{A}=\begin{bmatrix}
  1 & 0 & 0  \\
  1 & 1 & 0  \\
  1 & 1 & 1
\end{bmatrix}$$

## Matrix operations
- matrix $\mathbf{A} = \mathbf{B}$ if both matrices have exactly the same dimension and if each element of $\mathbf{A}$ equals to the corresponding element of e.g. $\mathbf{A} = \mathbf{B}$ if
$\mathbf{A}=\begin{bmatrix}
  1 & 3 & 4  \\
  0 & 2 & 5  \\
  0 & 0 & 3
\end{bmatrix}$ and $\mathbf{B}=\begin{bmatrix}
  1 & 3 & 4  \\
  0 & 2 & 5  \\
  0 & 0 & 3
\end{bmatrix}$

<br />

- for any matrix $\mathbf{A}$ the **transpose**, denoted by $\mathbf{A}^\top$ or $\mathbf{A}^\prime$, is obtained by interchanging rows and columns, e.g. given matrix $\mathbf{A}=\begin{bmatrix}
  1 & 3 & 4  \\
  0 & 2 & 5  \\
  0 & 0 & 3
\end{bmatrix}$ we have $\mathbf{A}^\top=\begin{bmatrix}
  1 & 0 & 0  \\
  3 & 2 & 0  \\
  4 & 5 & 3
\end{bmatrix}$. The transpose of a transpose of a matrix yield the original matrix, $\Big(\mathbf{A}^\top\Big)^\top = \mathbf{A}$

<br />

- we can **add** two matrices if they have the same dimension, e.g. 
$$\mathbf{A} + \mathbf{B} = \mathbf{A} =\begin{bmatrix}
  x_{11} & x_{12}   \\
  x_{21} & x_{22} 
\end{bmatrix} + \begin{bmatrix}
  y_{11} & y_{12}   \\
  y_{21} & y_{22} 
\end{bmatrix} = \begin{bmatrix}
  x_{11}+y_{11} & x_{12}+y_{12}   \\
  x_{21}+y_{21} & x_{22}+y_{22} 
\end{bmatrix}$$

<br />

- we can **multiply** a matrix by **a scalar** $\delta$ e.g. $$\delta \cdot \mathbf{A} = \begin{bmatrix}
  \delta \cdot x_{11} & \delta \cdot x_{12}   \\
  \delta \cdot x_{21} & \delta \cdot x_{22} 
\end{bmatrix}$$

<br />

- we can **multiply two matrices** if they are **conformable**, i.e. first matrix has the same number of columns as the number of rows in the second matrix. We then can write: 
$$\mathbf{C} = \mathbf{A} \cdot \mathbf{B}  = \begin{bmatrix}
  x_{11} & x_{12} & x_{13}  \\
  x_{21} & x_{22} & x_{23}
\end{bmatrix} \times \begin{bmatrix}
  y_{11} & y_{12}   \\
  y_{21} & y_{22}  \\
  y_{31} & y_{32}
\end{bmatrix} = \\\ 
\begin{bmatrix}
  x_{11} \cdot y_{11} + x_{12} \cdot y_{21} + x_{13} \cdot y_{31}  & x_{11} \cdot y_{12} + x_{12} \cdot y_{22} + x_{13} \cdot y_{32}  \\
  x_{21} \cdot y_{11} + x_{22} \cdot y_{21} + x_{23} \cdot y_{31} & x_{21} \cdot y_{12} + x_{22} \cdot y_{22} + x_{23} \cdot y_{32}
\end{bmatrix}$$

## Inverse of a matrix 
For a square matrix $\mathbf{A}$ there may exist a matrix $\mathbf{B}$ such that $\mathbf{A} \cdot \mathbf{B} = \mathbf{I}$. An **inverse**, if it exists, is denoted as $\mathbf{A}^{-1}$ and we can rewrite the definition as $$\mathbf{A} \cdot \mathbf{A}^{-1} = \mathbf{I}$$ where $\mathbf{I}$ is an identity matrix (equivalent to 1). There is no division for matrices, instead we can use inverse to multiply the matrix by an inverse, similar to when instead of dividing the number $a$ by $b$ we multiply $a$ by reciprocal of $b = \frac{1}{b}$

For a $2 \times 2$ matrix we can follow the below formula for obtaining the inverse 
$$\begin{bmatrix}
  x_{11} & x_{12}   \\
  x_{21} & x_{22} 
\end{bmatrix}^{-1} = \frac{1}{x_{11} \cdot x_{22} - x_{12} \cdot x_{21}} \cdot \begin{bmatrix}
  x_{22} & -x_{12}   \\
  -x_{21} & x_{11} 
\end{bmatrix}$$

<br />

## Orthogonal matrix
- A matrix $\mathbf{A}$ for which $\mathbf{A^\top} = \mathbf{A^{-1}}$ is true is said to be **orthogonal**

## Exercises {-}

::: {#exr-m-matrix}

Given matrices 

 $\mathbf{A} = \begin{bmatrix}
  1 & 2   \\
  3 & 4
  \end{bmatrix}$, 
  $\mathbf{B} = \begin{bmatrix}
  1 & 0   \\
  0 & 1
  \end{bmatrix}$ and $\mathbf{C} = \begin{bmatrix}
  1 & 0   \\
  0 & 2
  \end{bmatrix}$


a) what is the dimension of matrix $\mathbf{A}$?
b) what is $\mathbf{A}^\top$?
c) which of the matrices is i) a square matrix, ii) an identity matrix,  iii) null matrix, iv) diagonal matrix, v) a triangular matrix,? 
d) calculate $\mathbf{A} + \mathbf{B}$?
e) calculate $\mathbf{A} \cdot \mathbf{C}$?
f) calculate $\mathbf{B}^\top$
g) calculate $\mathbf{A}^{-1}$
<!-- h) calculate $(\mathbf{A} + \mathbf{B})^{-1}$ -->
<!-- i) answer again a) - h) this time using R functions and/or commands -->

:::

::: {#exr-matrix-R}

Repeat above calculations using R. 

:::


## Answers{-}

::: {.solution}
@exr-m-matrix
:::

a) $2 \times 2$
b) $\mathbf{A}^\top = \begin{bmatrix}
  1 & 3   \\
  2 & 4
  \end{bmatrix}$
c) i) a square matrix: $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$, ii) identity matrix: $\mathbf{B}$, iii) null matrix: none, iv) diagonal matrix: $\mathbf{B}$ (identity matrix is diagonal) and $\mathbf{C}$, v) triangular  $\mathbf{B}$ and  $\mathbf{C}$ as both identity matrix $\mathbf{B}$ and diagonal matrix $\mathbf{C}$ is triangular, both lower and upper triangular
d) $\mathbf{A} + \mathbf{B} = \begin{bmatrix}
  1 & 2   \\
  3 & 4
  \end{bmatrix} + \begin{bmatrix}
  1 & 0   \\
  0 & 1
  \end{bmatrix} = \begin{bmatrix}
  2 & 2   \\
  3 & 5
  \end{bmatrix}$
e) $\mathbf{A} \cdot \mathbf{C} = \begin{bmatrix}
  1 \cdot 1 + 2 \cdot 0 & 1 \cdot 0 + 2 \cdot 2   \\
  3 \cdot 1 + 4 \cdot 0 & 3 \cdot 0 + 4 \cdot 2
  \end{bmatrix} = \begin{bmatrix}
  1 & 4   \\
  3 & 8
  \end{bmatrix}$
f) $\mathbf{B}^\top = \begin{bmatrix}
  1 & 0   \\
  0 & 1
  \end{bmatrix}$
  
g) 

$$\mathbf{A}^{-1} = \begin{bmatrix}
  1 & 2   \\
  3 & 4 
\end{bmatrix}^{-1} = \frac{1}{1 \cdot 4 - 2 \cdot 3} \cdot \begin{bmatrix}
  4 & -2   \\
  -3 & 1
\end{bmatrix} = -\frac{1}{2} \cdot \begin{bmatrix}
  4 & -2   \\
  -3 & 1
\end{bmatrix} = \begin{bmatrix}
  -2 & 1   \\
  \frac{3}{2} & -\frac{1}{2}
\end{bmatrix}$$

<!-- h) calculate $(\mathbf{A} + \mathbf{B})^{-1}$ -->

::: {.solution}
@exr-matrix-R
:::

```{r}
#| echo: true
#| code-fold: false

# define matrix A, B and C
A <- matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE)
B <- matrix(c(1, 0, 0, 1), nrow = 2, byrow = TRUE)
C <- matrix(c(1, 0, 0, 2), nrow = 2, byrow = TRUE)

# a)
dim(A)

# b)
t(A)

# c) square matrix
# to check if matrix is square we can compare number of rows and columns
func_is_square <- function(x = A){
  # function check if number of rows is equal to number of columns
  # return TRUE is square, FALSE otherwise
  is_square = TRUE
  if (nrow(x) != ncol(x)){
    is_square = FALSE
  }
  
  return(is_square)
}

func_is_square(A); func_is_square(B); func_is_square(C)

# c) identity matrix
# 1. we need to check if matrix is square
# 2. we need to check if there are only 1s on diagonal
# 3. we need to check that non-diagonal elements are zero
# we can write loop through elements of matrix and check 
# - if elements on diagonal are 1
# - and if elements on non-diagonal are 0

func_is_identity <- function(x){

  is_identity <-  TRUE
  
  if (func_is_square(x) == TRUE){ # first check if square
    
    for(i in 1:nrow(x)){
      for (j in 1:ncol(x)){
        
        if (i == j){ # check diagonal
          if (x[i,j] != 1){
            is_identity <- FALSE
          }
        }else{ # check non-diagonal
          if (x[i,j] != 0){
            is_identity <- FALSE
          }
        }
      }
    }
    
  }else{
    is_identity <- FALSE
  }
  
  return(is_identity)
}
func_is_identity(A); func_is_identity(B); func_is_identity(C)

# c) null matrix
# we need to check if all elements are equal to zero 
func_is_null <- function(x){
  
  is_null <- TRUE
  
  # see if any element is non-zero
  if (any(x != 0)){ 
    is_null <- FALSE}
  
  return(is_null)  
}

func_is_null(A); func_is_null(B); func_is_null(C)

# c) diagonal matrix
# check if it is square
# check if elements outside main diagonal are zero
func_is_diag <- function(x){

  is_diag <-  TRUE
  
  if (func_is_square(x) == TRUE){ # first check if square
    
    for(i in 1:nrow(x)){
      for (j in 1:ncol(x)){
        
        if (i != j){ # check outside main diagonal
          if (x[i,j] != 0){
            is_diag <- FALSE
          }
        }
      }
    }  
      
  }else{
    is_diag <- FALSE
  }
  
  return(is_diag)
}

func_is_diag(A); func_is_diag(B); func_is_diag(C)

# c) triangular
# upper triangular: all entries below main diagonal are zero
# lower triangular: all entries above main diagnoal are zero
func_is_upper_tri <- function(x){
  
  is_upper <-  TRUE
  
  if (func_is_square(x) == TRUE){ # first check if square
    
    for(i in 1:nrow(x)){
      for (j in 1:ncol(x)){
        
        if (i > j){ # check below the main diagonal
          if (x[i,j] != 0){
            is_upper <- FALSE
          }
        }
      }
    }  
      
  }else{
    is_upper <- FALSE
  }
  return(is_upper)
}

func_is_lower_tri <- function(x){
  
  is_lower <-  TRUE
  
  if (func_is_square(x) == TRUE){ # first check if square
    
    for(i in 1:nrow(x)){
      for (j in 1:ncol(x)){
        
        if (i < j){ # check below the main diagonal
          if (x[i,j] != 0){
            is_lower <- FALSE
          }
        }
      }
    }  
      
  }else{
    is_lower <- FALSE
  }
  
  return(is_lower)
}

func_is_upper_tri(A); func_is_upper_tri(B); func_is_upper_tri(C)
func_is_lower_tri(A); func_is_lower_tri(B); func_is_lower_tri(C)

# d) 
A + B

# e)
A %*% C

# f)
t(B)

# g)
solve(A)
```




