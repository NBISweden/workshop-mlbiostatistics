---
title: "Probability Theory"
date: "2022-09-12"
output: bookdown::html_document2
---

# (PART) Probability Theory {-}

# Learning outcomes {-}

- understand the concept of random variables
- understand the concept of probability
- understand and learn to use resampling to compute probabilities
- understand the concept probability mass function
- understand the concept probability density function
- understand the concept cumulative distribution functions
- use normal distribution
- understand the central limit theorem

```{r setup, include=FALSE}
require(ggplot2)
require(reshape2)
require(knitr)
require(kableExtra)
require(tidyverse)

knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, error=FALSE, warnings=FALSE, dpi=600, fig.path = "Rfigures/prob_")
options(digits=2)
##Allow duplicate chunk labels in child documents
options(knitr.duplicate.label = "allow")
```

```{css, echo=FALSE}
.toggle {
  height: 1.55em;
  overflow-y: hidden;
}
.toggle.open {
  height: auto;
}
```


# Introduction to probability {#prob-01intro}

Some things are more likely to occur than others. Compare:

- the chance of the sun rising tomorrow with the chance that no-one is infected with COVID-19 tomorrow
- the chance of a dark winter in Stockholm with the chance of no rainy days over the summer months in Stockholm

We intuitively believe that the chance of sun rising or dark winter occurring are enormously higher than COVID-19 disappearing over night or having no rain over the entire summer. **Probability** gives us a scale for measuring the likeliness of events to occur. **Probability rules** enable us to reason about uncertain events. The probability rules are expressed in terms of [sets](https://en.wikipedia.org/wiki/Set_(mathematics)), a well-defined collection of distinct objects.

Suppose we perform an experiment that we do not know the outcome of, i.e. we are uncertain about the outcome. We can however list all the outcomes that might occur.

- **sample space** is the set $S$ of these possible outcomes of the experiment, e.g. getting 1, 2 etc. on the 6-sided dice $S=\{1,2,3,4,5,6\}$
- **an event** is a subset of the sample space
- an event is said to **occur** if the outcome of the experiment belong to this set
- The **complement**, $E'$, of the event $E$ contains all the outcomes in $S$ which are not in $E$
- Two sets $E$ and $F$, such that $E \cap F = \emptyset$, are said to be **disjoint**


## Axioms of probability
1. $0 \leq P(E) \leq 1$ for any event $E \subseteq S$
2. $P(S) = 1$
3. if $E$, $F$ are disjoint events, then $P(E \cup F) = P(E) + P(F)$

## Common rules of probability

Based on the axioms the following rules of probability can be proved.

- **Complement rule**: let $E \subseteq S$ be any event, then $P(E') = 1 - P(E)$ 
- **Impossible event**: $P(\emptyset)=0$
- **Probability of a subset**: let $E,F \subseteq S$ be events such that $E \subseteq F$ then $P(F) \geq P(E)$
- **Addition rule**: let $E,F \subseteq S$ be any two events, then $P(E \cup F) = P(E) + P(F) - P(E \cap F)$

## Conditional probability
Let $E,F \subseteq S$ be two events that $P(E)>0$ then the conditional probability of $F$ given that $E$ occurs is defined to be: $$P(F|E) = \frac{P(E\cap F)}{P(E)}$$

**Product rule** follows conditional probability: let $E,F \subseteq S$ be events such that $P(E)>0$ then: $$P(E \cap F) = P(F|E)P(E) = P(E|F)P(F)$$

## The urn model

The urn model is a simple model commonly used in statistics and probability. In the urn model, real objects (such as people, mice, cells, genes, molecules, etc) are represented by balls of different colors or labels. A fair coin can be represented by an urn with two balls representing the coin's two sides; H and T. A group of people can be modelled in an urn model, if age is the variable of interest, we write the age of each person on the balls. If we instead are interested in if the people are allergic to pollen or not, we color the balls according to allergy status.

```{r urns, echo=FALSE, fig.cap="Urn models of a fair coin, age of a group of people, pollen allergy status of a group of people.", out.width = '20%', fig.align="center", fig.show="hold"}
knitr::include_graphics("figures/coinurn.png")
knitr::include_graphics("figures/ageurn.png")
knitr::include_graphics("figures/pollenageurn.png")
```

In the urn model every unit (ball) is equally likely of being selected. This means that the urn model is well suited to represent flipping a fair coin. However, a biased coin can also be modelled using an urn model, by changing the number of balls that represent each side of the coin.

By drawing balls from the urn with (or without) replacement, probabilities and other properties of the model can be inferred.


## Random variables

The outcome of a random experiment can be described by a **random variable**. Whenever chance is involved in the outcome of an experiment the outcome is a random variable.

A random variable can not be predicted exactly, but the probability of all possible outcomes can be described. The **sample space** is the set of all possible outcomes of a random variable. Note, the sample space is not always countable.

A random variable is usually denoted by a capital letter, $X, Y, Z, \dots$. Values collected in an experiment are **observations** of the random variable, usually denoted by lowercase letters $x, y, z, \dots$.

The **sample space** is the collection of all possible observation values.

The **population** is the collection of all possible observations.

A **sample** is a subset of the population.

Example random variables and probabilites:

- The weight of a random newborn baby, $W$. $P(W>4.0kg)$
- The smoking status of a random mother, $S$. $P(S=1)$
- The hemoglobin concentration in blood, $Hb$. $P(Hb<125 g/L)$
- The number of mutations in a gene
- BMI of a random man
- Weight status of a random man (underweight, normal weight, overweight, obese)
- The result of throwing a die

Conditional probability can be written for example $P(W \geq 3.5 | S = 1)$, which is the probability that $X \geq 3.5$ if $S = 1$, in words "the probability that a smoking mother has a baby with birth weight of 3.5 kg or more".

Random variables are divided based on their data type, categorical (nominal or ordinal) or numeric (discrete or continuous).

# Discrete random variables {#prob-02discrv}

A categorical random variable has nominal or ordinal outcomes such as; {red, blue, green} or {tiny, small, average, large, huge}.

A discrete random number is usually counts and has a countable number of outcome values, such as {1,2,3,4,5,6}; {0,2,4,6,8} or all integers.

A discrete or categorical random variable can be described by its **probability mass function (PMF)**.

The probability that the random variable, $X$, takes the value $x$ is denoted $P(X=x) = p(x)$. Note that:
  
1. $0 \leq p(x) \leq 1$, a probability is always between 0 and 1.
2. $\sum p(x) = 1$, the sum over all possible outcomes is 1.

```{example, label="rolldie", echo=TRUE}
**The number of dots on a die**
  
When rolling a die the there are six possible outcomes; 1, 2, 3, 4, 5 and 6, each of which have the same probability, if the die is fair. The outcome of one dice roll can be described by a random variable $X$. The probability of a particular outcome $x$ is denoted $P(X=x)$ or $p(x)$. 
```

The probability mass function of a fair six-sided die can be summarized in a table;

```{r pmfdie}
kable(matrix(c(1:6,rep(1/6,6)),ncol=6, byrow=TRUE, dimnames=list(c('x','p(x)'), c()))) %>% kable_styling(full_width = FALSE)
```

or in a barplot;

```{r die, fig.height=3, fig.width=7, fig.cap="Probability mass function of a die.", out.width="45%", fig.align='center'}
plot(data.frame(x=1:6, p=1/6) %>% ggplot(aes(x=x, y=p)) + geom_bar(stat="identity") + theme_bw() + ylim(c(0,.25)) + scale_x_continuous(breaks = 1:6))
``` 

```{example, label="nuclsite", echo=TRUE}
**Nucleotide in a given site**
  
The nucleotide at a given genomic site can be one of the four nucleotides; {A, C, T, G}. Unlike a die the four nulceotides are ususlly not equally likely. Thenucleotide in the site can be described by a random variable, $X$. The probability mass function can be summarized in a table or a bar plot.
```

```{r nucl, fig.height=3, fig.width=7, fig.cap="Probability mass function of a nucleotide site.", out.width="45%", fig.align='center'}
p <- c(A=0.4, C=0.2, T=0.1, G=0.3)
kable(matrix(c(names(p), p), ncol=4, byrow=TRUE, dimnames=list(c('x','p(x)'), c()))) %>% kable_styling(full_width = FALSE)
data.frame(p) %>% rownames_to_column("x") %>% ggplot(aes(x=x, y=p)) + geom_bar(stat="identity") + theme_bw()
``` 

```{example, label="smoke"}
**The smoking status of a random mother**

The random variable has two possible outcomes; non-smoker (0) and smoker (1). The probability of a random mother being a smoker is 0.39.
```

```{r pmfsmoker}
kable(matrix(c("0","0.61","1","0.39"),ncol=2, dimnames=list(c('x','p(x)'), c('non-smoker','smoker')))) %>% kable_styling(full_width = FALSE)
```

```{example, label="bacteria", echo=TRUE}
**CFU**

The number of bacterial colonies on a plate is a random number.
```

```{r CFU, fig.cap="Probability mass distribution of the number of bacterial colonies on an agar plate.", fig.height=3, fig.width=7}
x=1:50
ggplot(data.frame(x=x, fx=dpois(x, lambda=25)), aes(x,fx)) + geom_bar(stat="identity") + theme_bw() + ylab("p(x)")
```

The probability mass function can be used to compute the probability of an event, such as the events 

* *get an even number when rolling a die* - sum up the probabiliy for 2, 4, and 6, p(even)=p(2)+p(4)+p(6)=0.5
* *less than 15 bacterial colonies on the agar plate* - sum upp the probabilities $P(X<15) = \sum_{k=0}^{14} p(k) = $ `r ppois(14, 25)`. 

The **cumulative distribution function (CDF)** is defined for numeric discrete random variables as $F(x) = P(X<=x)$.
<!-- For common distributions there are functions in R for computing CDFs (we will get back to this). -->

## Expected value and variance

For a numeric discrete random variable, the **expected value** of the random variable can be computed, when the probability mass function is know. The expected value is the mean of a random variable and is denoted $E[X]$ or $\mu$ and can be computed by summing up all possible outcome values weighted by their probability;

$$E[X] = \mu = \sum_{i=1}^N x_i p(x_i)$$
For a **uniform distribution**, where every outcome has the same probability (in the urn model, every outcome is represented by one ball), the expected value can be computed as the sum of all outcome values divided by the total number of outcome values;

$$E[X] = \mu = \frac{1}{N}\sum_{i=1}^N x_i$$
### Linear transformations and combinations

  $$E(aX) = a E(X)$$
  
  $$E(X + Y) = E(X) + E(Y)$$
  
  $$E[aX + bY] = aE[X] + bE[Y],$$
where $a$ and $b$ are constants.

The **variance** is a measure of spread and the variance of a random variable is defined as the expected value of the squared difference of the random variable and its mean. The 

$$var(X) = \sigma^2 = E[(X-\mu)^2] = \sum_{i=1}^n (x_i-\mu)^2 p(x_i)$$
### Linear transformations and combinations
  
  $$var(aX) = a^2 var(X)$$
      
For independent random variables X and Y
    
  $$var(aX + bY) = a^2var(X) + b^2var(Y)$$



## Simulate distributions

Once the distribution is known, we can compute probabilities, such as $P(X=x), P(X<x)$ and $P(X \geq x)$. If the distribution is not known, simulation might be the solution.

```{example, label="cointoss", echo=TRUE}
**Simulate coin toss**

In a single coin toss the probabity of heads is 0.5. In 20 coin tosses, what is the probability of at least 15 heads?
```

<!-- Biological example. Randomize patients into control and treatment group, 50 % In a bacterial sample, every single bacteria can either be antibiotic resistant or not. In a sample 50% of the bacteria are antibiotic resistant, grow them on an agar plat and pick 20 colonies  -->


The outcome of a single coin toss is a random variable, $X$ with two possible outcomes $\{H, T\}$. We know that $P(X=H) = 0.5$. The random variable of interest is the number of heads in 20 coin tosses, $Y$. The probability that we need to compute is $P(Y \geq 15)$.

```{r coinurn, echo=FALSE, fig.cap="A coin toss. Urn model with one black ball (heads) and one white ball (tails).", out.width = '20%', fig.align="lecenter"}
knitr::include_graphics("figures/coinurn.png")
```

A single coin toss can be modelled by an urn with two balls. When a ball is drawn randomly from the urn, the probability to get the black ball (heads) is $P(X=H) = 0.5$.

In R we can simulate random draws from an urn model using the function `sample`.

```{r coin, echo=TRUE}
## A single coin toss
sample(c("H", "T"), size=1)
## Another coin toss
sample(c("H", "T"), size=1)
```

Every time you run `sample` a new coin toss is simulated. 

If we want to simulate tossing 20 coins (or one coin 20 times) we can use the same urn model, if the ball is replaced after each draw.

The argument `size` tells the function how many balls we want to draw from the urn. To draw 20 balls from the urn, set `size=20,` remember to replace the ball after each draw!

```{r coins, echo=TRUE}
## 20 independent coin tosses
(coins <- sample(c("H", "T"), size=20, replace=TRUE))
```

How many heads did we get in the 20 random draws?

```{r coinsH, echo=TRUE}
## How many heads?
sum(coins == "H")
```

We can repeat this experiment (toss 20 coins and count the number of heads) several times to estimate the distribution of number of heads in 20 coin tosses.

To do the same thing several times we use the function `replicate.`

To simulate tossing 20 coins and counting the number of heads 10000 times, do the following;

```{r Nheads, echo=TRUE}
Nheads <- replicate(10000, {
  coins <- sample(c("H", "T"), size=20, replace=TRUE)
  sum(coins == "H")
})
```

Plot the distribution of the number of heads in a histogram.

```{r histNheads, out.width="70%", echo=TRUE}
hist(Nheads, breaks=0:20-0.5)
```

Now, let's get back to the question; when tossing 20 coins, what is the probability of at least 15 heads?

$P(X \geq 15)$

Count how many times out of our `r length(Nheads)` experiments the number is 15 or greater

```{r sumNheads15, echo=TRUE}
sum(Nheads >= 15)
```

From this we conclude that

$P(X \geq 15) =$ `r sum(Nheads>=15)`/`r length(Nheads)` = `r sum(Nheads>=15)/length(Nheads)`

Resampling can also be used to compute other properties of a random variable, such as the expected variable.

The **law of large numbers** states that if the same experiment is performed many times the average of the result will be close to the expected value.


The coin flip is a common example in statistics, but many situations with two outcomes can be modeled using similar models. If we consider the outcome of interest succes (like heads in the coin example) and the alternative outcome failure we can for example model;

* Drug effect: A patient can respond to  drug treatment (success) or not (failure)
* Side effect: After a treatment a patient might experience a side effect (success) or not (failure)
* Treatment or placebo: Randomly assign a study participant into treatment or placebo group
* Antibiotic resistance: A bacteria is either resistant to an antibiotic or not

The probability of success and failure can be equal, like in the coin example, but they don't have to be equal. If the probability of a patient responding to a treatment is $p=0.80$, we know that the probability of the patient not responding is $1-p=0.20$, this can be modelled using an urn model with 4 black balls (cured) and 1 white ball (not cured).

## Parametric discrete distributions

### Bernoulli trial

A Bernoulli trial is a random experiment with two outcomes; success and failure. The probability of success, $P(success) = p$, is constant. The probability of failure is $P(failure) = 1-p$.

When coding it is convenient to code success as 1 and failure as 0.

The outcome of a Bernoulli trial is a discrete random variable, $X$.

$$p(x) = \left\{
\begin{array}{ll}
p & \mathrm{if}\,x=1\mathrm,\,success\\
1-p & \mathrm{if}\,x=0\mathrm,\,failure
\end{array}
\right.$$

```{r pbernoulli, eval=FALSE}
kable(matrix(c('x','0','1','p(x)','1-p','p'), byrow=TRUE, ncol=3)) %>% kable_styling("striped", full_width = FALSE)
```

Using the definitions of expected value and variance it can be shown that;

$$E[X] = p\\
var(X) = p(1-p)$$

A Bernoulli trial can be simulated, as seen in previous section, using the function `sample`.

### Binomial distribution

The number of successes in a series of independent and identical Bernoulli trials is a **binomial** random variable, $X$.

$X = \sum_{i=0}^n Z_i,$

where all $Z_i$ describe the outcome of independent and identical Bernoulli trials with probability $p$ for *success* ($P(Z_i=1) = p$).

The probability mass function of $X$ is called the binomial distribution. In short we use the notation;

$$X \sim Bin(n, p)$$

The probability mass function is

$$P(X=k) = {n \choose k} p^k (1-p)^{n-k}$$
It can be shown that

$$E[X] = np\\
var(X) = np(1-p)$$

A binomial random variableis the number of successes when sampling $n$ objects *with* replacement from an urn with objects of two types, of which the interesting type (success) has probability $p$.

The probability mass function, $P(X=k)$ can be computed using the R function `dbinom` and the cumulative distribution function $P(X \leq k)$ can be computed using `pbinom`.

Examples fo binomial random variables;

* The number of patients responding to a treatment out of $n$ patients in a study, if the probability of a patient responding to treatment is $p$.
* The number of patients experiencing a side effect out of $n$ patients in a study, if the probability of a side effect is $p$.
* the number of mutations in a gene of length $n$, if the mutations are independent and identically distributed and the probability of a mutation at every single position is $p$.

### Hypergeometric distribution

The hypergeometric distribution occurs when sampling $n$ objects *without* replacement from an urn with $N$ objects of two types, of which the interesting type has probability $p$.

The probability mass function

$$P(X=k) = \frac{{Np\choose{k}}{{N-Np}\choose{n-k}}}{N\choose{n}}$$
can be computed in R using `dhyper` and the cumulative distribution function $P(X \leq k)$ can be computed using `phyper`.

Examples of hypergeometric random variables;

* In a student group of 25 individuals, 10 are R beginners. If 5 individuals are randomly choosen to belong to group A, the number of R beginners in group A is a hypergeometric random variable.
* A drug company is producing 1000 pills per day, 5% have an amount of active substance below a threshold. In a random sample of 10 pill, how many contain to little active substance? The number of pills with to little active substance is a hypergeometric random variable.
* You investigate the differential expression of 10000 genes, 150 of these genes blong to the superinteresting pathway XXX. You run a black-box algorithm that report 80 differentially expressed (DE) genes. If the black-box algorithm choose DE genes at random, the number of DE genes that blong to pathway XXX is a hypergeometric random variable.

### Poisson distribution

The Poisson distribution describe the number of times a rare event occurs in a large number of trials.

A rare disease has a very low probability for a single individual. The number of individuals in a large population that catch the disease in a certain time period can be modelled using the Poisson distribution.

The probability mass function;

$$P(X=k) = \frac{\mu}{k!}e^{-\mu},$$
where $\mu$ is the expected value, which is $\mu = n \pi$, where $n$ is the number of objects sampled from the population and $\pi$ is the probability of a single object.

The Poisson distribution can approximate the binomial distribution if $n$ is large ($n>10$) and $\pi$ is small ($\pi < 0.1$).


Examples of Poisson random variables;
* A rare disease has a very low probability for a single individual. The number of individuals in a large population that catch the disease in a certain time period is a Poisson random variable.
* Number of reads aligned to a gene region

### Distributions in R

Probability mass functions, $P(X=x)$, for the binomial, hypergeometric and Poisson distributions can in R can be computed using functions `dbinom`, `dhyper`, and `dpois`, respectively.

Cumulative distribution functions, $P(X \leq x)$ can be computed using `pbinom`, `phyper` and `ppois`.

Also, functions for computing an $x$ such that $P(X \leq x) = q$, where $q$ is a probability of interest are available using `qbinom`, `qhyper`, and `qpois`.

# Exercises: Discrete random variables {.unnumbered #prob-exercise1-discrv}

The exercise Rmd can be downloaded from the [probability exercise files](https://uppsala.instructure.com/courses/70535/files/folder/Exercises/probability) directory, also solutions (both Rmd and html) can be downloaded from the same directory (but with a time restriction).

```{r discreteexercises, child="prob_exr_1_discreteRV.Rmd"}

```

<!-- # Exercises with solutions: Discrete random variables {.unnumbered #prob-exercise1-solutions-discrv} -->
  
```{r discretesolutions, child="prob_exr_1_discreteRV_solutions.Rmd", eval=FALSE}

```

# Continuous random variable {#prob-contrv}

A continuous random number is not limited to discrete values, but any continuous number within one or several ranges is possible.

Examples: weight, height, speed, intensity, ...

A continuous random variable can be described by its **probability density function**, pdf.

```{r pdfnewborn, fig.show="hold", fig.cap="Probability density function of the weight of a newborn baby.", fig.height=4, fig.width=7}
## ounce <- 0.0283495231
## pound <- 0.45359237
## ggplot(babies, aes(x=wt*ounce)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() #+ ggtitle("Weight (kg) of a newborn baby"))
##plot(ggplot(fat, aes(x=weight*pound)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() + ggtitle("Weight (kg) of a man"))
##Simulate weights
baby <- data.frame(smoke=sample(1000, x=0:2, prob=c(0.45,0.45,0.1), replace=TRUE)) %>% mutate(m=c(3.4, 3.0, 2.5)[smoke+1], wt=rnorm(n(), mean=m, sd=0.5))
ggplot(baby, aes(x=wt)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() 
```

```{r normpdf2, out.width="70%", fig.align="center", eval=FALSE}
x <- seq(-4,4,.01)
ggplot(data.frame(x=x, fx=dnorm(x)), aes(x,fx)) + geom_line() + theme_bw() + ylab("f(x)")
```

  The probability density function, $f(x)$, is defined such that the total area under the curve is 1.

$$
  \int_{-\infty}^{\infty} f(x) dx = 1
$$
  
```{r wtbabiesdens3, out.width="49%", warning=FALSE, message=FALSE, fig.show="hold", fig.keep="all"}
ggplot(baby, aes(x=wt)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() 
#plot(ggplot(data.frame(w=wt), aes(x=w)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density())

df.wt <- density(baby$wt)
df.wt <- data.frame(x=df.wt$x, y=df.wt$y)
plot(ggplot(baby, aes(x=wt)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() + geom_area(data=df.wt %>% filter(x<3.75, x>2.33), aes(x=x, y=y)) + scale_x_continuous(breaks=c(2,2.33,3,3.75,4,5), labels=c('2','a','3','b','4','5')) + geom_hline(yintercept=0) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_line(color = c("grey92", NA, "grey92", NA, "grey92", "grey92"))))
#plot(ggplot(data.frame(w=wt), aes(x=w)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() + geom_area(data=df.wt %>% filter(x<3.75, x>2.33), aes(x=x, y=y)) + scale_x_continuous(breaks=c(2,2.33,3,3.75,4,5), labels=c('2','a','3','b','4','5')) + geom_hline(yintercept=0) + theme(panel.grid=element_blank()))
```

```{r pdfwtnorm, out.width="100%", eval=TRUE}
w<-seq(1.5,5.5,.01)
df.nwt <- data.frame(w=w, f=dnorm(w, 3.5, 0.5))
#ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("Weight (kg)") + ylab("f(x)")
```

```{r pdfab, eval=FALSE}
ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("Weight (kg)") + ylab("f(x)") + geom_area(data=df.nwt %>% filter(w<3.75, w>2.33)) + scale_x_continuous(breaks=c(2,2.33,3,3.75,4,5), labels=c('2','a','3','b','4','5')) + geom_hline(yintercept=0) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_line(color = c("grey92", NA, "grey92", NA, "grey92", "grey92")))
```

The area under the curve from a to b is the probability that the random variable $X$ takes a value between a and b.

$P(a \leq X \leq b) = \int_a^b f(x) dx$
  
<!-- #### Cumulative distribution function, cdf -->
  
The **cumulative distribution function**, cdf, sometimes called just the
distribution function, $F(x)$, is defined as:
  
  $$F(x) = P(X \leq x) = \int_{-\infty}^x f(x) dx$$
  
```{r wtpdfcdf, out.width="49%", fig.show="hold"}
plot(ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("x") + ylab("f(x)") + geom_area(data=df.nwt %>% filter(w<4.0)) + annotate("label",label=sprintf("P(X<4.0) = F(4.0) = %.2f", pnorm(4,3.5,0.5)), x=2.7, y=0.4, hjust=0))
df.nwt$F <- pnorm(df.nwt$w, 3.5, 0.5)
plot(ggplot(df.nwt, aes(x=w, y=F)) + geom_line() + xlab("x") + ylab("F(x)") + theme_bw() + geom_point(aes(x=4, y=pnorm(4,3.5,.5))) + annotate("label",label=sprintf("F(4.0)=%.2f", pnorm(4,3.5,.5)), x=4, y=.84, hjust=-0.2))##+ ggtitle("Cumulative distribution function") 
```

$$P(X \leq x) = F(x)$$
  
  As we know that the total probability (over all x) is 1, we can conclude that 

$$P(X > x) = 1 - F(x)$$
  and thus

$$P(a < X \leq b) = F(b) - F(a)$$
  
## Parametric continuous distributions

Two important parameters of a distribution is the expected value, $\mu$, that describe the distributions location and the variance, $\sigma^2$, that describe the spread.

The expected value, or population mean, is defined as;

$$E[X] = \mu = \int_{-\infty}^\infty x f(x) dx$$
We will learn more about the expected value and how to estimate a population mean from a sample later in the course.

The variance is defined as the expected value of the squared distance from the population mean;

$$\sigma^2 = E[(X-\mu)^2] = \int_{-\infty}^\infty (x-\mu)^2 f(x) dx$$

The square root of the variance is called the standard deviation, $\sigma$.

## Normal distribution

The normal distribution (sometimes referred to as the Gaussian distribution) is a common probability distribution and many continuous random variables can be described by the normal distribution or be approximated by the normal distribution.

The normal probability density function

$$f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}$$
  
describes the distribution of a normal random variable, $X$, with expected value $\mu$ and standard deviation $\sigma$, $e$ and $\pi$ are two common mathematical constants, $e \approx 2.71828$ and $\pi \approx 3.14159$.

In short we write $X \sim N(\mu, \sigma)$.

```{r norm, out.width="50%", fig.show="hold", fig.align="center"}
#ggplot(pop.FN, aes(x=Bodyweight)) + geom_histogram(binwidth=1, aes(y=stat(density)), color="white") + theme_bw() + geom_line(data=den.FN, aes(x=x, y=nfx), color="red")
x <- seq(-3.5, 3.5, .1)
dN <- data.frame(x=x, fx=dnorm(x))
plot(ggplot(dN, aes(x=x, y=fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("f(x)") + theme_bw())
```

The bell-shaped normal distributions is symmetric around $\mu$ and $f(x) \rightarrow 0$ as $x \rightarrow \infty$ and as $x \rightarrow -\infty$.

As $f(x)$ is well defined, values for the cumulative distribution function $F(x) = \int_{- \infty}^x f(x) dx$ can be computed.

```{r cdfnorm, out.width="45%", fig.show="hold"}
dN$Fx <- pnorm(x)
ggplot(dN, aes(x=x, y=fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("f(x)") + theme_bw() + ggtitle("Probability density function")
ggplot(dN, aes(x=x, y=Fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("F(x)") + theme_bw() + ggtitle("Cumulative distribution function")
```


If $X$ is normally distributed with expected value $\mu$ and
standard deviation $\sigma$ we write:
  
  $$X \sim N(\mu, \sigma)$$
  
  Using transformation rules we can define

$$Z = \frac{X-\mu}{\sigma}, \, Z \sim N(0,1)$$ 
  
Values for the cumulative standard normal distribution, $F(z)$, are tabulated and easy to compute in R using the function ``pnorm``.

```{r FZ, out.width="50%", fig.cap="The shaded area under hte curve is the tabulated value $P(Z \\leq z) = F(z)$."}
w<-seq(-3.5,3.5,.01)
df.nwt <- data.frame(w=w, f=dnorm(w))
plot(ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("x") + ylab("f(x)") + geom_area(data=df.nwt %>% filter(w<0.7)) + annotate("label",label=sprintf("P(Z<z) = F(z)"), x=-1.0, y=0.1, hjust=0) + scale_x_continuous(breaks=c(-2,0,+.7,2), labels=c("-2","0","z","2")) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_line(color = c("grey92", "grey92", NA, "grey92"))))
```


```{r FZtab}
z <- sapply(seq(-3.4, -3.49, -0.01), function(x) seq(x,0,0.1))
z <- -z[nrow(z):1,]
z <- sapply(seq(0, 0.09, 0.01), function(x) seq(x,3.49,0.1))
zscore.df <- apply(pnorm(z), 2, function(x) sprintf("%.4f",x))
row.names(zscore.df) <- sprintf("%.1f", z[,1])
colnames(zscore.df) <- seq(0,0.09,0.01)
kable(zscore.df, caption="Normal distribution. The table gives $F(z) = P(Z \\leq z)$ for $Z \\in N(0,1)$.") %>% kable_styling()
```

Some value of particular interest:
  
$$F(1.64) = 0.95\\
F(1.96) = 0.975$$

As the normal distribution is symmetric F(-z) = 1 - F(z) 

$$F(-1.64) = 0.05\\
F(-1.96) = 0.025$$

$$P(-1.96 < Z < 1.96) = 0.95$$

<!-- Show table? -->
  
  <!-- dnorm -->
  
  <!-- pnorm -->
  
### Sum of two normal random variables
  
  If $X \sim N(\mu_1, \sigma_1)$ and $Y \sim N(\mu_2, \sigma_2)$ are two independent normal random variables, then their sum is also a random variable:
  
  $$X + Y \sim N(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})$$
  
  and 

$$X - Y \sim N(\mu_1 - \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})$$
  This can be extended to the case with $n$ independent and identically distributed random varibles $X_i$ ($i=1 \dots n$). If all $X_i$ are normally distributed with mean $\mu$ and standard deviation $\sigma$, $X_i \in N(\mu, \sigma)$, then the sum of all $n$ random variables will also be normally distributed with mean $n\mu$ and standard deviation $\sqrt{n} \sigma$.


## Central limit theorem

```{theorem, label="CLT", echo=TRUE}
The sum of $n$ independent and equally distributed random variables
is normally distributed, if $n$ is large enough.
```


As a result of central limit theorem, the distribution of fractions or mean values of a sample follow the normal distribution, at least if the sample is large enough (a rule of thumb is that the sample size $n>30$).


<!-- ```{example, "Mean BMI", eval=FALSE} -->
  <!-- Percentage of body fat, age, weight, height, BMI and ten body circumference -->
  <!-- measurements are recorded for 252 men. Consider these 252 as a population and compute the population mean ans standard deviation. -->
  <!-- ``` -->
  
```{example, label="BMdistr", echo=TRUE}
**Mean BMI**

In a population of 252 men we can study the distribution of BMI.
```

```{r fatdata}
fat <- read.table("http://jse.amstat.org/datasets/fat.dat.txt")
colnames(fat) <- c("case","body.fat","body.fat.siri","density","age","weight","height","BMI","ffweight","neck","chest","abdomen","hip","thigh","knee","ankle" ,"bicep","forearm","wrist" )
```

```{r BMIhist, out.width="60%"}
pl <- ggplot(fat, aes(x=BMI)) + geom_histogram(aes(y=stat(density)), binwidth=2, color="white") + theme_bw()
plot(pl)
```


```{r BMI, echo=TRUE}
##Population mean
(mu <- mean(fat$BMI))
##Population variance
(sigma2 <- var(fat$BMI)/nrow(fat)*(nrow(fat)-1))
##Population standard variance
(sigma <- sqrt(sigma2))
```

Randomly sample 3, 5, 10, 15, 20, 30 men and compute the mean value, $m$. Repeat many times to get the distribution of mean values.

```{r owexample, out.width="70%", fig.width=7, fig.show="hold"}
#hist(fat$BMI)
bmi <- fat$BMI
n <- c(3,5,10,15,20, 30)
rs <- sapply(n, function(k) replicate(10000, mean(sample(bmi, k))))
colnames(rs) <- paste(sprintf("n=%i, m=%.4f", n, colMeans(rs)))
plot(ggplot(melt(rs, varnames=c("rep", "n")), aes(x=value, color=factor(n))) + geom_density() + theme_bw() + facet_wrap(~n) + theme(legend.position="none"))
```

Note, mean is just the sum divided by the number of samples $n$.

## $\chi^2$-distribution

The random variable $Y = \sum_{i=1}^n X_i^2$ is $\chi^2$ distributed with $n-1$ degrees of freedom, if $X_i$ are independent identically distributed random variables $X_i \in N(0,1)$.

In short $Y \in \chi^2(n-1)$.

```{r Xdistr, fig.width=7, fig.height=7, fig.cap="The $\\chi^2$-distribution."}
x <- seq(-0,5, .01)
dX <- data.frame(x=x, n=rep(c(2, 3,4,5,7, 10), each=length(x))) %>%
  mutate(fx=dchisq(x, df=n-1))
#dT <- data.frame(x=x, fx=dt(x, df=n-1))
ggplot(dX, aes(x=x, y=fx, color=factor(paste0("n=",n), levels=paste0("n=",sort(unique(n)))))) + geom_line() + theme_bw() + scale_color_discrete("") + ylim(c(0,0.5))
```

```{example, label="chisq", echo=FALSE}
The sample variance $S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2$ is such that $\frac{(n-1)S^2}{\sigma^2}$ is $\chi^2$ distributed with $n-1$ degrees of freedom.
```

<!-- Example. $\chi^2$-test for variance -->
  
## F-distribution
  
The ratio of two $\chi^2$-distributed variables divided by their degrees of freedom is F-distributed

```{r Fdistr, fig.width=7, fig.height=7, fig.cap="The F-distribution"}
x <- seq(-0,5, .01)
dX <- data.frame(x=x, n1=rep(c(2,4,10), each=length(x)), n2=rep(c(2,4,10), each=length(x)*3)) %>%
  mutate(fx=df(x, df1=n1-1, df2=n2-1))
#dT <- data.frame(x=x, fx=dt(x, df=n-1))
ggplot(dX, aes(x=x, y=fx, color=factor(sprintf("n1=%i, n2=%i",n1, n2), levels=sprintf("n1=%i, n2=%i",rep(sort(unique(n1)), each=3), sort(unique(n2)))))) + geom_line() + theme_bw() + scale_color_discrete("") + ylim(c(0,0.8))
```

```{example, label="Fdist", echo=TRUE}
The ratio of two sample variances is F-distributed
```
<!-- Example. F-test of equality of variances -->
  
## t-distribution
  
The ratio of a normally distributed variable and a $\chi^2$-distributed variable is t-distributed.

```{r exampletdistr, fig.width=7, fig.height=7, fig.cap="The t-distribution."}
x <- seq(-3.5,3.5, .01)
dT <- data.frame(x=x, n=rep(c(2, 3,5,7, 10, 15, 20, 30), each=length(x))) %>%
  mutate(fx=dt(x, df=n-1))
#dT <- data.frame(x=x, fx=dt(x, df=n-1))
ggplot(dT, aes(x=x, y=fx, color=factor(paste0("n=",n), levels=paste0("n=",sort(unique(n)))))) + geom_line() + theme_bw() + scale_color_discrete("")
```


```{example, label="tdistr", echo=TRUE}
**t-distribution**

The ratio between sample mean and sample variance is t-distributed.
```

## Distributions in R

Probability density functions for the normal, t, $\chi^2$ and F distributions can in R can be computed using functions `dnorm`, `dt`, `dchisq`, and `df`, respectively.

Cumulative distribution functions can be computed using `pnorm`, `pt`, `pchisq` and `pf`.

Also, functions for computing an $x$ such that $P(X<x) = q$, where $q$ is a probability of interest are available using `qnorm`, `qt`, `qchisq` and `qf`.

# Exercises: Continuous random variables {.unnumbered #prob-exercise2-contrv}

The exercise Rmd can be downloaded from the [probability exercise files](https://uppsala.instructure.com/courses/70535/files/folder/Exercises/probability) directory, also solutions (both Rmd and html) can be downloaded from the same directory (but with a time restriction).

```{r contexercises, child="prob_exr_2_contRV.Rmd"}

```

<!-- # Exercises with solutions: Continuous random variables {.unnumbered #prob-exercise2-solutions-contrv} -->
  
```{r contsolutions, child="prob_exr_2_contRV_solutions.Rmd", eval=FALSE}

```

# Random sample {#prob-sample}

In many (most) experiments it is not feasible (or even possible) to examine the entire population. Instead we study a random sample.

A random sample is a random subset of individuals from a population.
<!-- In random sampling from a population, where the variable $X$ has a known distribution with mean $\mu$ and standard deviation $\sigma$ -->
  
A simple random sample is a random subset of individuals from a population, where every individual has the same probability of being choosen.

<!-- **Notation** -->

<!-- A random sample $x_1,x_2,\dots,x_n$ from a distribution, $D$, consists of $n$ observations of the independent random variables $X_1, X_2,\dots,X_n$ all with the distribution $D$. -->

## The urn model to perform simple random sampling

Let every individual in the population be represented by a ball. The value on each ball is the measurement we are interested in, for example height, shoe size, hair color, healthy/sick, type of cancer/no cancer, blood glucose value, etc.

Draw $n$ balls from the urn, without replacement, to get a random sample of size $n$.


    

## Sample properties

Summary statistics can be computed for a sample, such as the sum, proportion, mean and variance.

**Notation:**
A random sample $x_1,x_2,\dots,x_n$ from a distribution, $D$, consists of $n$ observations of the independent random variables $X_1, X_2,\dots,X_n$ all with the distribution $D$.

### Sample proportion

The proportion of a population with a particular property is $\pi$.

The number of individuals with the property in a simple random sample of size $n$ is a random variable $X$. The proportion of individuals in a sample with the property is also a random variable;

$$P = \frac{X}{n}$$
with expected value 
$$E[P] = \frac{E[X]}{n} = \frac{n\pi}{n} = \pi$$

## Sample mean and standard deviation

For a particular sample of size $n$; $x_1, \dots, x_n$, the sample mean is denoted $m = \bar x$. The sample mean is calculated as;
  
$$m = \bar x = \frac{1}{n}\displaystyle\sum_{i=1}^n x_i$$
and the sample variance as;

$$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i-m)^2$$

Note that the mean of $n$ independent identically distributed random variables, $X_i$ is itself a random variable;
  
$$\bar X = \frac{1}{n}\sum_{i=1}^n X_i,$$
If $X_i \sim N(\mu, \sigma)$ then $\bar X \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$.

When we only have a sample of size $n$, the sample mean $m$ is our best estimate of the population mean. It is possible to show that the sample mean is an unbiased estimate of the population mean, i.e. the average (over many size $n$ samples) of the sample mean is $\mu$.

$$E[\bar X] = \frac{1}{n} n E[X] = E[X] = \mu$$

Similarly, the sample variance is an unbiased estimate of the population variance.

## Standard error

Eventhough the sample can be used to calculate unbiased estimates of the population value, the sample estimate will not be perfect. The standard deviation of the sampling distribution (the distribution of sample estimates) is called the **standard error**.

For the sample mean, $\bar X$, the variance is

$$E[(\bar X - \mu)] = var(\bar X) = var(\frac{1}{n}\sum_i X_i) = \frac{1}{n^2} \sum_i var(X_i) = \frac{1}{n^2} n var(X) = \frac{\sigma^2}{n}$$
The standard error of the mean is thus;

$$SEM = \frac{\sigma}{\sqrt{n}}$$
Replacing $\sigma$ with the sample standard deviation, $s$, we get an estimate of the standard error of the mean;

$$SEM \approx \frac{s}{\sqrt{n}}$$

  
# Exercises: Random sample {.unnumbered #prob-exercise3-sample}

The exercise Rmd can be downloaded from the [probability exercise files](https://uppsala.instructure.com/courses/70535/files/folder/Exercises/probability) directory, also solutions (both Rmd and html) can be downloaded from the same directory (but with a time restriction).

```{r sampleexercises, child="prob_exr_3_randomsample.Rmd"}

```

<!-- # Exercises with solutions: Random sample {.unnumbered #prob-exercise3-solutions-sample} -->
  
```{r samplesolutions, child="prob_exr_3_randomsample_solutions.Rmd", eval=FALSE}

```
