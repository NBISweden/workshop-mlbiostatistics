{
  "hash": "39a81affce0d8e2a2d04e8a6df9592bd",
  "result": {
    "markdown": "---\ntitle: \"Introduction to linear models\"\n# author: Olga Dethlefsen\nformat: \n  revealjs:\n    slide-number: true\n    theme: [default, custom.scss]\n    chalkboard: \n      buttons: true\n  html:\n    code-fold: false\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n\n\n# We will learn\n\n*before lunch*\n\n-   what are linear models\n-   how to estimate model parameters\n-   hypothesis testing on linear regression\n-   evaluating how well model fits the data\n-   model assumptions\n\n*after lunch*\n\n-   linear model selection and regularization\n-   GLM with logistic regression\n\n## Why linear models?\n\n. . .\n\n::: columns\n::: {.column width=\"50%\"}\nWith linear models we can answer questions such as:\n\n::: incremental\n-   is there a relationship between exposure and outcome, e.g. height and weight?\n-   how strong is the relationship between the two variables?\n-   what will be a predicted value of the outcome given a new set of exposure values?\n-   which variables are associated with the response, e.g. is it height that dictates the weight or height and age?\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-2-1.png){width=576}\n:::\n:::\n\n:::\n:::\n\n## Statistical vs. deterministic relationship\n\n. . .\n\nRelationships in probability and statistics can generally be one of three things:\n\n\n-   deterministic\n-   random\n-   statistical\n\n\n## Statistical vs. deterministic relationship {.smaller}\n\n*deterministic*\n\nA **deterministic** relationship involves **an exact relationship** between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation $Fahrenheit=\\frac{9}{5}\\cdot Celcius+32$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n## Statistical vs. deterministic relationship {.smaller}\n\n*random*\n\nThere is **no relationship** between variables in the **random relationship**, e.g. number of plants Olga buys and time of the year as Olga buys plants whenever she feels like it throughout the entire year\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n## Statistical vs. deterministic relationship {.smaller}\n\n*statistical relationship = deterministic + random*\n\n**A statistical relationship** is a **mixture of deterministic and random relationship**, e.g. the savings that Olga has left in the bank account depend on Olga's monthly salary income (deterministic part) and the money spent on buying plants (random part)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## What linear models are\n\n*definition*\n\n<br>\n\n::: columns\n::: {.column width=\"60%\"}\n::: incremental\n-   In an linear model we model the relationship between a single continuous variable $Y$ and one or more variables $X$.\n-   One very general form for the model would be: $$Y = f(X_1, X_2, \\dots X_p) + \\epsilon$$ where $f$ is some unknown function and $\\epsilon$ is the error in this representation.\n-   The $X$ variables can be numerical, categorical or a mixture of both.\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"35%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-6-1.png){width=576}\n:::\n:::\n\n:::\n:::\n\n<!-- . . . -->\n\n<!-- -   For instance a **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \\beta \\cdot x + \\epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled. -->\n\n## What linear models are\n\n*definition*\n\n<br>\n\n::: incremental\n-   Formally, linear models are a way of describing a response variable in terms of **linear combination** of predictor variables.\n-   Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared\n:::\n\n<!-- ## What linear models are and are not -->\n\n<!-- *definition* -->\n\n<!-- <br> -->\n\n<!-- ::: incremental -->\n\n<!-- -   Formally, a linear model is one in which the **parameters appear linearly in the deterministic part of the model**. -->\n\n<!-- -   For instance these are linear models: $$Y_i = \\alpha + \\beta \\cdot x_i + \\gamma \\cdot y_i + \\epsilon_i$$ or $$Y_i = \\alpha + \\beta \\cdot x_i^2 \\epsilon$$ -->\n\n<!-- -   vs. an example on a non-linear model where parameter $\\beta$ appears in the exponent of $x_i$: $$Y_i = \\alpha + x_i^\\beta + \\epsilon$$ -->\n\n<!-- -   Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared -->\n\n<!-- ::: -->\n\n<!-- ## What linear models are and are not -->\n\n<!-- ```{r} -->\n<!-- #| warning: false -->\n<!-- #| echo: false -->\n<!-- #| fig-align: center -->\n<!-- #| fig-width: 8 -->\n<!-- #| fig-height: 6 -->\n\n<!-- my.ggtheme <- theme_bw() +  -->\n<!--   theme(axis.title = element_text(size = font.size),  -->\n<!--         axis.text = element_text(size = font.size),  -->\n<!--         legend.text = element_text(size = font.size),  -->\n<!--         legend.title = element_blank(),  -->\n<!--         legend.position = \"none\",  -->\n<!--         axis.title.y=element_text(angle=0)) -->\n\n<!-- # simple linear regression -->\n<!-- x <- seq(-10, 10, 1) -->\n<!-- set.seed(123) -->\n<!-- y <- x + rnorm(length(x), mean(x), 2) -->\n<!-- data.xy <- data.frame(x=x, y = y, ymodel = x) -->\n<!-- p.simple <-  data.xy %>% ggplot(aes(x = x, y = y)) + -->\n<!--   geom_point() + -->\n<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->\n<!--   my.ggtheme +  -->\n<!--   ggtitle(\"A\") -->\n\n<!-- # simple linear regression with group -->\n<!-- x <- seq(0, 10, length.out = 20) -->\n<!-- set.seed(123) -->\n<!-- y1 <- 0 + x + rnorm(length(x), 0, 2) -->\n<!-- set.seed(123) -->\n<!-- y2 <- 0 + 4*x + rnorm(length(x), 0, 2) -->\n\n<!-- x.all <- c(x, x) -->\n<!-- y.all <- c(y1, y2) -->\n<!-- group <- c(rep(\"CTRL\", length(x)), rep(\"TX\", length(x))) -->\n<!-- ymodel <- c(0+x, 0+4*x) -->\n\n<!-- data.xy <- data.frame(x=x.all, y = y.all, ymodel = ymodel) -->\n\n<!-- p.group <- data.xy %>% ggplot(aes(x = x, y = y, colour = group)) + -->\n<!--   geom_point() + -->\n<!--   geom_line(aes(x = x, y=ymodel)) +  -->\n<!--   theme_classic() + -->\n<!--   scale_color_brewer(palette = \"Set2\") +  -->\n<!--   my.ggtheme +  -->\n<!--   ggtitle(\"B\")  -->\n\n<!-- # advanced 1 -->\n<!-- x <- seq(-10, 10, 1) -->\n<!-- set.seed(123) -->\n<!-- y <- x^2 + rnorm(length(x), mean(x), 10) -->\n<!-- data.xy <- data.frame(x=x, y = y, ymodel = x^2) -->\n<!-- p.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->\n<!--   geom_point() + -->\n<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->\n<!--   theme_classic() + -->\n<!--   my.ggtheme +  -->\n<!--   ggtitle(\"C\") -->\n\n\n<!-- # advanced 2 -->\n<!-- x <- seq(-10, 10, 1) -->\n<!-- set.seed(123) -->\n<!-- y <- (x + (x^3))/1000 + rnorm(length(x), mean(x), 0.05) -->\n<!-- data.xy <- data.frame(x=x, x2=x^2, x3=x^3, y = y, ymodel = (x + x^3)/1000) -->\n<!-- p.adv2 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->\n<!--   geom_point() + -->\n<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->\n<!--   theme_classic() + -->\n<!--   my.ggtheme +  -->\n<!--   ggtitle(\"D\") -->\n\n<!-- grid.arrange(p.simple, p.group, p.adv1, p.adv2, ncol = 2) -->\n\n<!-- ``` -->\n\n<!-- ## What linear models are and are not -->\n\n<!-- ```{r} -->\n<!-- #| warning: false -->\n<!-- #| echo: false -->\n<!-- #| fig-align: center -->\n<!-- #| fig-width: 8 -->\n<!-- #| fig-height: 6 -->\n\n<!-- my.ggtheme <- theme_bw() +  -->\n<!--   theme(axis.title = element_text(size = font.size),  -->\n<!--         axis.text = element_text(size = font.size),  -->\n<!--         legend.text = element_text(size = font.size),  -->\n<!--         legend.title = element_blank(),  -->\n<!--         legend.position = \"none\",  -->\n<!--         axis.title.y=element_text(angle=0)) -->\n\n<!-- # simple linear regression -->\n<!-- x <- seq(-10, 10, 1) -->\n<!-- set.seed(123) -->\n<!-- y <- x + rnorm(length(x), mean(x), 2) -->\n<!-- data.xy <- data.frame(x=x, y = y, ymodel = x) -->\n<!-- p.simple <- data.xy %>% ggplot(aes(x = x, y = y)) + -->\n<!--   geom_point() + -->\n<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->\n<!--   my.ggtheme +  -->\n<!--   ggtitle(TeX(r'(A: $y_i = x_1 + e_i$)')) -->\n\n<!-- # simple linear regression with group -->\n<!-- x <- seq(0, 10, length.out = 20) -->\n<!-- set.seed(123) -->\n<!-- y1 <- 0 + x + rnorm(length(x), 0, 2) -->\n<!-- set.seed(123) -->\n<!-- y2 <- 0 + 4*x + rnorm(length(x), 0, 2) -->\n\n<!-- x.all <- c(x, x) -->\n<!-- y.all <- c(y1, y2) -->\n<!-- group <- c(rep(\"CTRL\", length(x)), rep(\"TX\", length(x))) -->\n<!-- ymodel <- c(0+x, 0+4*x) -->\n\n<!-- data.xy <- data.frame(x=x.all, y = y.all, ymodel = ymodel) -->\n\n<!-- p.group <- data.xy %>% ggplot(aes(x = x, y = y, colour = group)) + -->\n<!--   geom_point() + -->\n<!--   geom_line(aes(x = x, y=ymodel)) +  -->\n<!--   theme_classic() + -->\n<!--   scale_color_brewer(palette = \"Set2\") +  -->\n<!--   my.ggtheme +  -->\n<!--   ggtitle(TeX(r'(B: $x_1 + I_{x_i} + e_i$)')) -->\n\n<!-- # advanced 1 -->\n<!-- x <- seq(-10, 10, 1) -->\n<!-- set.seed(123) -->\n<!-- y <- x^2 + rnorm(length(x), mean(x), 10) -->\n<!-- data.xy <- data.frame(x=x, y = y, ymodel = x^2) -->\n<!-- p.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->\n<!--   geom_point() + -->\n<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->\n<!--   theme_classic() + -->\n<!--   my.ggtheme +  -->\n<!--   ggtitle(TeX(r'(C: $y_i = x_i^2 + e_i$)')) -->\n\n\n<!-- # advanced 2 -->\n<!-- x <- seq(-10, 10, 1) -->\n<!-- set.seed(123) -->\n<!-- y <- (x + (x^3))/1000 + rnorm(length(x), mean(x), 0.05) -->\n<!-- data.xy <- data.frame(x=x, x2=x^2, x3=x^3, y = y, ymodel = (x + x^3)/1000) -->\n<!-- p.adv2 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->\n<!--   geom_point() + -->\n<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->\n<!--   theme_classic() + -->\n<!--   my.ggtheme +  -->\n<!--   ggtitle(TeX(r'(D: $y_i = x + x_i^3 + e_i$)')) -->\n\n<!-- grid.arrange(p.simple, p.group, p.adv1, p.adv2, ncol = 2) -->\n\n<!-- ``` -->\n\n<!-- ## Terminology -->\n\n<!-- There are many terms and notations used interchangeably -->\n\n<!-- ::: incremental -->\n\n<!-- -   $y$ is being called: -->\n\n<!--     -   response -->\n\n<!--     -   outcome -->\n\n<!--     -   dependent variable -->\n\n<!-- -   $x$ is being called: -->\n\n<!--     -   exposure -->\n\n<!--     -   explanatory variable -->\n\n<!--     -   dependent variable -->\n\n<!--     -   predictor -->\n\n<!--     -   covariate -->\n\n<!-- ::: -->\n\n## Simple linear regression\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n::: incremental\n-   It is used to check the association between **the numerical outcome and one numerical explanatory variable**\n-   In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n:::\n\n## Simple linear regression\n\n::: {#exm-simple-lm}\n## Weight and plasma volume\n\nLet's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\n\n\n::: {.cell}\n\n:::\n\n\n``` r\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n```\n:::\n\n\n\n\n\n\n## Simple linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-lm-presentation_files/figure-revealjs/fig-lm-01-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-02-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-03-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-03b-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-04-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-05-1.png){.fragment width=\"600\" height=\"600\"}\n:::\n\n## Simple linear regression\n\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable](session-lm-presentation_files/figure-revealjs/fig-lm-example-reg-1.png){#fig-lm-example-reg width=960}\n:::\n:::\n\n\n. . .\n\nThe equation for the red line is: $$Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8$$\n\n. . .\n\nand in general: $$Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n$$ {#eq-lm-no-error}\n\n. . .\n\n## Simple linear regression\n\nIn other words, by finding the best-fitting straight line we are **building a statistical model** to represent the relationship between plasma volume ($Y$) and explanatory body weight variable ($x$)\n\n## Simple linear regression {.smaller}\n\n<br>\n\n``` r\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n```\n\n<br>\n\n::: columns\n::: {.column width=\"55%\"}\n::: incremental\n-   If we were to use our model $Y_i=0.086 + 0.044 \\cdot x_i$ to find plasma volume given a weight of 58 kg (our first observation, $i=1$)\n-   we would notice that we would get $Y=0.086 + 0.044 \\cdot 58 = 2.638$\n-   $2.638$ is not exactly the as same as $2.75$, the first measurement we have in our dataset, i.e. $2.75 - 2.638 = 0.112 \\neq 0$.\n-   We thus add to the previous equation (@eq-lm-no-error) an **error term** to account for this and now we can write our **simple regression model** more formally as:\n-   $$Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$$ {#eq-lm} where:\n-   $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n-   $y$: is called: response, outcome, dependent variable\n-   $\\alpha$ and $\\beta$ are **model coefficients**\n-   and $\\epsilon_i$ is an **error terms**\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Least squares\n\n::: incremental\n-   in the above **\"body weight - plasma volume\"** example, the values of $\\alpha$ and $\\beta$ have just appeared\n-   in practice, $\\alpha$ and $\\beta$ values are **unknown** and we use data to **estimate these coefficients**, noting the estimates with a **hat**, $\\hat{\\alpha}$ and $\\hat{\\beta}$\n-   **least squares** is one of the methods of parameters estimation, i.e. finding $\\hat{\\alpha}$ and $\\hat{\\beta}$\n:::\n\n## Least squares {.smaller}\n\n*minimizing RSS*\n\n::: columns\n::: {.column width=\"50%\"}\nLet $\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:\n\n-   Then $\\epsilon_i = y_i - \\hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model\n-   RSS, the **residual sum of squares** is defined as: $$RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2$$ or equivalently as: $$RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2$$\n-   the least squares approach chooses $\\hat{\\alpha}$ and $\\hat{\\beta}$ **to minimize the RSS**.\n:::\n\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice versa*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line](session-lm-presentation_files/figure-revealjs/fig-reg-errors-1.png){#fig-reg-errors width=768}\n:::\n:::\n\n:::\n:::\n\n<!-- ## Least squares -->\n\n<!-- ::: {#thm-lss} -->\n\n<!-- ## Least squares estimates for a simple linear regression -->\n\n<!-- $$\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}$$ $$\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}$$ -->\n\n<!-- where: -->\n\n<!-- -   $\\bar{x}$: mean value of $x$ -->\n\n<!-- -   $\\bar{y}$: mean value of $y$ -->\n\n<!-- -   $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2$ -->\n\n<!-- -   $S_{yy}$: sum of squares of $Y$ defined as $S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2$ -->\n\n<!-- -   $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$ -->\n\n<!-- ::: -->\n\n<!-- ## Least squares -->\n\n<!-- *Live demo* -->\n\n<!-- ::: {#exm-lss} -->\n\n<!-- ## Least squares -->\n\n<!-- Let's try least squares method to find coefficient estimates in the **\"body weight and plasma volume example\"** -->\n\n<!-- ``` r -->\n\n<!-- # initial data -->\n\n<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->\n\n<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->\n\n<!-- # rename variables for convenience -->\n\n<!-- x <- weight -->\n\n<!-- y <- plasma -->\n\n<!-- # mean values of x and y -->\n\n<!-- x.bar <- mean(x) -->\n\n<!-- y.bar <- mean(y) -->\n\n<!-- # Sum of squares -->\n\n<!-- Sxx <-  sum((x - x.bar)^2) -->\n\n<!-- Sxy <- sum((x-x.bar)*(y-y.bar)) -->\n\n<!-- # Coefficient estimates -->\n\n<!-- beta.hat <- Sxy / Sxx -->\n\n<!-- alpha.hat <- y.bar - Sxy/Sxx*x.bar -->\n\n<!-- # Print estimated coefficients alpha and beta -->\n\n<!-- print(alpha.hat) -->\n\n<!-- print(beta.hat) -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- #| code-fold: false -->\n\n<!-- # initial data -->\n\n<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->\n\n<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->\n\n<!-- # rename variables for convenience -->\n\n<!-- x <- weight -->\n\n<!-- y <- plasma -->\n\n<!-- # mean values of x and y -->\n\n<!-- x.bar <- mean(x) -->\n\n<!-- y.bar <- mean(y) -->\n\n<!-- # Sum of squares -->\n\n<!-- Sxx <-  sum((x - x.bar)^2) -->\n\n<!-- Sxy <- sum((x-x.bar)*(y-y.bar)) -->\n\n<!-- # Coefficient estimates -->\n\n<!-- beta.hat <- Sxy / Sxx -->\n\n<!-- alpha.hat <- y.bar - Sxy/Sxx*x.bar -->\n\n<!-- # Print estimated coefficients alpha and beta -->\n\n<!-- print(alpha.hat) -->\n\n<!-- print(beta.hat) -->\n\n<!-- ``` -->\n\n<!-- ::: -->\n\n## Slope\n\n$plasma = 0.0857 + 0.0436 * weight$\n\nLinear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n*Increasing weight by 5 kg corresponds to* $3.14 - 2.92 = 0.22$ increase in plasma volume. Increasing weight by 1 kg corresponds $2.96 - 2.92 = 0.04$ increase in plasma volume\n\n## Intercept\n\n$plasma = 0.0857 + 0.0436 * weight$\n\nLinear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n*Intercept value corresponds to expected outcome when the explanatory variable value equals to zero. It is not always meaningful*\n\n## Hypothesis testing\n\n**Is there a relationship between the response and the predictor?**\n\n::: incremental\n-   the calculated $\\hat{\\alpha}$ and $\\hat{\\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**\n-   their precision is measured by their **estimated standard errors**, `e.s.e`($\\hat{\\alpha}$) and `e.s.e`($\\hat{\\beta}$)\n-   these estimated standard errors are used in **hypothesis testing**\n:::\n\n## Hypothesis testing {.smaller}\n\n**The most common hypothesis test** involves testing the `null hypothesis` of:\n\n-   $H_0:$ There is no relationship between $X$ and $Y$\n-   versus the `alternative hypothesis` $H_a:$ there is some relationship between $X$ and $Y$\n\n. . .\n\n**Mathematically**, this corresponds to testing:\n\n-   $H_0: \\beta=0$\n-   versus $H_a: \\beta\\neq0$\n-   since if $\\beta=0$ then the model $Y_i=\\alpha+\\beta x_i + \\epsilon_i$ reduces to $Y=\\alpha + \\epsilon_i$\n\n. . .\n\n**Under the null hypothesis** $H_0: \\beta = 0$ <!-- we have: $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)$$  --> ![](figures/linear-models/lm-tstatistics.png)\n\n-   $n$ is number of observations\n-   $p$ is number of model parameters\n-   $\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}$ is the ratio of the departure of the estimated value of a parameter, $\\hat\\beta$, from its hypothesized value, $\\beta$, to its standard error, called `t-statistics`\n-   the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom\n\n## Hypothesis testing {.smaller}\n\n::: {#exm-hypothesis-testing}\n## Hypothesis testing\n\nLet's look again at our example data and linear model fitted in R with lm() function.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n```\n:::\n:::\n\n:::\n\n. . .\n\n-   Under `Estimate` we see estimates of our model coefficients, $\\hat{\\alpha}$ (intercept) and $\\hat{\\beta}$ (slope, here weight), followed by their estimated standard errors, `Std. Errors`\n\n. . .\n\n-   If we were to test if there is an **association between weight and plasma volume** we would write under the null hypothesis $H_0: \\beta = 0$ $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582$$\n\n. . .\n\n-   and we would **compare** `t-statistics` to `Student's t distribution` with $n-p = 8 - 2 = 6$ degrees of freedom (as we have 8 observations and two model parameters, $\\alpha$ and $\\beta$)\n\n. . .\n\n-   we can use **Student's t distribution table** or **R code** to obtain the associated *P*-value\n\n``` r\n2*pt(2.856582, df=6, lower=F)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02893095\n```\n:::\n:::\n\n\n. . .\n\n-   here the observed t-statistics is large and therefore yields a small *P*-value, meaning that **there is sufficient evidence to reject null hypothesis in favor of the alternative** and conclude that there is a significant association between weight and plasma volume\n\n## Vector-matrix notations {.scrollable .smaller}\n\nLet's **rewrite** our simple linear regression model $Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n$ **into vector-matrix notation** in **6 steps**.\n\n. . .\n\nStep 1. We rename our $\\alpha$ to $\\beta_0$ and $\\beta$ to $\\beta_1$.\n\n. . .\n\nStep 2. We notice that we have $n$ equations such as:\n\n$$y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1$$ $$y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2$$ $$y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3$$ $$\\dots$$ $$y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n$$\n\n. . .\n\nStep 3. We group all $Y_i$ and $\\epsilon_i$ into column vectors: $\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}$ and $\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}$\n\n. . .\n\nStep 4. We stack two parameters $\\beta_0$ and $\\beta_1$ into another column vector:$$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$$\n\n. . .\n\nStep 5. We append a vector of ones with the single predictor for each $i$ and create a matrix with two columns called **design matrix** $$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}$$\n\n. . .\n\nStep 6. We write our linear model in a vector-matrix notations as: $$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n## Vector-matrix notations {.smaller}\n\n::: {#def-vector-matrix-lm}\n## vector matrix form of the linear model\n\nThe vector-matrix representation of a linear model with $p-1$ predictors can be written as $$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nwhere:\n\n-   $\\mathbf{Y}$ is $n \\times1$ vector of observations\n-   $\\mathbf{X}$ is $n \\times p$ **design matrix**\n-   $\\boldsymbol\\beta$ is $p \\times1$ vector of parameters\n-   $\\boldsymbol\\epsilon$ is $n \\times1$ vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, $\\sigma^2$)\n\nIn full, the above vectors and matrix have the form:\n\n$\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}$ $\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix}$ $\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}$ $\\mathbf{X}=\\begin{bmatrix} 1 & x_{1,1} & \\dots & x_{1,p-1} \\\\ 1 & x_{2,1} & \\dots & x_{2,p-1} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_{n,1} & \\dots & x_{n,p-1} \\end{bmatrix}$\n:::\n\n## Vector-matrix notations\n\n::: {#thm-lss-vector-matrix}\n## Least squares in vector-matrix notation\n\nThe least squares estimates for a linear regression of the form: $$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nis given by: $$\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n:::\n\n## Vector-matrix notations {.smaller .scrollable}\n\n::: {#exm-vector-matrix-notation}\n## vector-matrix-notation\n\nFollowing the above definition we can write the **\"weight - plasma volume model\"** as: $$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$ where:\n\n$\\mathbf{Y}=\\begin{bmatrix} 2.75 \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12 \\end{bmatrix}$\n\n$\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ $\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{8} \\end{bmatrix}$ $\\mathbf{X}=\\begin{bmatrix} 1 & 58.0 \\\\ 1 & 70.0 \\\\ 1 & 74.0 \\\\ 1 & 63.5 \\\\ 1 & 62.0 \\\\ 1 & 70.5 \\\\ 1 & 71.0 \\\\ 1 & 66.0 \\\\ \\end{bmatrix}$\n\nand we can estimate model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n:::\n\n## Vector-matrix notations: least squares\n\n*Live demo*\n\nEstimating model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n\n``` r\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\nprint(X)\n\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n       0.08572428\nweight 0.04361534\n```\n:::\n:::\n\n\n## Vector-matrix notations: least squares\n\n*Live demo*\n\nIn R we can use `lm()`, the built-in function, to fit a linear regression model and we can replace the above code with one line\n\n``` r\nlm(plasma ~ weight)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nCoefficients:\n(Intercept)       weight  \n    0.08572      0.04362  \n```\n:::\n:::\n\n\n<!-- ## Confidence intervals and prediction intervals -->\n\n<!-- - when we estimate coefficients we can also find their **confidence intervals**, typically 95\\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter -->\n\n<!-- - we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval -->\n\n<!-- ::: {exm-prediction-and-intervals} -->\n\n<!-- ## prediction and intervals -->\n\n<!-- Let's: -->\n\n<!-- - find confidence intervals for our coefficient estimates -->\n\n<!-- - predict plasma volume for a men weighting 60 kg -->\n\n<!-- - find prediction interval -->\n\n<!-- - plot original data, fitted regression model, predicted observation and prediction interval -->\n\n<!-- ```{r} -->\n\n<!-- #| code-fold: false -->\n\n<!-- # fit regression model -->\n\n<!-- model <- lm(plasma ~ weight) -->\n\n<!-- print(summary(model)) -->\n\n<!-- # find confidence intervals for the model coefficients -->\n\n<!-- confint(model) -->\n\n<!-- # predict plasma volume for a new observation of 60 kg -->\n\n<!-- # we have to create data frame with a variable name matching the one used to build the model -->\n\n<!-- new.obs <- data.frame(weight = 60) -->\n\n<!-- predict(model, newdata = new.obs) -->\n\n<!-- # find prediction intervals -->\n\n<!-- prediction.interval <- predict(model, newdata = new.obs,  interval = \"prediction\") -->\n\n<!-- print(prediction.interval) -->\n\n<!-- # plot the original data, fitted regression and predicted value -->\n\n<!-- plot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\", ylim=c(2,4)) -->\n\n<!-- lines(weight, model$fitted.values, col=\"red\") # fitted model in red -->\n\n<!-- points(new.obs, predict(model, newdata = new.obs), pch=19, col=\"blue\") # predicted value at 60kg -->\n\n<!-- segments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval -->\n\n<!-- ``` -->\n\n<!-- ## References -->\n\n<!-- ::: {#refs} -->\n\n<!-- ::: -->\n\n# Assessing model fit\n\n<!-- -   In addition to knowing how to estimate model parameters we need to learn to assess the goodness of fit of a model. -->\n<!-- -   We do that by calculating the amount of variability in the response that is explained by the model. -->\n\n## $R^2$: summary of the fitted model\n\n*TSS*\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-lm-presentation_files/figure-revealjs/fig-lm-fit-00-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-fit-01-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-fit-02-1.png){.fragment width=\"600\" height=\"600\"}\n:::\n\n## $R^2$: summary of the fitted model\n\n*RSS*\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-lm-presentation_files/figure-revealjs/fig-lm-rss-01-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-rss-02-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-rss-03-1.png){.fragment width=\"600\" height=\"600\"}\n:::\n\n## $R^2$: summary of the fitted model\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-31-1.png){width=768}\n:::\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n<br>\n\n-   **TSS**, denoted **Total corrected sum-of-squares** is the residual sum-of-squares for Model 0 $$S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}$$ corresponding the to the sum of squared distances to the purple line\n\n-   **RSS**, the residual sum-of-squares, is defined as: $$RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$ and corresponds to the squared distances between the observed values $y_i, \\dots,y_n$ to fitted values $\\hat{y_1}, \\dots \\hat{y_n}$, i.e. distances to the red fitted line\n:::\n:::\n\n## $R^2$: summary of the fitted model\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-32-1.png){width=768}\n:::\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n<br>\n\n::: {#def-r2}\n## $R^2$\n\nA simple but useful measure of model fit is given by $$R^2 = 1 - \\frac{RSS}{TSS}$$ where:\n\n-   RSS is the residual sum-of-squares for Model 1, the fitted model of interest\n-   TSS is the sum of squares of the **null model**\n-   $R^2$ is also referred as **coefficient of determination**\n-   $R^2$ is expressed on a scale, as a proportion between 0 and 1 of the total variation in the data.\n:::\n:::\n:::\n\n<!-- ## $R^2$: summary of the fitted model -->\n\n<!-- ::: incremental -->\n\n<!-- -   $R^2$ quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model -->\n\n<!-- -   $R^2$ is also referred as **coefficient of determination** -->\n\n<!-- -   It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data -->\n\n<!-- -   Values of $R^2$ approaching 1 indicate the model to be a good fit -->\n\n<!-- -   Values of $R^2$ less than 0.5 suggest that the model gives rather a poor fit to the data -->\n\n<!-- ::: -->\n\n## $R^2$ and correlation coefficient\n\n. . .\n\n::: {#thm-r2}\n## $R^2$\n\nIn the case of simple linear regression:\n\nModel 1: $Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$ $$R^2 = r^2$$ where:\n\n-   $R^2$ is the coefficient of determination\n-   $r^2$ is the sample correlation coefficient\n:::\n\n::: notes\nWhat other coefficient is expressed on a 0-1 scale?\n:::\n\n## $R^2(adj)$\n\n::: incremental\n-   In the case of multiple linear regression we are using the **adjusted version of** $R^2$ to assess the model fit as the number of explanatory variables increase, $R^2$ also increases.\n:::\n\n. . .\n\n::: {#thm-r2adj}\n## $R^2(adj)$\n\nFor any multiple linear regression $$Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i$$ $R^2(adj)$ is defined as $$R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}$$\n\n-   $p$ is the number of independent predictors, i.e. the number of variables in the model, excluding the constant and $n$ is number of observations.\n\n$R^2(adj)$ can also be calculated from $R^2$: $$R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}$$\n:::\n\n<!-- ## $R^2$ -->\n\n<!-- *Live demo* -->\n\n<!-- ``` r -->\n\n<!-- htwtgen <- read.csv(\"data/lm/heights_weights_gender.csv\") -->\n\n<!-- head(htwtgen) -->\n\n<!-- attach(htwtgen) -->\n\n<!-- ## Simple linear regression -->\n\n<!-- model.simple <- lm(Height ~ Weight, data=htwtgen) -->\n\n<!-- # TSS -->\n\n<!-- TSS <- sum((Height - mean(Height))^2) -->\n\n<!-- # RSS -->\n\n<!-- # residuals are returned in the model type names(model.simple) -->\n\n<!-- RSS <- sum((model.simple$residuals)^2) -->\n\n<!-- R2 <- 1 - (RSS/TSS) -->\n\n<!-- print(R2) -->\n\n<!-- print(summary(model.simple)) -->\n\n<!-- ``` -->\n\n<!-- ## $R^2$ -->\n\n<!-- *Live demo* -->\n\n<!-- ``` r -->\n\n<!-- htwtgen <- read.csv(\"data/lm/heights_weights_gender.csv\") -->\n\n<!-- head(htwtgen) -->\n\n<!-- attach(htwtgen) -->\n\n<!-- ## Multiple regression -->\n\n<!-- model.multiple <- lm(Height ~ Weight + Gender, data=htwtgen) -->\n\n<!-- n <- length(Weight) -->\n\n<!-- p <- 1 -->\n\n<!-- RSS <- sum((model.multiple$residuals)^2) -->\n\n<!-- R2_adj <- 1 - (RSS/(n-p-1))/(TSS/(n-1)) -->\n\n<!-- print(R2_adj) -->\n\n<!-- print(summary(model.multiple)) -->\n\n<!-- ``` -->\n\n# Checking model assumptions\n\n## The assumptions of a linear model\n\n::: incremental\n-   Before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data.\n-   Informally we have been using box plots and scatter plots to look at the data.\n-   There are however formal assumptions that should be met when fitting linear models.\n:::\n\n## The assumptions of a linear model\n\n. . .\n\n<br>\n\n**Linearity:**\n\n-   The relationship between $X$ and $Y$ is linear\n\n. . .\n\n**Independence of errors**\n\n-   $Y$ is independent of errors, there is no relationship between the residuals and $Y$\n\n. . .\n\n**Normality of errors**\n\n-   The residuals must be approximately normally distributed\n\n. . .\n\n**Equal variances**\n\n-   The variance of the residuals is the same for all values of $X$\n\n## Checking assumptions {.smaller}\n\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**.\n\n. . .\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Modeling BMI with waist explanatory variable. Model assumptions seems to be met with no noticible patterns based on residuals. Few potential outliers could be considered to be removed.](session-lm-presentation_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\n## Checking assumptions {.smaller}\n\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**.\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Modeling glucose with age. Model assumptions seems to be violated, e.g. with residulas not following normal distributions.](session-lm-presentation_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n# Exercises\n\nLet's practice in exercises fitting linear models, hypothesis testing and assessing model fit.\n\n# Linear models: regression and classification tasks\n\n## Linear models in ML context\n\n<br>\n\n::: columns\n::: {.column width=\"55%\"}\n::: incremental\n-   We can think of linear models in machine learning context, as they are both used for **regression** and **classification**.\n-   Often some or many of the variables used in linear models **are not** associated with the response.\n-   There are **feature selection** methods for excluding **irrelevant** variables and improving prediction results.\n    -   subset selection, **Shrinkage methods** and dimension reduction\n    -   or grouped by filter methods, wrapper methods and **embedded methods**.\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Evaluating linear models\n\n<br>\n\nRegression models can be evaluated by assessing a model fit or by directly evaluating the prediction error via using data splitting strategies.\n\n. . .\n\n::: columns\n::: {.column width=\"55%\"}\n**Model fit**\n\n**Adjusted R-squared** $$R_{adj}^2=1-\\frac{RSS}{TSS}\\frac{n-1}{n-p-1}$$\n\n**Akaike information criterion (AIC)** $$\\text{AIC} = n \\ln(\\text{RSS}/n) + 2p$$\n\n**Bayesian information criterion (BIC)** $$\\text{BIC} = n \\ln(\\text{RSS}/n) + p \\ln(n)$$\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Prediction error**\n\n**Mean Squared Error (MSE)** $$MSE = \\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2$$ **Root Mean Squared Error (RMSE)** $$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2}$$\n\n**Mean Absolute Error (MAE)** $$MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y}_i|$$\n:::\n:::\n\n## Feature selection\n\n*Group discussion* <br>\n\nIt is time to try to find the best model to explain `BMI` using `diabetes` data. Given from what we have learnt so far about linear regression models, how would you find the best model?\n\nAs a reminder, we have below variables in the data:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"id\"       \"chol\"     \"stab.glu\" \"hdl\"      \"ratio\"    \"glyhb\"   \n [7] \"location\" \"age\"      \"gender\"   \"height\"   \"weight\"   \"frame\"   \n[13] \"bp.1s\"    \"bp.1d\"    \"bp.2s\"    \"bp.2d\"    \"waist\"    \"hip\"     \n[19] \"time.ppn\" \"BMI\"      \"obese\"    \"diabetic\"\n```\n:::\n:::\n\n\n## Feature selection\n\n*Definition* <br>\n\nFeature selection is the process of selecting the most relevant and informative subset of features from a larger set of potential features in order to improve the performance and interpretability of a machine learning model.\n\n### There are generally three main groups of feature selection methods:\n\n::: incremental\n-   **Filter methods** use statistical measures to score the features and select the most relevant ones, e.g. based on correlation coefficient. Computationally efficient but may overlook complex interactions between features.\n-   **Wrapper methods** use ML algorithm to evaluate the performance of different subsets of features, e.g. forward/backward feature selection. Computationally heavy.\n-   **Embedded methods** incorporate feature selection as part of the ML algorithm itself, e.g. **regularized regression** or **Random Forest**. These methods are computationally efficient and can be more accurate than filter methods.\n:::\n\n## Regularized regression\n\n*definition* <br>\n\n::: incremental\n-   Regularized regression expands on the regression by adding a **penalty term(s)** to **shrink the model coefficients** of less important features towards zero.\n-   This can help to prevent overfitting and improve the accuracy of the predictive model.\n-   Depending on the penalty added, we talk about **Ridge**, **Lasso** or **Elastic Nets regression**.\n:::\n\n## Regularized regression\n\n*Ridge regression* <br>\n\n-   Previously we saw that the least squares fitting procedure estimates model coefficients $\\beta_0, \\beta_1, \\cdots, \\beta_p$ using the values that minimize the residual sum of squares: $$RSS = \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2$$ {#eq-lm}\n\n. . .\n\n-   In **regularized regression** the coefficients are estimated by minimizing slightly different quantity. Specifically, in **Ridge regression** we estimate $\\hat\\beta^{L}$ that minimizes $$\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2$$ {#eq-ridge}\n\nwhere:\n\n$\\lambda \\ge 0$ is a **tuning parameter** to be determined separately e.g. via cross-validation\n\n## Regularized regression\n\n*Ridge regression* <br>\n\n$$\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2$$ {#eq-ridge}\n\n@eq-ridge trades two different criteria:\n\n-   lasso regression seeks coefficient estimates that fit the data well, by making RSS small\n-   however, the second term $\\lambda \\sum_{j=1}^{p}\\beta_j^2$, called **shrinkage penalty** is small when $\\beta_1, \\cdots, \\beta_p$ are close to zero, so it has the effect of **shrinking** the estimates of $\\beta_j$ towards zero.\n-   the tuning parameter $\\lambda$ controls the relative impact of these two terms on the regression coefficient estimates\n    -   when $\\lambda = 0$, the penalty term has no effect\n    -   as $\\lambda \\rightarrow \\infty$ the impact of the shrinkage penalty grows and the ridge regression coefficient estimates approach zero\n\n## Regularized regression\n\n*Ridge regression* <br>\n\n\n::: {.cell}\n\n:::\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of Ridge regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero for larger lambda values.](session-lm-presentation_files/figure-revealjs/ridge-run-1.png){width=960}\n:::\n:::\n\n\n## Bias-variance trade-off\n\n<br>\n\nRidge regression's advantages over least squares estimates stems from **bias-variance trade-off**\n\n::: incremental\n-   The bias-variance trade-off describes the relationship between model complexity, prediction accuracy, and the ability of the model to generalize to new data.\n-   **Bias** refers to the error that is introduced by approximating a real-life problem with a simplified model\n    -   e.g. a high bias model is one that makes overly simplistic assumptions about the underlying data, resulting in *under-fitting* and poor accuracy.\n-   **Variance** refers to the sensitivity of a model to fluctuations in the training data.\n    -   e.g. a high variance model is one that is overly complex and captures noise in the training data, resulting in *overfitting* and poor generalization to new data.\n:::\n\n## Bias-variance trade-off\n\n<br>\n\nThe goal of machine learning is to find a model with **the right balance between bias and variance**.\n\n::: incremental\n-   The bias-variance trade-off can be visualized in terms of MSE, means squared error of the model. The **MSE** can be decomposed into: $$MSE(\\hat\\beta) := bias^2(\\hat\\beta) + Var(\\hat\\beta) + noise$$\n    -   The irreducible error is the inherent noise in the data that cannot be reduced by any model\n    -   The bias and variance terms can be reduced by choosing an appropriate model complexity.\n    -   The trade-off lies in finding the right balance between bias and variance that minimizes the total MSE.\n-   In practice, this trade-off can be addressed by **regularizing the model**, selecting an appropriate model complexity, or by using ensemble methods that combine multiple models to reduce the variance.\n:::\n\n## Bias-variance trade-off\n\n<br>\n\n\n::: {.cell layout-align=\"center\" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. al, A Introduction to statistical learning](images/bias-variance.png){#fig-bias-variance fig-align='center' width=100%}\n:::\n:::\n\n\n## Ridge vs. Lasso\n\nIn **Ridge** regression we minimize: $$\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2$$ {#eq-ridge2} where $\\lambda \\sum_{j=1}^{p}\\beta_j^2$ is also known as **L2** regularization element or $l_2$ penalty\n\nIn **Lasso** regression, that is Least Absolute Shrinkage and Selection Operator regression we change penalty term to absolute value of the regression coefficients: $$\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j| = RSS + \\lambda \\sum_{j=1}^{p}|\\beta_j|$$ {#eq-lasso} where $\\lambda \\sum_{j=1}^{p}|\\beta_j|$ is also known as **L1** regularization element or $l_1$ penalty\n\nLasso regression was introduced to help model interpretation. With Ridge regression we improve model performance but unless $\\lambda = \\infty$ all beta coefficients are non-zero, hence all variables remain in the model. By using $l_1$ penalty we can force some of the coefficients estimates to be exactly equal to 0, hence perform **variable selection**\n\n## Ridge vs. Lasso\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Example of Ridge regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero for larger lambda values.](session-lm-presentation_files/figure-revealjs/ridge-run-02-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![Example of Lasso regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero and are also set to zero for larger lambda values.](session-lm-presentation_files/figure-revealjs/lasso-run-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Elastic Net\n\n<br>\n\n**Elastic Net** use both L1 and L2 penalties to try to find middle grounds by performing parameter shrinkage and variable selection. $$\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j| + \\lambda \\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda \\sum_{j=1}^{p}|\\beta_j| + \\lambda \\sum_{j=1}^{p}\\beta_j^2 $$ {#eq-elastic-net}\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of Elastic Net regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values and alpha value 0.1, showing the changes of model coefficients as a function of lambda being somewhere between those for Ridge and Lasso regression.](session-lm-presentation_files/figure-revealjs/elastic-net-1.png){width=960}\n:::\n:::\n\n\n## Elastic Net\n\n*In R with `glmnet`* <br>\n\nIn the `glmnet` library we can fit Elastic Net by setting parameters $\\alpha$. Under the hood `glmnet` minimizes a cost function: $$\\sum_{i_=1}^{n}(y_i-\\hat y_i)^2 + \\lambda \\left ( (1-\\alpha) \\sum_{j=1}^{p}\\beta_j^2 + \\alpha \\sum_{j=1}^{p}|\\beta_j|\\right )$$ where:\n\n-   $n$ is the number of samples\n-   $p$ is the number of parameters\n-   $\\lambda$, $\\alpha$ hyperparameters control the shrinkage\n\nWhen $\\alpha = 0$ this corresponds to Ridge regression and when $\\alpha=1$ this corresponds to Lasso regression. A value of $0 < \\alpha < 1$ gives us **Elastic Net regularization**, combining both L1 and L2 regularization terms.\n\n# Generalized Linear Models\n\n## Why Generalized Linear Models\n\n*GLM*\n\n::: columns\n::: {.column width=\"50%\"}\n-   GLMs extend linear model framework to outcome variables that do not follow normal distribution.\n-   They are most frequently used to model binary, categorical or count data.\n-   For instance, fitting a regression line to binary data yields predicted values that could take any value, including $<0$,\n-   not to mention that it is hard to argue that the values of 0 and 1s are normally distributed.\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell fig-cap-location='bottom'}\n::: {.cell-output-display}\n![Example of fitting linear model to binary data, to model obesity status coded as 1 (Yes) and 0 (No) with waist variable. Linear model does not fit the data well in this case and the predicted values can take any value, not only 0 and 1.](session-lm-presentation_files/figure-revealjs/log-example-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Logistic regression\n\n::: columns\n::: {.column width=\"50%\"}\n-   Since the response variable takes only two values (Yes/No) we use GLM model\n-   to fit **logistic regression** model for the **probability of suffering from obesity (Yes).**\n-   We let $p_i=P(Y_i=1)$ denote the probability of suffering from obesity (success)\n-   and we assume that the response follows binomial distribution: $Y_i \\sim Bi(1, p_i)$ distribution.\n-   We can then write the regression model now as: $$log(\\frac{p_i}{1-p_i})=\\beta_0 + \\beta_1x_i$$ and given the properties of logarithms this is also equivalent to: $$p_i = \\frac{exp(\\beta_0 + \\beta_1x_i)}{1 + exp(\\beta_0 + \\beta_1x_i)}$$\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/fig-obesity-1.png){#fig-obesity width=960}\n:::\n:::\n\n:::\n:::\n\n## Logistic regression\n\n<br>\n\n-   In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a **link function**.\n-   Here, the **link function** $log(\\frac{p_i}{1-p_i})$ provides the link between the binomial distribution of $Y_i$ (suffering from obesity) and the linear predictor (waist)\n-   Thus the **GLM model** can be written as $$g(\\mu_i)=\\mathbf{X}\\boldsymbol\\beta$$ where `g()` is the link function.\n\n## Logistic regression\n\n*R* <br>\n\nIn R we can use `glm()` function to fit GLM models:\n\n``` r\n# fit logistic regression model\nlogmodel_1 <- glm(obese ~ waist, family = binomial(link=\"logit\"), data = data_diabetes)\n\n# print model summary\nprint(summary(logmodel_1))\n```\n\n\n::: {.cell}\n\n```\n## \n## Call:\n## glm(formula = obese ~ waist, family = binomial(link = \"logit\"), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -18.29349    1.83541  -9.967   <2e-16 ***\n## waist         0.18013    0.01843   9.774   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 518.24  on 394  degrees of freedom\n## Residual deviance: 282.93  on 393  degrees of freedom\n##   (8 observations deleted due to missingness)\n## AIC: 286.93\n## \n## Number of Fisher Scoring iterations: 6\n```\n:::\n\n\n## Logistic regression\n\n*R* <br>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Fitted logistic model to diabetes data given the 130 study participants and using waist as explantatory variable to model obesity status.](session-lm-presentation_files/figure-revealjs/unnamed-chunk-47-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Logistic regression\n\n*Hypothesis testing* <br>\n\n::: columns\n::: {.column width=\"40%\"}\n-   Similarly to linear models, we can determine whether each variable is related to the outcome of interest by testing the null hypothesis that the relevant logistic regression coefficient is zero.\n-   This can be performed by **Wald test** which is equals to estimated logistic regression coefficient divided by its standard error and follows the Standard Normal distribution: $$W = \\frac{\\hat\\beta-\\beta}{e.s.e.(\\hat\\beta)} \\sim N(0,1)$$\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n``` r\n# fit logistic regression model\nlogmodel_1 <- glm(obese ~ waist, family = binomial(link=\"logit\"), data = data_diabetes)\nsummary(logmodel_1)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = obese ~ waist, family = binomial(link = \"logit\"), \n    data = data_diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -18.29349    1.83541  -9.967   <2e-16 ***\nwaist         0.18013    0.01843   9.774   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 518.24  on 394  degrees of freedom\nResidual deviance: 282.93  on 393  degrees of freedom\n  (8 observations deleted due to missingness)\nAIC: 286.93\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n:::\n:::\n\n## Logistic regression\n\n*Deviance* <br>\n\n-   Deviance is the number that measures the goodness of fit of a logistic regression model.\n-   We use saturated and residual deviance to assess model, instead of $R^2$ or $R^2(adj)$.\n-   We can also use deviance to check the association between explanatory variable and the outcome\n-   In the **likelihood ratio test** the test statistics is the **deviance** for the full model minus the deviance for the full model excluding the relevant explanatory variable. This test statistics follow a Chi-squared distribution with 1 degree of freedom.\n\n## Logistic regression\n\n*Odds ratio* <br>\n\n::: columns\n::: {.column width=\"40%\"}\n-   In logistic regression we often interpret the model coefficients by taking $e^{\\hat{\\beta}}$\n-   and we talk about **odd ratios**.\n-   For instance we can say, given our above model, $e^{0.18} = 1.2$ that for each unit increase in `waist` the odds of suffering from obesity get multiplied by 1.2.\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n``` r\n# fit logistic regression model\nlogmodel_1 <- glm(obese ~ waist, family = binomial(link=\"logit\"), data = data_diabetes)\n\n# print model summary\nprint(summary(logmodel_1))\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = obese ~ waist, family = binomial(link = \"logit\"), \n    data = data_diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -18.29349    1.83541  -9.967   <2e-16 ***\nwaist         0.18013    0.01843   9.774   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 518.24  on 394  degrees of freedom\nResidual deviance: 282.93  on 393  degrees of freedom\n  (8 observations deleted due to missingness)\nAIC: 286.93\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n:::\n:::\n\n## Common GLM models\n\n<br>\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper lightable-hover table\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto; font-size: 20px; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Type.of.outcome </th>\n   <th style=\"text-align:left;\"> Type.of.GLM </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Continous numerical </td>\n   <td style=\"text-align:left;\"> Simple or multiple linear </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Binary </td>\n   <td style=\"text-align:left;\"> Logistic </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Categorical outcome with more than two groups </td>\n   <td style=\"text-align:left;\"> Multinomial or ordinal logistic regression </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Event rate or count </td>\n   <td style=\"text-align:left;\"> Poisson </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Time to event </td>\n   <td style=\"text-align:left;\"> Exponential </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n# Logistic Lasso\n\n## Logistic Lasso\n\n*Logistic regression + Lasso regularization*\n\n<br>\n\n-   Logistic Lasso combines logistic regression with Lasso regularization to analyze binary outcome data while simultaneously performing variable selection and regularization.\n-   We estimate set of coefficients $\\hat \\beta$ that minimize the combined logistic loss function and the Lasso penalty:\n\n$$\n\\left( \\sum_{i=1}^n [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)] \\right) + \\lambda \\sum_{j=1}^p |\\beta_j|\n$$\n\nwhere:\n\n-   $y_i$ represents the binary outcome (0 or 1) for the ( i )-th observation.\n-   $p_i$ is the predicted probability of $y_i = 1$ given by the logistic model $p_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip})}}$.\n-   $\\lambda$ is the regularization parameter that controls the strength of the Lasso penalty $\\lambda \\sum_{j=1}^p |\\beta_j|$, which encourages sparsity in the coefficients $\\beta_j$ by shrinking some of them to zero.\n-   $n$ is the number of observations, and $p$ is the number of predictors.\n\n# Common cases\n\n-   Let's go over common linear models cases and some exercises.\n",
    "supporting": [
      "session-lm-presentation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"session-lm-presentation_files/libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"session-lm-presentation_files/libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}