{
  "hash": "50f21388299d6511035ab63402401067",
  "result": {
    "markdown": "---\ntitle: \"Introduction to linear models\"\n# author: Olga Dethlefsen\nformat: \n  revealjs:\n    slide-number: true\n    theme: [default, custom.scss]\n    chalkboard: \n      buttons: true\n  html:\n    code-fold: false\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n\n\n# We will learn\n\n*before lunch*\n\n- what are linear models\n- how to estimate model parameters\n- hypothesis testing on linear regression\n- evaluating how well model fits the data\n- model assumptions\n\n*after lunch*\n\n- linear model selection and regularization\n- GLM with logistic regression\n\n\n## Why linear models?\n\n. . .\n\n::: columns\n::: {.column width=\"50%\"}\nWith linear models we can answer questions such as:\n\n::: incremental\n-   is there a relationship between exposure and outcome, e.g. height and weight?\n-   how strong is the relationship between the two variables?\n-   what will be a predicted value of the outcome given a new set of exposure values?\n-   which variables are associated with the response, e.g. is it height that dictates the weight or height and age?\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-2-1.png){width=576}\n:::\n:::\n\n:::\n:::\n\n## Statistical vs. deterministic relationship\n\n. . .\n\nRelationships in probability and statistics can generally be one of three things:\n\n::: incremental\n-   deterministic\n-   random\n-   statistical\n:::\n\n## Statistical vs. deterministic relationship {.smaller}\n\n*deterministic*\n\nA **deterministic** relationship involves **an exact relationship** between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation $Fahrenheit=\\frac{9}{5}\\cdot Celcius+32$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n## Statistical vs. deterministic relationship {.smaller}\n\n*random*\n\nThere is **no relationship** between variables in the **random relationship**, e.g. number of plants Olga buys and time of the year as Olga buys plants whenever she feels like it throughout the entire year\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n## Statistical vs. deterministic relationship {.smaller}\n\n*statistical relationship = deterministic + random*\n\n**A statistical relationship** is a **mixture of deterministic and random relationship**, e.g. the savings that Olga has left in the bank account depend on Olga's monthly salary income (deterministic part) and the money spent on buying plants (random part)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## What linear models are and are not\n\n*definition*\n\n<br>\n\n::: columns\n::: {.column width=\"60%\"}\n::: incremental\n- In an linear model we model the relationship between a single continuous variable $Y$ and one or more variables $X$.\n- One very general form for the model would be: $$Y = f(X_1, X_2, \\dots X_p) + \\epsilon$$ where $f$ is some unknown function and $\\epsilon$ is the error in this representation.\n- The $X$ variables can be numerical, categorical or a mixture of both.\n\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"35%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-6-1.png){width=576}\n:::\n:::\n\n:::\n:::\n\n<!-- . . . -->\n\n<!-- -   For instance a **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \\beta \\cdot x + \\epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled. -->\n\n\n## What linear models are and are not\n\n*definition*\n\n<br>\n\n::: incremental\n- Formally, linear models are a way of describing a response variable in terms of **linear combination** of predictor variables.\n- Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared\n:::\n\n\n<!-- ## What linear models are and are not -->\n\n<!-- *definition* -->\n\n<!-- <br> -->\n\n<!-- ::: incremental -->\n<!-- -   Formally, a linear model is one in which the **parameters appear linearly in the deterministic part of the model**. -->\n<!-- -   For instance these are linear models: $$Y_i = \\alpha + \\beta \\cdot x_i + \\gamma \\cdot y_i + \\epsilon_i$$ or $$Y_i = \\alpha + \\beta \\cdot x_i^2 \\epsilon$$ -->\n<!-- -   vs. an example on a non-linear model where parameter $\\beta$ appears in the exponent of $x_i$: $$Y_i = \\alpha + x_i^\\beta + \\epsilon$$ -->\n<!-- -   Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared -->\n<!-- ::: -->\n\n## What linear models are and are not\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## What linear models are and are not\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n<!-- ## Terminology -->\n\n<!-- There are many terms and notations used interchangeably -->\n\n<!-- ::: incremental -->\n<!-- -   $y$ is being called: -->\n<!--     -   response -->\n<!--     -   outcome -->\n<!--     -   dependent variable -->\n<!-- -   $x$ is being called: -->\n<!--     -   exposure -->\n<!--     -   explanatory variable -->\n<!--     -   dependent variable -->\n<!--     -   predictor -->\n<!--     -   covariate -->\n<!-- ::: -->\n\n## Simple linear regression\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n::: incremental\n-   It is used to check the association between **the numerical outcome and one numerical explanatory variable**\n-   In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n:::\n\n## Simple linear regression\n\n::: {#exm-simple-lm}\n## Weight and plasma volume\n\nLet's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\n\n\n::: {.cell}\n\n:::\n\n\n``` r\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n```\n:::\n\n. . .\n\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*.](session-lm-presentation_files/figure-revealjs/fig-lm-intro-example-1.png){#fig-lm-intro-example width=384}\n:::\n:::\n\n\n\n\n## Simple linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-lm-presentation_files/figure-revealjs/fig-lm-01-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-02-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-03-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-03b-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-04-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-05-1.png){.fragment width=\"600\" height=\"600\"}\n:::\n\n## Simple linear regression\n\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable](session-lm-presentation_files/figure-revealjs/fig-lm-example-reg-1.png){#fig-lm-example-reg width=960}\n:::\n:::\n\n\n. . .\n\nThe equation for the red line is: $$Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8$$\n\n. . .\n\nand in general: $$Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n$$ {#eq-lm-no-error}\n\n. . .\n\n## Simple linear regression\n\nIn other words, by finding the best-fitting straight line we are **building a statistical model** to represent the relationship between plasma volume ($Y$) and explanatory body weight variable ($x$)\n\n## Simple linear regression {.smaller}\n\n<br>\n\n``` r\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n```\n\n<br>\n\n::: columns\n::: {.column width=\"55%\"}\n::: incremental\n-   If we were to use our model $Y_i=0.086 + 0.044 \\cdot x_i$ to find plasma volume given a weight of 58 kg (our first observation, $i=1$)\n-   we would notice that we would get $Y=0.086 + 0.044 \\cdot 58 = 2.638$\n-   $2.638$ is not exactly the as same as $2.75$, the first measurement we have in our dataset, i.e. $2.75 - 2.638 = 0.112 \\neq 0$.\n-   We thus add to the previous equation (@eq-lm-no-error) an **error term** to account for this and now we can write our **simple regression model** more formally as:\n-   $$Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$$ {#eq-lm} where:\n-   $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n-   $y$: is called: response, outcome, dependent variable\n-   $\\alpha$ and $\\beta$ are **model coefficients**\n-   and $\\epsilon_i$ is an **error terms**\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Least squares\n\n::: incremental\n-   in the above **\"body weight - plasma volume\"** example, the values of $\\alpha$ and $\\beta$ have just appeared\n-   in practice, $\\alpha$ and $\\beta$ values are **unknown** and we use data to **estimate these coefficients**, noting the estimates with a **hat**, $\\hat{\\alpha}$ and $\\hat{\\beta}$\n-   **least squares** is one of the methods of parameters estimation, i.e. finding $\\hat{\\alpha}$ and $\\hat{\\beta}$\n:::\n\n## Least squares {.smaller}\n\n*minimizing RSS*\n\n::: columns\n::: {.column width=\"50%\"}\nLet $\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:\n\n-   Then $\\epsilon_i = y_i - \\hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model\n-   RSS, the **residual sum of squares** is defined as: $$RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2$$ or equivalently as: $$RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2$$\n-   the least squares approach chooses $\\hat{\\alpha}$ and $\\hat{\\beta}$ **to minimize the RSS**.\n:::\n\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice versa*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line](session-lm-presentation_files/figure-revealjs/fig-reg-errors-1.png){#fig-reg-errors width=768}\n:::\n:::\n\n:::\n:::\n\n<!-- ## Least squares -->\n\n<!-- ::: {#thm-lss} -->\n<!-- ## Least squares estimates for a simple linear regression -->\n\n<!-- $$\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}$$ $$\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}$$ -->\n\n<!-- where: -->\n\n<!-- -   $\\bar{x}$: mean value of $x$ -->\n<!-- -   $\\bar{y}$: mean value of $y$ -->\n<!-- -   $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2$ -->\n<!-- -   $S_{yy}$: sum of squares of $Y$ defined as $S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2$ -->\n<!-- -   $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$ -->\n<!-- ::: -->\n\n<!-- ## Least squares -->\n\n<!-- *Live demo* -->\n\n<!-- ::: {#exm-lss} -->\n<!-- ## Least squares -->\n\n<!-- Let's try least squares method to find coefficient estimates in the **\"body weight and plasma volume example\"** -->\n\n<!-- ``` r -->\n<!-- # initial data -->\n<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->\n<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->\n\n<!-- # rename variables for convenience -->\n<!-- x <- weight -->\n<!-- y <- plasma -->\n\n<!-- # mean values of x and y -->\n<!-- x.bar <- mean(x) -->\n<!-- y.bar <- mean(y) -->\n\n<!-- # Sum of squares -->\n<!-- Sxx <-  sum((x - x.bar)^2) -->\n<!-- Sxy <- sum((x-x.bar)*(y-y.bar)) -->\n\n<!-- # Coefficient estimates -->\n<!-- beta.hat <- Sxy / Sxx -->\n<!-- alpha.hat <- y.bar - Sxy/Sxx*x.bar -->\n\n<!-- # Print estimated coefficients alpha and beta -->\n<!-- print(alpha.hat) -->\n<!-- print(beta.hat) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| code-fold: false -->\n\n<!-- # initial data -->\n<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->\n<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->\n\n<!-- # rename variables for convenience -->\n<!-- x <- weight -->\n<!-- y <- plasma -->\n\n<!-- # mean values of x and y -->\n<!-- x.bar <- mean(x) -->\n<!-- y.bar <- mean(y) -->\n\n<!-- # Sum of squares -->\n<!-- Sxx <-  sum((x - x.bar)^2) -->\n<!-- Sxy <- sum((x-x.bar)*(y-y.bar)) -->\n\n<!-- # Coefficient estimates -->\n<!-- beta.hat <- Sxy / Sxx -->\n<!-- alpha.hat <- y.bar - Sxy/Sxx*x.bar -->\n\n<!-- # Print estimated coefficients alpha and beta -->\n<!-- print(alpha.hat) -->\n<!-- print(beta.hat) -->\n\n<!-- ``` -->\n<!-- ::: -->\n\n\n\n## Slope\n\n$plasma = 0.0857 + 0.0436 * weight$\n\nLinear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n*Increasing weight by 5 kg corresponds to* $3.14 - 2.92 = 0.22$ increase in plasma volume. Increasing weight by 1 kg corresponds $2.96 - 2.92 = 0.04$ increase in plasma volume\n\n## Intercept\n\n$plasma = 0.0857 + 0.0436 * weight$\n\nLinear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n*Intercept value corresponds to expected outcome when the explanatory variable value equals to zero. It is not always meaningful*\n\n## Hypothesis testing\n\n**Is there a relationship between the response and the predictor?**\n\n::: incremental\n-   the calculated $\\hat{\\alpha}$ and $\\hat{\\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**\n-   their precision is measured by their **estimated standard errors**, `e.s.e`($\\hat{\\alpha}$) and `e.s.e`($\\hat{\\beta}$)\n-   these estimated standard errors are used in **hypothesis testing** and in constructing **confidence and prediction intervals**\n:::\n\n## Hypothesis testing {.smaller}\n\n**The most common hypothesis test** involves testing the `null hypothesis` of:\n\n-   $H_0:$ There is no relationship between $X$ and $Y$\n-   versus the `alternative hypothesis` $H_a:$ there is some relationship between $X$ and $Y$\n\n. . .\n\n**Mathematically**, this corresponds to testing:\n\n-   $H_0: \\beta=0$\n-   versus $H_a: \\beta\\neq0$\n-   since if $\\beta=0$ then the model $Y_i=\\alpha+\\beta x_i + \\epsilon_i$ reduces to $Y=\\alpha + \\epsilon_i$\n\n. . .\n\n**Under the null hypothesis** $H_0: \\beta = 0$ <!-- we have: $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)$$  --> ![](figures/linear-models/lm-tstatistics.png)\n\n-   $n$ is number of observations\n-   $p$ is number of model parameters\n-   $\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}$ is the ratio of the departure of the estimated value of a parameter, $\\hat\\beta$, from its hypothesized value, $\\beta$, to its standard error, called `t-statistics`\n-   the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom\n\n## Hypothesis testing {.smaller}\n\n::: {#exm-hypothesis-testing}\n## Hypothesis testing\n\nLet's look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the `R summary` of the model\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n```\n:::\n:::\n\n:::\n\n. . .\n\n-   Under `Estimate` we see estimates of our model coefficients, $\\hat{\\alpha}$ (intercept) and $\\hat{\\beta}$ (slope, here weight), followed by their estimated standard errors, `Std. Errors`\n\n. . .\n\n-   If we were to test if there is an **association between weight and plasma volume** we would write under the null hypothesis $H_0: \\beta = 0$ $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582$$\n\n. . .\n\n-   and we would **compare** `t-statistics` to `Student's t distribution` with $n-p = 8 - 2 = 6$ degrees of freedom (as we have 8 observations and two model parameters, $\\alpha$ and $\\beta$)\n\n. . .\n\n-   we can use **Student's t distribution table** or **R code** to obtain the associated *P*-value\n\n``` r\n2*pt(2.856582, df=6, lower=F)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02893095\n```\n:::\n:::\n\n\n. . .\n\n-   here the observed t-statistics is large and therefore yields a small *P*-value, meaning that **there is sufficient evidence to reject null hypothesis in favor of the alternative** and conclude that there is a significant association between weight and plasma volume\n\n## Vector-matrix notations {.scrollable .smaller}\n\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of **multiple regression**, with more than one explanatory variable in the model, it is **more efficient to use vectors and matrices to define the regression model**.\n\n. . .\n\nLet's **rewrite** our simple linear regression model $Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n$ **into vector-matrix notation** in **6 steps**.\n\n. . .\n\nStep 1. First we rename our $\\alpha$ to $\\beta_0$ and $\\beta$ to $\\beta_1$ as it is easier to keep tracking the number of model parameters this way\n\n. . .\n\nStep 2. Then we notice that we actually have $n$ equations such as:\n\n$$y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1$$ $$y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2$$ $$y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3$$ $$\\dots$$ $$y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n$$\n\n. . .\n\nStep 3. We group all $Y_i$ and $\\epsilon_i$ into column vectors: $\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}$ and $\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}$\n\n. . .\n\nStep 4. We stack two parameters $\\beta_0$ and $\\beta_1$ into another column vector:$$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$$\n\n. . .\n\nStep 5. We append a vector of ones with the single predictor for each $i$ and create a matrix with two columns called **design matrix** $$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}$$\n\n. . .\n\nStep 6. We write our linear model in a vector-matrix notations as: $$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n## Vector-matrix notations {.smaller}\n\n::: {#def-vector-matrix-lm}\n## vector matrix form of the linear model\n\nThe vector-matrix representation of a linear model with $p-1$ predictors can be written as $$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nwhere:\n\n-   $\\mathbf{Y}$ is $n \\times1$ vector of observations\n-   $\\mathbf{X}$ is $n \\times p$ **design matrix**\n-   $\\boldsymbol\\beta$ is $p \\times1$ vector of parameters\n-   $\\boldsymbol\\epsilon$ is $n \\times1$ vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, $\\sigma^2$)\n\nIn full, the above vectors and matrix have the form:\n\n$\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}$ $\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix}$ $\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}$ $\\mathbf{X}=\\begin{bmatrix} 1 & x_{1,1} & \\dots & x_{1,p-1} \\\\ 1 & x_{2,1} & \\dots & x_{2,p-1} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_{n,1} & \\dots & x_{n,p-1} \\end{bmatrix}$\n:::\n\n## Vector-matrix notations\n\n::: {#thm-lss-vector-matrix}\n## Least squares in vector-matrix notation\n\nThe least squares estimates for a linear regression of the form: $$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nis given by: $$\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n:::\n\n## Vector-matrix notations {.smaller .scrollable}\n\n::: {#exm-vector-matrix-notation}\n## vector-matrix-notation\n\nFollowing the above definition we can write the **\"weight - plasma volume model\"** as: $$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$ where:\n\n$\\mathbf{Y}=\\begin{bmatrix} 2.75 \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12 \\end{bmatrix}$\n\n$\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ $\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{8} \\end{bmatrix}$ $\\mathbf{X}=\\begin{bmatrix} 1 & 58.0 \\\\ 1 & 70.0 \\\\ 1 & 74.0 \\\\ 1 & 63.5 \\\\ 1 & 62.0 \\\\ 1 & 70.5 \\\\ 1 & 71.0 \\\\ 1 & 66.0 \\\\ \\end{bmatrix}$\n\nand we can estimate model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n:::\n\n## Vector-matrix notations: least squares\n\n*Live demo*\n\nEstimating model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n\n``` r\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\nprint(X)\n\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n       0.08572428\nweight 0.04361534\n```\n:::\n:::\n\n\n\n## Vector-matrix notations: least squares\n\n*Live demo*\n\nIn R we can use `lm()`, the built-in function, to fit a linear regression model and we can replace the above code with one line\n\n``` r\nlm(plasma ~ weight)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nCoefficients:\n(Intercept)       weight  \n    0.08572      0.04362  \n```\n:::\n:::\n\n\n<!-- ## Confidence intervals and prediction intervals -->\n\n<!-- - when we estimate coefficients we can also find their **confidence intervals**, typically 95\\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter -->\n\n<!-- - we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval -->\n\n<!-- ::: {exm-prediction-and-intervals} -->\n\n<!-- ## prediction and intervals -->\n\n<!-- Let's: -->\n\n<!-- - find confidence intervals for our coefficient estimates -->\n\n<!-- - predict plasma volume for a men weighting 60 kg -->\n\n<!-- - find prediction interval -->\n\n<!-- - plot original data, fitted regression model, predicted observation and prediction interval -->\n\n<!-- ```{r} -->\n\n<!-- #| code-fold: false -->\n\n<!-- # fit regression model -->\n\n<!-- model <- lm(plasma ~ weight) -->\n\n<!-- print(summary(model)) -->\n\n<!-- # find confidence intervals for the model coefficients -->\n\n<!-- confint(model) -->\n\n<!-- # predict plasma volume for a new observation of 60 kg -->\n\n<!-- # we have to create data frame with a variable name matching the one used to build the model -->\n\n<!-- new.obs <- data.frame(weight = 60) -->\n\n<!-- predict(model, newdata = new.obs) -->\n\n<!-- # find prediction intervals -->\n\n<!-- prediction.interval <- predict(model, newdata = new.obs,  interval = \"prediction\") -->\n\n<!-- print(prediction.interval) -->\n\n<!-- # plot the original data, fitted regression and predicted value -->\n\n<!-- plot(weight, plasma, pch=19, xlab=\"weight [kg]\", ylab=\"plasma [l]\", ylim=c(2,4)) -->\n\n<!-- lines(weight, model$fitted.values, col=\"red\") # fitted model in red -->\n\n<!-- points(new.obs, predict(model, newdata = new.obs), pch=19, col=\"blue\") # predicted value at 60kg -->\n\n<!-- segments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval -->\n\n<!-- ``` -->\n\n<!-- ## References -->\n\n<!-- ::: {#refs} -->\n\n<!-- ::: -->\n\n# Assessing model fit\n\n-   In addition to knowing how to estimate model parameters we need to learn to assess the goodness of fit of a model.\n-   We do that by calculating the amount of variability in the response that is explained by the model.\n\n## $R^2$: summary of the fitted model\n\n*TSS*\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-lm-presentation_files/figure-revealjs/fig-lm-fit-00-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-fit-01-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-fit-02-1.png){.fragment width=\"600\" height=\"600\"}\n:::\n\n## $R^2$: summary of the fitted model\n\n*RSS*\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-lm-presentation_files/figure-revealjs/fig-lm-rss-01-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-rss-02-1.png){.fragment width=\"600\" height=\"600\"}\n\n![](session-lm-presentation_files/figure-revealjs/fig-lm-rss-03-1.png){.fragment width=\"600\" height=\"600\"}\n:::\n\n## $R^2$: summary of the fitted model\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-33-1.png){width=768}\n:::\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n<br>\n\n-   **TSS**, denoted **Total corrected sum-of-squares** is the residual sum-of-squares for Model 0 $$S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}$$ corresponding the to the sum of squared distances to the purple line\n\n-   **RSS**, the residual sum-of-squares, is defined as: $$RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$ and corresponds to the squared distances between the observed values $y_i, \\dots,y_n$ to fitted values $\\hat{y_1}, \\dots \\hat{y_n}$, i.e. distances to the red fitted line\n:::\n:::\n\n## $R^2$: summary of the fitted model\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/unnamed-chunk-34-1.png){width=768}\n:::\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n<br>\n\n::: {#def-r2}\n## $R^2$\n\nA simple but useful measure of model fit is given by $$R^2 = 1 - \\frac{RSS}{TSS}$$ where:\n\n-   RSS is the residual sum-of-squares for Model 1, the fitted model of interest\n-   TSS is the sum of squares of the **null model**\n-   $R^2$ is also referred as **coefficient of determination**\n-   $R^2$ is expressed on a scale, as a proportion between 0 and 1 of the total variation in the data.\n:::\n:::\n:::\n\n<!-- ## $R^2$: summary of the fitted model -->\n\n<!-- ::: incremental -->\n\n<!-- -   $R^2$ quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model -->\n\n<!-- -   $R^2$ is also referred as **coefficient of determination** -->\n\n<!-- -   It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data -->\n\n<!-- -   Values of $R^2$ approaching 1 indicate the model to be a good fit -->\n\n<!-- -   Values of $R^2$ less than 0.5 suggest that the model gives rather a poor fit to the data -->\n\n<!-- ::: -->\n\n## $R^2$ and correlation coefficient\n\n. . .\n\n::: {#thm-r2}\n## $R^2$\n\nIn the case of simple linear regression:\n\nModel 1: $Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$ $$R^2 = r^2$$ where:\n\n-   $R^2$ is the coefficient of determination\n-   $r^2$ is the sample correlation coefficient\n:::\n\n::: notes\nWhat other coefficient is expressed on a 0-1 scale?\n:::\n\n## $R^2(adj)$\n\n::: incremental\n-   In the case of multiple linear regression we are using the **adjusted version of** $R^2$ to assess the model fit as the number of explanatory variables increase, $R^2$ also increases.\n:::\n\n. . .\n\n::: {#thm-r2adj}\n## $R^2(adj)$\n\nFor any multiple linear regression $$Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i$$ $R^2(adj)$ is defined as $$R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}$$\n\n-   $p$ is the number of independent predictors, i.e. the number of variables in the model, excluding the constant and $n$ is number of observations.\n\n$R^2(adj)$ can also be calculated from $R^2$: $$R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}$$\n:::\n\n\n\n<!-- ## $R^2$ -->\n\n<!-- *Live demo* -->\n\n<!-- ``` r -->\n\n<!-- htwtgen <- read.csv(\"data/lm/heights_weights_gender.csv\") -->\n\n<!-- head(htwtgen) -->\n\n<!-- attach(htwtgen) -->\n\n<!-- ## Simple linear regression -->\n\n<!-- model.simple <- lm(Height ~ Weight, data=htwtgen) -->\n\n<!-- # TSS -->\n\n<!-- TSS <- sum((Height - mean(Height))^2) -->\n\n<!-- # RSS -->\n\n<!-- # residuals are returned in the model type names(model.simple) -->\n\n<!-- RSS <- sum((model.simple$residuals)^2) -->\n\n<!-- R2 <- 1 - (RSS/TSS) -->\n\n<!-- print(R2) -->\n\n<!-- print(summary(model.simple)) -->\n\n<!-- ``` -->\n\n<!-- ## $R^2$ -->\n\n<!-- *Live demo* -->\n\n<!-- ``` r -->\n\n<!-- htwtgen <- read.csv(\"data/lm/heights_weights_gender.csv\") -->\n\n<!-- head(htwtgen) -->\n\n<!-- attach(htwtgen) -->\n\n<!-- ## Multiple regression -->\n\n<!-- model.multiple <- lm(Height ~ Weight + Gender, data=htwtgen) -->\n\n<!-- n <- length(Weight) -->\n\n<!-- p <- 1 -->\n\n<!-- RSS <- sum((model.multiple$residuals)^2) -->\n\n<!-- R2_adj <- 1 - (RSS/(n-p-1))/(TSS/(n-1)) -->\n\n<!-- print(R2_adj) -->\n\n<!-- print(summary(model.multiple)) -->\n\n<!-- ``` -->\n\n# Checking model assumptions\n\n## The assumptions of a linear model\n\n::: incremental\n-   Before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data.\n-   Informally we have been using box plots and scatter plots to look at the data.\n-   There are however formal assumptions that should be met when fitting linear models.\n:::\n\n## The assumptions of a linear model\n\n. . .\n\n<br>\n\n**Linearity:**\n\n-   The relationship between $X$ and $Y$ is linear\n\n. . .\n\n**Independence of errors**\n\n-   $Y$ is independent of errors, there is no relationship between the residuals and $Y$\n\n. . .\n\n**Normality of errors**\n\n-   The residuals must be approximately normally distributed\n\n. . .\n\n**Equal variances**\n\n-   The variance of the residuals is the same for all values of $X$\n\n## Checking assumptions {.smaller}\n\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**.\n\n. . .\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Modeling BMI with waist explanatory variable. Model assumptions seems to be met with no noticible patterns based on residuals. Few potential outliers could be considered to be removed.](session-lm-presentation_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\n## Checking assumptions {.smaller}\n\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**.\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Modeling glucose with age. Model assumptions seems to be violated, e.g. with residulas not following normal distributions.](session-lm-presentation_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\n# Exercises\n\nLet's practice in exercises fitting linear models, hypothesis testing and assessing model fit.\n\n# Beyond linear models\n\n-   Generalized linear models\n\n## Why Generalized Linear Models\n\n*GLM*\n\n::: columns\n::: {.column width=\"50%\"}\n-   GLMs extend linear model framework to outcome variables that do not follow normal distribution.\n-   They are most frequently used to model binary, categorical or count data.\n-   For instance, fitting a regression line to binary data yields predicted values that could take any value, including $<0$,\n-   not to mention that it is hard to argue that the values of 0 and 1s are normally distributed.\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell fig-cap-location='bottom'}\n::: {.cell-output-display}\n![Example of fitting linear model to binary data, to model obesity status coded as 1 (Yes) and 0 (No) with waist variable. Linear model does not fit the data well in this case and the predicted values can take any value, not only 0 and 1.](session-lm-presentation_files/figure-revealjs/log-example-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Logistic regression\n\n::: columns\n::: {.column width=\"50%\"}\n-   Since the response variable takes only two values (Yes/No) we use GLM model\n-   to fit **logistic regression** model for the **probability of suffering from obesity (Yes).**\n-   We let $p_i=P(Y_i=1)$ denote the probability of suffering from obesity (success)\n-   and we assume that the response follows binomial distribution: $Y_i \\sim Bi(1, p_i)$ distribution.\n-   We can then write the regression model now as: $$log(\\frac{p_i}{1-p_i})=\\beta_0 + \\beta_1x_i$$ and given the properties of logarithms this is also equivalent to: $$p_i = \\frac{exp(\\beta_0 + \\beta_1x_i)}{1 + exp(\\beta_0 + \\beta_1x_i)}$$\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](session-lm-presentation_files/figure-revealjs/fig-obesity-1.png){#fig-obesity width=960}\n:::\n:::\n\n:::\n:::\n\n## Logistic regression\n\n<br>\n\n-   In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a **link function**.\n-   Here, the **link function** $log(\\frac{p_i}{1-p_i})$ provides the link between the binomial distribution of $Y_i$ (suffering from obesity) and the linear predictor (waist)\n-   Thus the **GLM model** can be written as $$g(\\mu_i)=\\mathbf{X}\\boldsymbol\\beta$$ where `g()` is the link function.\n\n## Logistic regression\n\n*R* <br>\n\nIn R we can use `glm()` function to fit GLM models:\n\n``` r\n# fit logistic regression model\nlogmodel_1 <- glm(obese ~ waist, family = binomial(link=\"logit\"), data = data_diabetes)\n\n# print model summary\nprint(summary(logmodel_1))\n```\n\n\n::: {.cell}\n\n```\n## \n## Call:\n## glm(formula = obese ~ waist, family = binomial(link = \"logit\"), \n##     data = data_diabetes)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -17.357      2.973  -5.837 5.30e-09 ***\n## waist         17.174      2.974   5.775 7.71e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 102.79  on 128  degrees of freedom\n## AIC: 106.79\n## \n## Number of Fisher Scoring iterations: 5\n```\n:::\n\n\n## Logistic regression\n\n*R* <br>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Fitted logistic model to diabetes data given the 130 study participants and using waist as explantatory variable to model obesity status.](session-lm-presentation_files/figure-revealjs/unnamed-chunk-40-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Logistic regression\n\n*Hypothesis testing* <br>\n\n::: columns\n::: {.column width=\"40%\"}\n-   Similarly to linear models, we can determine whether each variable is related to the outcome of interest by testing the null hypothesis that the relevant logistic regression coefficient is zero.\n-   This can be performed by **Wald test** which is equals to estimated logistic regression coefficient divided by its standard error and follows the Standard Normal distribution: $$W = \\frac{\\hat\\beta-\\beta}{e.s.e.(\\hat\\beta)} \\sim N(0,1)$$\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n``` r\n# fit logistic regression model\nlogmodel_1 <- glm(obese ~ waist, family = binomial(link=\"logit\"), data = data_diabetes)\nsummary(logmodel_1)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = obese ~ waist, family = binomial(link = \"logit\"), \n    data = data_diabetes)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -17.357      2.973  -5.837 5.30e-09 ***\nwaist         17.174      2.974   5.775 7.71e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 178.71  on 129  degrees of freedom\nResidual deviance: 102.79  on 128  degrees of freedom\nAIC: 106.79\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n:::\n:::\n\n## Logistic regression\n\n*Deviance* <br>\n\n-   Deviance is the number that measures the goodness of fit of a logistic regression model.\n-   We use saturated and residual deviance to assess model, instead of $R^2$ or $R^2(adj)$.\n-   We can also use deviance to check the association between explanatory variable and the outcome\n-   In the **likelihood ratio test** the test statistics is the **deviance** for the full model minus the deviance for the full model excluding the relevant explanatory variable. This test statistics follow a Chi-squared distribution with 1 degree of freedom.\n\n## Logistic regression\n\n*Odds ratio* <br>\n\n::: columns\n::: {.column width=\"40%\"}\n-   In logistic regression we often interpret the model coefficients by taking $e^{\\hat{\\beta}}$\n-   and we talk about **odd ratios**.\n-   For instance we can say, given our above model, $e^{17.174} = 28745736$ that for each unit increase in `waist` the odds of suffering from obesity get multiplied by 28745736.\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n``` r\n# fit logistic regression model\nlogmodel_1 <- glm(obese ~ waist, family = binomial(link=\"logit\"), data = data_diabetes)\n\n# print model summary\nprint(summary(logmodel_1))\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = obese ~ waist, family = binomial(link = \"logit\"), \n    data = data_diabetes)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -17.357      2.973  -5.837 5.30e-09 ***\nwaist         17.174      2.974   5.775 7.71e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 178.71  on 129  degrees of freedom\nResidual deviance: 102.79  on 128  degrees of freedom\nAIC: 106.79\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n:::\n:::\n\n## Common GLM models\n\n<br>\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper lightable-hover table\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto; font-size: 20px; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Type.of.outcome </th>\n   <th style=\"text-align:left;\"> Type.of.GLM </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Continous numerical </td>\n   <td style=\"text-align:left;\"> Simple or multiple linear </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Binary </td>\n   <td style=\"text-align:left;\"> Logistic </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Categorical outcome with more than two groups </td>\n   <td style=\"text-align:left;\"> Multinomial or ordinal logistic regression </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Event rate or count </td>\n   <td style=\"text-align:left;\"> Poisson </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Time to event </td>\n   <td style=\"text-align:left;\"> Exponential </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n# Common cases\n\n-   Let's go over common linear models cases\n",
    "supporting": [
      "session-lm-presentation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"session-lm-presentation_files/libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"session-lm-presentation_files/libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}