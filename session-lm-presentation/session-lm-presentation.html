<!DOCTYPE html>
<html lang="en"><head>
<script src="session-lm-presentation_files/libs/clipboard/clipboard.min.js"></script>
<script src="session-lm-presentation_files/libs/quarto-html/tabby.min.js"></script>
<script src="session-lm-presentation_files/libs/quarto-html/popper.min.js"></script>
<script src="session-lm-presentation_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="session-lm-presentation_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session-lm-presentation_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="session-lm-presentation_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="session-lm-presentation_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <title>Introduction to linear models</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="session-lm-presentation_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="session-lm-presentation_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="session-lm-presentation_files/libs/revealjs/dist/theme/quarto.css">
  <link href="session-lm-presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="session-lm-presentation_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="session-lm-presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="session-lm-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="session-lm-presentation_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="session-lm-presentation_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  
  <script src="session-lm-presentation_files/libs/kePrint-0.0.1/kePrint.js"></script>
  <link href="session-lm-presentation_files/libs/lightable-0.0.1/lightable.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Introduction to linear models</h1>

<div class="quarto-title-authors">
</div>

</section>
<section>
<section id="we-will-learn" class="title-slide slide level1 center">
<h1>We will learn</h1>
<p><em>before lunch</em></p>
<ul>
<li>what are linear models</li>
<li>how to estimate model parameters</li>
<li>hypothesis testing on linear regression</li>
<li>evaluating how well model fits the data</li>
<li>model assumptions</li>
</ul>
<p><em>after lunch</em></p>
<ul>
<li>linear model selection and regularization</li>
<li>GLM with logistic regression</li>
</ul>
</section>
<section id="why-linear-models" class="slide level2">
<h2>Why linear models?</h2>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<p>With linear models we can answer questions such as:</p>
<div>
<ul>
<li class="fragment">is there a relationship between exposure and outcome, e.g.&nbsp;height and weight?</li>
<li class="fragment">how strong is the relationship between the two variables?</li>
<li class="fragment">what will be a predicted value of the outcome given a new set of exposure values?</li>
<li class="fragment">which variables are associated with the response, e.g.&nbsp;is it height that dictates the weight or height and age?</li>
</ul>
</div>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:45%;">
<div class="cell">
<div class="cell-output-display">
<p><img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-2-1.png" width="576"></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="statistical-vs.-deterministic-relationship" class="slide level2">
<h2>Statistical vs.&nbsp;deterministic relationship</h2>
<div class="fragment">
<p>Relationships in probability and statistics can generally be one of three things:</p>
<ul>
<li>deterministic</li>
<li>random</li>
<li>statistical</li>
</ul>
</div>
</section>
<section id="statistical-vs.-deterministic-relationship-1" class="slide level2 smaller">
<h2>Statistical vs.&nbsp;deterministic relationship</h2>
<p><em>deterministic</em></p>
<p>A <strong>deterministic</strong> relationship involves <strong>an exact relationship</strong> between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation <span class="math inline">\(Fahrenheit=\frac{9}{5}\cdot Celcius+32\)</span></p>
<div class="fragment">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-3-1.png" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="statistical-vs.-deterministic-relationship-2" class="slide level2 smaller">
<h2>Statistical vs.&nbsp;deterministic relationship</h2>
<p><em>random</em></p>
<p>There is <strong>no relationship</strong> between variables in the <strong>random relationship</strong>, e.g.&nbsp;number of plants Olga buys and time of the year as Olga buys plants whenever she feels like it throughout the entire year</p>

<img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-4-1.png" width="576" class="r-stretch quarto-figure-center"></section>
<section id="statistical-vs.-deterministic-relationship-3" class="slide level2 smaller">
<h2>Statistical vs.&nbsp;deterministic relationship</h2>
<p><em>statistical relationship = deterministic + random</em></p>
<p><strong>A statistical relationship</strong> is a <strong>mixture of deterministic and random relationship</strong>, e.g.&nbsp;the savings that Olga has left in the bank account depend on Olga’s monthly salary income (deterministic part) and the money spent on buying plants (random part)</p>

<img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-5-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="what-linear-models-are" class="slide level2">
<h2>What linear models are</h2>
<p><em>definition</em></p>
<p><br></p>
<div class="columns">
<div class="column" style="width:60%;">
<div>
<ul>
<li class="fragment">In an linear model we model the relationship between a single continuous variable <span class="math inline">\(Y\)</span> and one or more variables <span class="math inline">\(X\)</span>.</li>
<li class="fragment">One very general form for the model would be: <span class="math display">\[Y = f(X_1, X_2, \dots X_p) + \epsilon\]</span> where <span class="math inline">\(f\)</span> is some unknown function and <span class="math inline">\(\epsilon\)</span> is the error in this representation.</li>
<li class="fragment">The <span class="math inline">\(X\)</span> variables can be numerical, categorical or a mixture of both.</li>
</ul>
</div>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:35%;">
<div class="cell">
<div class="cell-output-display">
<p><img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-6-1.png" width="576"></p>
</div>
</div>
</div>
</div>
<!-- . . . -->
<!-- -   For instance a **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \beta \cdot x + \epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled. -->
</section>
<section id="what-linear-models-are-1" class="slide level2">
<h2>What linear models are</h2>
<p><em>definition</em></p>
<p><br></p>
<div>
<ul>
<li class="fragment">Formally, linear models are a way of describing a response variable in terms of <strong>linear combination</strong> of predictor variables.</li>
<li class="fragment">Linear models can become quite advanced by including <strong>many variables</strong>, e.g.&nbsp;the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the <strong>transformation of the variables</strong>, e.g.&nbsp;a function of kilometers cycled squared</li>
</ul>
</div>
<!-- ## What linear models are and are not -->
<!-- *definition* -->
<!-- <br> -->
<!-- ::: incremental -->
<!-- -   Formally, a linear model is one in which the **parameters appear linearly in the deterministic part of the model**. -->
<!-- -   For instance these are linear models: $$Y_i = \alpha + \beta \cdot x_i + \gamma \cdot y_i + \epsilon_i$$ or $$Y_i = \alpha + \beta \cdot x_i^2 \epsilon$$ -->
<!-- -   vs. an example on a non-linear model where parameter $\beta$ appears in the exponent of $x_i$: $$Y_i = \alpha + x_i^\beta + \epsilon$$ -->
<!-- -   Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared -->
<!-- ::: -->
<!-- ## What linear models are and are not -->
<!-- ```{r} -->
<!-- #| warning: false -->
<!-- #| echo: false -->
<!-- #| fig-align: center -->
<!-- #| fig-width: 8 -->
<!-- #| fig-height: 6 -->
<!-- my.ggtheme <- theme_bw() +  -->
<!--   theme(axis.title = element_text(size = font.size),  -->
<!--         axis.text = element_text(size = font.size),  -->
<!--         legend.text = element_text(size = font.size),  -->
<!--         legend.title = element_blank(),  -->
<!--         legend.position = "none",  -->
<!--         axis.title.y=element_text(angle=0)) -->
<!-- # simple linear regression -->
<!-- x <- seq(-10, 10, 1) -->
<!-- set.seed(123) -->
<!-- y <- x + rnorm(length(x), mean(x), 2) -->
<!-- data.xy <- data.frame(x=x, y = y, ymodel = x) -->
<!-- p.simple <-  data.xy %>% ggplot(aes(x = x, y = y)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
<!--   my.ggtheme +  -->
<!--   ggtitle("A") -->
<!-- # simple linear regression with group -->
<!-- x <- seq(0, 10, length.out = 20) -->
<!-- set.seed(123) -->
<!-- y1 <- 0 + x + rnorm(length(x), 0, 2) -->
<!-- set.seed(123) -->
<!-- y2 <- 0 + 4*x + rnorm(length(x), 0, 2) -->
<!-- x.all <- c(x, x) -->
<!-- y.all <- c(y1, y2) -->
<!-- group <- c(rep("CTRL", length(x)), rep("TX", length(x))) -->
<!-- ymodel <- c(0+x, 0+4*x) -->
<!-- data.xy <- data.frame(x=x.all, y = y.all, ymodel = ymodel) -->
<!-- p.group <- data.xy %>% ggplot(aes(x = x, y = y, colour = group)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(x = x, y=ymodel)) +  -->
<!--   theme_classic() + -->
<!--   scale_color_brewer(palette = "Set2") +  -->
<!--   my.ggtheme +  -->
<!--   ggtitle("B")  -->
<!-- # advanced 1 -->
<!-- x <- seq(-10, 10, 1) -->
<!-- set.seed(123) -->
<!-- y <- x^2 + rnorm(length(x), mean(x), 10) -->
<!-- data.xy <- data.frame(x=x, y = y, ymodel = x^2) -->
<!-- p.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
<!--   theme_classic() + -->
<!--   my.ggtheme +  -->
<!--   ggtitle("C") -->
<!-- # advanced 2 -->
<!-- x <- seq(-10, 10, 1) -->
<!-- set.seed(123) -->
<!-- y <- (x + (x^3))/1000 + rnorm(length(x), mean(x), 0.05) -->
<!-- data.xy <- data.frame(x=x, x2=x^2, x3=x^3, y = y, ymodel = (x + x^3)/1000) -->
<!-- p.adv2 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
<!--   theme_classic() + -->
<!--   my.ggtheme +  -->
<!--   ggtitle("D") -->
<!-- grid.arrange(p.simple, p.group, p.adv1, p.adv2, ncol = 2) -->
<!-- ``` -->
<!-- ## What linear models are and are not -->
<!-- ```{r} -->
<!-- #| warning: false -->
<!-- #| echo: false -->
<!-- #| fig-align: center -->
<!-- #| fig-width: 8 -->
<!-- #| fig-height: 6 -->
<!-- my.ggtheme <- theme_bw() +  -->
<!--   theme(axis.title = element_text(size = font.size),  -->
<!--         axis.text = element_text(size = font.size),  -->
<!--         legend.text = element_text(size = font.size),  -->
<!--         legend.title = element_blank(),  -->
<!--         legend.position = "none",  -->
<!--         axis.title.y=element_text(angle=0)) -->
<!-- # simple linear regression -->
<!-- x <- seq(-10, 10, 1) -->
<!-- set.seed(123) -->
<!-- y <- x + rnorm(length(x), mean(x), 2) -->
<!-- data.xy <- data.frame(x=x, y = y, ymodel = x) -->
<!-- p.simple <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
<!--   my.ggtheme +  -->
<!--   ggtitle(TeX(r'(A: $y_i = x_1 + e_i$)')) -->
<!-- # simple linear regression with group -->
<!-- x <- seq(0, 10, length.out = 20) -->
<!-- set.seed(123) -->
<!-- y1 <- 0 + x + rnorm(length(x), 0, 2) -->
<!-- set.seed(123) -->
<!-- y2 <- 0 + 4*x + rnorm(length(x), 0, 2) -->
<!-- x.all <- c(x, x) -->
<!-- y.all <- c(y1, y2) -->
<!-- group <- c(rep("CTRL", length(x)), rep("TX", length(x))) -->
<!-- ymodel <- c(0+x, 0+4*x) -->
<!-- data.xy <- data.frame(x=x.all, y = y.all, ymodel = ymodel) -->
<!-- p.group <- data.xy %>% ggplot(aes(x = x, y = y, colour = group)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(x = x, y=ymodel)) +  -->
<!--   theme_classic() + -->
<!--   scale_color_brewer(palette = "Set2") +  -->
<!--   my.ggtheme +  -->
<!--   ggtitle(TeX(r'(B: $x_1 + I_{x_i} + e_i$)')) -->
<!-- # advanced 1 -->
<!-- x <- seq(-10, 10, 1) -->
<!-- set.seed(123) -->
<!-- y <- x^2 + rnorm(length(x), mean(x), 10) -->
<!-- data.xy <- data.frame(x=x, y = y, ymodel = x^2) -->
<!-- p.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
<!--   theme_classic() + -->
<!--   my.ggtheme +  -->
<!--   ggtitle(TeX(r'(C: $y_i = x_i^2 + e_i$)')) -->
<!-- # advanced 2 -->
<!-- x <- seq(-10, 10, 1) -->
<!-- set.seed(123) -->
<!-- y <- (x + (x^3))/1000 + rnorm(length(x), mean(x), 0.05) -->
<!-- data.xy <- data.frame(x=x, x2=x^2, x3=x^3, y = y, ymodel = (x + x^3)/1000) -->
<!-- p.adv2 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
<!--   theme_classic() + -->
<!--   my.ggtheme +  -->
<!--   ggtitle(TeX(r'(D: $y_i = x + x_i^3 + e_i$)')) -->
<!-- grid.arrange(p.simple, p.group, p.adv1, p.adv2, ncol = 2) -->
<!-- ``` -->
<!-- ## Terminology -->
<!-- There are many terms and notations used interchangeably -->
<!-- ::: incremental -->
<!-- -   $y$ is being called: -->
<!--     -   response -->
<!--     -   outcome -->
<!--     -   dependent variable -->
<!-- -   $x$ is being called: -->
<!--     -   exposure -->
<!--     -   explanatory variable -->
<!--     -   dependent variable -->
<!--     -   predictor -->
<!--     -   covariate -->
<!-- ::: -->
</section>
<section id="simple-linear-regression" class="slide level2">
<h2>Simple linear regression</h2>

<img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-7-1.png" width="960" class="r-stretch quarto-figure-center"><div>
<ul>
<li class="fragment">It is used to check the association between <strong>the numerical outcome and one numerical explanatory variable</strong></li>
<li class="fragment">In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure</li>
</ul>
</div>
</section>
<section id="simple-linear-regression-1" class="slide level2">
<h2>Simple linear regression</h2>
<div id="exm-simple-lm" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Weight and plasma volume) </strong></span>Let’s look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>weight <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">58</span>, <span class="dv">70</span>, <span class="dv">74</span>, <span class="fl">63.5</span>, <span class="fl">62.0</span>, <span class="fl">70.5</span>, <span class="fl">71.0</span>, <span class="fl">66.0</span>) <span class="co"># body weight (kg)</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>plasma <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">2.75</span>, <span class="fl">2.86</span>, <span class="fl">3.37</span>, <span class="fl">2.76</span>, <span class="fl">2.62</span>, <span class="fl">3.49</span>, <span class="fl">3.05</span>, <span class="fl">3.12</span>) <span class="co"># plasma volume (liters)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="simple-linear-regression-2" class="slide level2">
<h2>Simple linear regression</h2>
<div class="r-stack">
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-01-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-02-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-03-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-03b-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-04-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-05-1.png" class="fragment" width="600" height="600"></p>
</div>
</section>
<section id="simple-linear-regression-3" class="slide level2">
<h2>Simple linear regression</h2>

<img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-example-reg-1.png" width="960" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;1: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and <em>vice verca</em>. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable</p><div class="fragment">
<p>The equation for the red line is: <span class="math display">\[Y_i=0.086 +  0.044 \cdot x_i \quad for \;i = 1 \dots 8\]</span></p>
</div>
<div class="fragment">
<p>and in general: <span id="eq-lm-no-error"><span class="math display">\[Y_i=\alpha + \beta \cdot x_i \quad for \; i = 1 \dots n \qquad(1)\]</span></span></p>
</div>
</section>
<section id="simple-linear-regression-4" class="slide level2">
<h2>Simple linear regression</h2>
<p>In other words, by finding the best-fitting straight line we are <strong>building a statistical model</strong> to represent the relationship between plasma volume (<span class="math inline">\(Y\)</span>) and explanatory body weight variable (<span class="math inline">\(x\)</span>)</p>
</section>
<section id="simple-linear-regression-5" class="slide level2 smaller">
<h2>Simple linear regression</h2>
<p><br></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>weight <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">58</span>, <span class="dv">70</span>, <span class="dv">74</span>, <span class="fl">63.5</span>, <span class="fl">62.0</span>, <span class="fl">70.5</span>, <span class="fl">71.0</span>, <span class="fl">66.0</span>) <span class="co"># body weight (kg)</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>plasma <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">2.75</span>, <span class="fl">2.86</span>, <span class="fl">3.37</span>, <span class="fl">2.76</span>, <span class="fl">2.62</span>, <span class="fl">3.49</span>, <span class="fl">3.05</span>, <span class="fl">3.12</span>) <span class="co"># plasma volume (liters)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><br></p>
<div class="columns">
<div class="column" style="width:55%;">
<div>
<ul>
<li class="fragment">If we were to use our model <span class="math inline">\(Y_i=0.086 + 0.044 \cdot x_i\)</span> to find plasma volume given a weight of 58 kg (our first observation, <span class="math inline">\(i=1\)</span>)</li>
<li class="fragment">we would notice that we would get <span class="math inline">\(Y=0.086 + 0.044 \cdot 58 = 2.638\)</span></li>
<li class="fragment"><span class="math inline">\(2.638\)</span> is not exactly the as same as <span class="math inline">\(2.75\)</span>, the first measurement we have in our dataset, i.e.&nbsp;<span class="math inline">\(2.75 - 2.638 = 0.112 \neq 0\)</span>.</li>
<li class="fragment">We thus add to the previous equation (<a href="#/simple-linear-regression-3">Equation&nbsp;1</a>) an <strong>error term</strong> to account for this and now we can write our <strong>simple regression model</strong> more formally as:</li>
<li class="fragment"><span id="eq-lm"><span class="math display">\[Y_i = \alpha + \beta \cdot x_i + \epsilon_i \qquad(2)\]</span></span> where:</li>
<li class="fragment"><span class="math inline">\(x\)</span>: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate</li>
<li class="fragment"><span class="math inline">\(y\)</span>: is called: response, outcome, dependent variable</li>
<li class="fragment"><span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are <strong>model coefficients</strong></li>
<li class="fragment">and <span class="math inline">\(\epsilon_i\)</span> is an <strong>error terms</strong></li>
</ul>
</div>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:40%;">
<div class="cell">
<div class="cell-output-display">
<p><img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-17-1.png" width="960"></p>
</div>
</div>
</div>
</div>
</section>
<section id="least-squares" class="slide level2">
<h2>Least squares</h2>
<div>
<ul>
<li class="fragment">in the above <strong>“body weight - plasma volume”</strong> example, the values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> have just appeared</li>
<li class="fragment">in practice, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> values are <strong>unknown</strong> and we use data to <strong>estimate these coefficients</strong>, noting the estimates with a <strong>hat</strong>, <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span></li>
<li class="fragment"><strong>least squares</strong> is one of the methods of parameters estimation, i.e.&nbsp;finding <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span></li>
</ul>
</div>
</section>
<section id="least-squares-1" class="slide level2 smaller">
<h2>Least squares</h2>
<p><em>minimizing RSS</em></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Let <span class="math inline">\(\hat{y_i}=\hat{\alpha} + \hat{\beta}x_i\)</span> be the prediction <span class="math inline">\(y_i\)</span> based on the <span class="math inline">\(i\)</span>-th value of <span class="math inline">\(x\)</span>:</p>
<ul>
<li>Then <span class="math inline">\(\epsilon_i = y_i - \hat{y_i}\)</span> represents the <span class="math inline">\(i\)</span>-th <strong>residual</strong>, i.e.&nbsp;the difference between the <span class="math inline">\(i\)</span>-th observed response value and the <span class="math inline">\(i\)</span>-th response value that is predicted by the linear model</li>
<li>RSS, the <strong>residual sum of squares</strong> is defined as: <span class="math display">\[RSS = \epsilon_1^2 + \epsilon_2^2 + \dots + \epsilon_n^2\]</span> or equivalently as: <span class="math display">\[RSS=(y_1-\hat{\alpha}-\hat{\beta}x_1)^2+(y_2-\hat{\alpha}-\hat{\beta}x_2)^2+...+(y_n-\hat{\alpha}-\hat{\beta}x_n)^2\]</span></li>
<li>the least squares approach chooses <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> <strong>to minimize the RSS</strong>.</li>
</ul>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:40%;">
<div class="cell" data-fig-cap-location="margin">
<div class="cell-output-display">
<div id="fig-reg-errors" class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-reg-errors-1.png" width="768"></p>
<figcaption>Figure&nbsp;2: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and <em>vice versa</em>. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<!-- ## Least squares -->
<!-- ::: {#thm-lss} -->
<!-- ## Least squares estimates for a simple linear regression -->
<!-- $$\hat{\beta} = \frac{S_{xy}}{S_{xx}}$$ $$\hat{\alpha} = \bar{y}-\frac{S_{xy}}{S_{xx}}\cdot \bar{x}$$ -->
<!-- where: -->
<!-- -   $\bar{x}$: mean value of $x$ -->
<!-- -   $\bar{y}$: mean value of $y$ -->
<!-- -   $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \displaystyle \sum_{i=1}^{n}(x_i-\bar{x})^2$ -->
<!-- -   $S_{yy}$: sum of squares of $Y$ defined as $S_{yy} = \displaystyle \sum_{i=1}^{n}(y_i-\bar{y})^2$ -->
<!-- -   $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \displaystyle \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})$ -->
<!-- ::: -->
<!-- ## Least squares -->
<!-- *Live demo* -->
<!-- ::: {#exm-lss} -->
<!-- ## Least squares -->
<!-- Let's try least squares method to find coefficient estimates in the **"body weight and plasma volume example"** -->
<!-- ``` r -->
<!-- # initial data -->
<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->
<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->
<!-- # rename variables for convenience -->
<!-- x <- weight -->
<!-- y <- plasma -->
<!-- # mean values of x and y -->
<!-- x.bar <- mean(x) -->
<!-- y.bar <- mean(y) -->
<!-- # Sum of squares -->
<!-- Sxx <-  sum((x - x.bar)^2) -->
<!-- Sxy <- sum((x-x.bar)*(y-y.bar)) -->
<!-- # Coefficient estimates -->
<!-- beta.hat <- Sxy / Sxx -->
<!-- alpha.hat <- y.bar - Sxy/Sxx*x.bar -->
<!-- # Print estimated coefficients alpha and beta -->
<!-- print(alpha.hat) -->
<!-- print(beta.hat) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- #| code-fold: false -->
<!-- # initial data -->
<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->
<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->
<!-- # rename variables for convenience -->
<!-- x <- weight -->
<!-- y <- plasma -->
<!-- # mean values of x and y -->
<!-- x.bar <- mean(x) -->
<!-- y.bar <- mean(y) -->
<!-- # Sum of squares -->
<!-- Sxx <-  sum((x - x.bar)^2) -->
<!-- Sxy <- sum((x-x.bar)*(y-y.bar)) -->
<!-- # Coefficient estimates -->
<!-- beta.hat <- Sxy / Sxx -->
<!-- alpha.hat <- y.bar - Sxy/Sxx*x.bar -->
<!-- # Print estimated coefficients alpha and beta -->
<!-- print(alpha.hat) -->
<!-- print(beta.hat) -->
<!-- ``` -->
<!-- ::: -->
</section>
<section id="slope" class="slide level2">
<h2>Slope</h2>
<p><span class="math inline">\(plasma = 0.0857 + 0.0436 * weight\)</span></p>
<p>Linear regression gives us estimates of model coefficient <span class="math inline">\(Y_i = \alpha + \beta x_i + \epsilon_i\)</span></p>

<img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-19-1.png" width="960" class="r-stretch quarto-figure-center"><p><em>Increasing weight by 5 kg corresponds to</em> <span class="math inline">\(3.14 - 2.92 = 0.22\)</span> increase in plasma volume. Increasing weight by 1 kg corresponds <span class="math inline">\(2.96 - 2.92 = 0.04\)</span> increase in plasma volume</p>
</section>
<section id="intercept" class="slide level2">
<h2>Intercept</h2>
<p><span class="math inline">\(plasma = 0.0857 + 0.0436 * weight\)</span></p>
<p>Linear regression gives us estimates of model coefficient <span class="math inline">\(Y_i = \alpha + \beta x_i + \epsilon_i\)</span></p>

<img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-20-1.png" width="960" class="r-stretch quarto-figure-center"><p><em>Intercept value corresponds to expected outcome when the explanatory variable value equals to zero. It is not always meaningful</em></p>
</section>
<section id="hypothesis-testing" class="slide level2">
<h2>Hypothesis testing</h2>
<p><strong>Is there a relationship between the response and the predictor?</strong></p>
<div>
<ul>
<li class="fragment">the calculated <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are <strong>estimates of the population values</strong> of the intercept and slope and are therefore subject to <strong>sampling variation</strong></li>
<li class="fragment">their precision is measured by their <strong>estimated standard errors</strong>, <code>e.s.e</code>(<span class="math inline">\(\hat{\alpha}\)</span>) and <code>e.s.e</code>(<span class="math inline">\(\hat{\beta}\)</span>)</li>
<li class="fragment">these estimated standard errors are used in <strong>hypothesis testing</strong></li>
</ul>
</div>
</section>
<section id="hypothesis-testing-1" class="slide level2 smaller">
<h2>Hypothesis testing</h2>
<p><strong>The most common hypothesis test</strong> involves testing the <code>null hypothesis</code> of:</p>
<ul>
<li><span class="math inline">\(H_0:\)</span> There is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></li>
<li>versus the <code>alternative hypothesis</code> <span class="math inline">\(H_a:\)</span> there is some relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></li>
</ul>
<div class="fragment">
<p><strong>Mathematically</strong>, this corresponds to testing:</p>
<ul>
<li><span class="math inline">\(H_0: \beta=0\)</span></li>
<li>versus <span class="math inline">\(H_a: \beta\neq0\)</span></li>
<li>since if <span class="math inline">\(\beta=0\)</span> then the model <span class="math inline">\(Y_i=\alpha+\beta x_i + \epsilon_i\)</span> reduces to <span class="math inline">\(Y=\alpha + \epsilon_i\)</span></li>
</ul>
</div>
<div class="fragment">
<p><strong>Under the null hypothesis</strong> <span class="math inline">\(H_0: \beta = 0\)</span> <!-- we have: $$\frac{\hat{\beta}-\beta}{e.s.e(\hat{\beta})} \sim t(n-p)$$  --> <img data-src="figures/linear-models/lm-tstatistics.png"></p>
<ul>
<li><span class="math inline">\(n\)</span> is number of observations</li>
<li><span class="math inline">\(p\)</span> is number of model parameters</li>
<li><span class="math inline">\(\frac{\hat{\beta}-\beta}{e.s.e(\hat{\beta})}\)</span> is the ratio of the departure of the estimated value of a parameter, <span class="math inline">\(\hat\beta\)</span>, from its hypothesized value, <span class="math inline">\(\beta\)</span>, to its standard error, called <code>t-statistics</code></li>
<li>the <code>t-statistics</code> follows Student’s t distribution with <span class="math inline">\(n-p\)</span> degrees of freedom</li>
</ul>
</div>
</section>
<section id="hypothesis-testing-2" class="slide level2 smaller">
<h2>Hypothesis testing</h2>
<div id="exm-hypothesis-testing" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Hypothesis testing) </strong></span>Let’s look again at our example data and linear model fitted in R with lm() function.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = plasma ~ weight)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.27880 -0.14178 -0.01928  0.13986  0.32939 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  0.08572    1.02400   0.084   0.9360  
weight       0.04362    0.01527   2.857   0.0289 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.2188 on 6 degrees of freedom
Multiple R-squared:  0.5763,    Adjusted R-squared:  0.5057 
F-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893</code></pre>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li>Under <code>Estimate</code> we see estimates of our model coefficients, <span class="math inline">\(\hat{\alpha}\)</span> (intercept) and <span class="math inline">\(\hat{\beta}\)</span> (slope, here weight), followed by their estimated standard errors, <code>Std. Errors</code></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>If we were to test if there is an <strong>association between weight and plasma volume</strong> we would write under the null hypothesis <span class="math inline">\(H_0: \beta = 0\)</span> <span class="math display">\[\frac{\hat{\beta}-\beta}{e.s.e(\hat{\beta})} = \frac{0.04362-0}{0.01527} = 2.856582\]</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>and we would <strong>compare</strong> <code>t-statistics</code> to <code>Student's t distribution</code> with <span class="math inline">\(n-p = 8 - 2 = 6\)</span> degrees of freedom (as we have 8 observations and two model parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>)</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>we can use <strong>Student’s t distribution table</strong> or <strong>R code</strong> to obtain the associated <em>P</em>-value</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(<span class="fl">2.856582</span>, <span class="at">df=</span><span class="dv">6</span>, <span class="at">lower=</span>F)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.02893095</code></pre>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li>here the observed t-statistics is large and therefore yields a small <em>P</em>-value, meaning that <strong>there is sufficient evidence to reject null hypothesis in favor of the alternative</strong> and conclude that there is a significant association between weight and plasma volume</li>
</ul>
</div>
</section>
<section id="vector-matrix-notations" class="slide level2 scrollable smaller">
<h2>Vector-matrix notations</h2>
<p>Let’s <strong>rewrite</strong> our simple linear regression model <span class="math inline">\(Y_i = \alpha + \beta_i + \epsilon_i \quad i=1,\dots n\)</span> <strong>into vector-matrix notation</strong> in <strong>6 steps</strong>.</p>
<div class="fragment">
<p>Step 1. We rename our <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta\)</span> to <span class="math inline">\(\beta_1\)</span>.</p>
</div>
<div class="fragment">
<p>Step 2. We notice that we have <span class="math inline">\(n\)</span> equations such as:</p>
<p><span class="math display">\[y_1 = \beta_0 + \beta_1 x_1 + \epsilon_1\]</span> <span class="math display">\[y_2 = \beta_0 + \beta_1 x_2 + \epsilon_2\]</span> <span class="math display">\[y_3 = \beta_0 + \beta_1 x_3 + \epsilon_3\]</span> <span class="math display">\[\dots\]</span> <span class="math display">\[y_n = \beta_0 + \beta_1 x_n + \epsilon_n\]</span></p>
</div>
<div class="fragment">
<p>Step 3. We group all <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> into column vectors: <span class="math inline">\(\mathbf{Y}=\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_{n} \end{bmatrix}\)</span> and <span class="math inline">\(\boldsymbol\epsilon=\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_{n} \end{bmatrix}\)</span></p>
</div>
<div class="fragment">
<p>Step 4. We stack two parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> into another column vector:<span class="math display">\[\boldsymbol\beta=\begin{bmatrix}
  \beta_0  \\
  \beta_1
\end{bmatrix}\]</span></p>
</div>
<div class="fragment">
<p>Step 5. We append a vector of ones with the single predictor for each <span class="math inline">\(i\)</span> and create a matrix with two columns called <strong>design matrix</strong> <span class="math display">\[\mathbf{X}=\begin{bmatrix}
  1 &amp; x_1  \\
  1 &amp; x_2  \\
  \vdots &amp; \vdots \\
  1 &amp; x_{n}
\end{bmatrix}\]</span></p>
</div>
<div class="fragment">
<p>Step 6. We write our linear model in a vector-matrix notations as: <span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon\]</span></p>
</div>
</section>
<section id="vector-matrix-notations-1" class="slide level2 smaller">
<h2>Vector-matrix notations</h2>
<div id="def-vector-matrix-lm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (vector matrix form of the linear model) </strong></span>The vector-matrix representation of a linear model with <span class="math inline">\(p-1\)</span> predictors can be written as <span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{Y}\)</span> is <span class="math inline">\(n \times1\)</span> vector of observations</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(n \times p\)</span> <strong>design matrix</strong></li>
<li><span class="math inline">\(\boldsymbol\beta\)</span> is <span class="math inline">\(p \times1\)</span> vector of parameters</li>
<li><span class="math inline">\(\boldsymbol\epsilon\)</span> is <span class="math inline">\(n \times1\)</span> vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, <span class="math inline">\(\sigma^2\)</span>)</li>
</ul>
<p>In full, the above vectors and matrix have the form:</p>
<p><span class="math inline">\(\mathbf{Y}=\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_{n} \end{bmatrix}\)</span> <span class="math inline">\(\boldsymbol\beta=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{p} \end{bmatrix}\)</span> <span class="math inline">\(\boldsymbol\epsilon=\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_{n} \end{bmatrix}\)</span> <span class="math inline">\(\mathbf{X}=\begin{bmatrix} 1 &amp; x_{1,1} &amp; \dots &amp; x_{1,p-1} \\ 1 &amp; x_{2,1} &amp; \dots &amp; x_{2,p-1} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; x_{n,1} &amp; \dots &amp; x_{n,p-1} \end{bmatrix}\)</span></p>
</div>
</section>
<section id="vector-matrix-notations-2" class="slide level2">
<h2>Vector-matrix notations</h2>
<div id="thm-lss-vector-matrix" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Least squares in vector-matrix notation) </strong></span>The least squares estimates for a linear regression of the form: <span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon\]</span></p>
<p>is given by: <span class="math display">\[\hat{\mathbf{\beta}}= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\]</span></p>
</div>
</section>
<section id="vector-matrix-notations-3" class="slide level2 smaller scrollable">
<h2>Vector-matrix notations</h2>
<div id="exm-vector-matrix-notation" class="theorem example">
<p><span class="theorem-title"><strong>Example 3 (vector-matrix-notation) </strong></span>Following the above definition we can write the <strong>“weight - plasma volume model”</strong> as: <span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon\]</span> where:</p>
<p><span class="math inline">\(\mathbf{Y}=\begin{bmatrix} 2.75 \\ 2.86 \\ 3.37 \\ 2.76 \\ 2.62 \\ 3.49 \\ 3.05 \\ 3.12 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(\boldsymbol\beta=\begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}\)</span> <span class="math inline">\(\boldsymbol\epsilon=\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_{8} \end{bmatrix}\)</span> <span class="math inline">\(\mathbf{X}=\begin{bmatrix} 1 &amp; 58.0 \\ 1 &amp; 70.0 \\ 1 &amp; 74.0 \\ 1 &amp; 63.5 \\ 1 &amp; 62.0 \\ 1 &amp; 70.5 \\ 1 &amp; 71.0 \\ 1 &amp; 66.0 \\ \end{bmatrix}\)</span></p>
<p>and we can estimate model parameters using <span class="math inline">\(\hat{\mathbf{\beta}}= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span>.</p>
</div>
</section>
<section id="vector-matrix-notations-least-squares" class="slide level2">
<h2>Vector-matrix notations: least squares</h2>
<p><em>Live demo</em></p>
<p>Estimating model parameters using <span class="math inline">\(\hat{\mathbf{\beta}}= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(plasma) <span class="co"># no. of observation</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(plasma, <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb6-3"><a href="#cb6-3"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="at">length=</span>n), weight)</span>
<span id="cb6-4"><a href="#cb6-4"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(X)</span>
<span id="cb6-5"><a href="#cb6-5"></a></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="co"># print Y and X to double-check that the format is according to the definition</span></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="fu">print</span>(Y)</span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="fu">print</span>(X)</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="co"># least squares estimate</span></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="co"># solve() finds inverse of matrix</span></span>
<span id="cb6-12"><a href="#cb6-12"></a>beta.hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">%*%</span><span class="fu">t</span>(X)<span class="sc">%*%</span>Y</span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="fu">print</span>(beta.hat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]
       0.08572428
weight 0.04361534</code></pre>
</div>
</div>
</section>
<section id="vector-matrix-notations-least-squares-1" class="slide level2">
<h2>Vector-matrix notations: least squares</h2>
<p><em>Live demo</em></p>
<p>In R we can use <code>lm()</code>, the built-in function, to fit a linear regression model and we can replace the above code with one line</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="fu">lm</span>(plasma <span class="sc">~</span> weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = plasma ~ weight)

Coefficients:
(Intercept)       weight  
    0.08572      0.04362  </code></pre>
</div>
</div>
<!-- ## Confidence intervals and prediction intervals -->
<!-- - when we estimate coefficients we can also find their **confidence intervals**, typically 95\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter -->
<!-- - we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval -->
<!-- ::: {exm-prediction-and-intervals} -->
<!-- ## prediction and intervals -->
<!-- Let's: -->
<!-- - find confidence intervals for our coefficient estimates -->
<!-- - predict plasma volume for a men weighting 60 kg -->
<!-- - find prediction interval -->
<!-- - plot original data, fitted regression model, predicted observation and prediction interval -->
<!-- ```{r} -->
<!-- #| code-fold: false -->
<!-- # fit regression model -->
<!-- model <- lm(plasma ~ weight) -->
<!-- print(summary(model)) -->
<!-- # find confidence intervals for the model coefficients -->
<!-- confint(model) -->
<!-- # predict plasma volume for a new observation of 60 kg -->
<!-- # we have to create data frame with a variable name matching the one used to build the model -->
<!-- new.obs <- data.frame(weight = 60) -->
<!-- predict(model, newdata = new.obs) -->
<!-- # find prediction intervals -->
<!-- prediction.interval <- predict(model, newdata = new.obs,  interval = "prediction") -->
<!-- print(prediction.interval) -->
<!-- # plot the original data, fitted regression and predicted value -->
<!-- plot(weight, plasma, pch=19, xlab="weight [kg]", ylab="plasma [l]", ylim=c(2,4)) -->
<!-- lines(weight, model$fitted.values, col="red") # fitted model in red -->
<!-- points(new.obs, predict(model, newdata = new.obs), pch=19, col="blue") # predicted value at 60kg -->
<!-- segments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval -->
<!-- ``` -->
<!-- ## References -->
<!-- ::: {#refs} -->
<!-- ::: -->
</section></section>
<section>
<section id="assessing-model-fit" class="title-slide slide level1 center">
<h1>Assessing model fit</h1>
<!-- -   In addition to knowing how to estimate model parameters we need to learn to assess the goodness of fit of a model. -->
<!-- -   We do that by calculating the amount of variability in the response that is explained by the model. -->
</section>
<section id="r2-summary-of-the-fitted-model" class="slide level2">
<h2><span class="math inline">\(R^2\)</span>: summary of the fitted model</h2>
<p><em>TSS</em></p>
<div class="r-stack">
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-fit-00-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-fit-01-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-fit-02-1.png" class="fragment" width="600" height="600"></p>
</div>
</section>
<section id="r2-summary-of-the-fitted-model-1" class="slide level2">
<h2><span class="math inline">\(R^2\)</span>: summary of the fitted model</h2>
<p><em>RSS</em></p>
<div class="r-stack">
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-rss-01-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-rss-02-1.png" class="fragment" width="600" height="600"></p>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-lm-rss-03-1.png" class="fragment" width="600" height="600"></p>
</div>
</section>
<section id="r2-summary-of-the-fitted-model-2" class="slide level2">
<h2><span class="math inline">\(R^2\)</span>: summary of the fitted model</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell">
<div class="cell-output-display">
<p><img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-31-1.png" width="768"></p>
</div>
</div>
</div><div class="column" style="width:60%;">
<p><br></p>
<ul>
<li><p><strong>TSS</strong>, denoted <strong>Total corrected sum-of-squares</strong> is the residual sum-of-squares for Model 0 <span class="math display">\[S(\hat{\beta_0}) = TSS = \sum_{i=1}^{n}(y_i - \bar{y})^2 = S_{yy}\]</span> corresponding the to the sum of squared distances to the purple line</p></li>
<li><p><strong>RSS</strong>, the residual sum-of-squares, is defined as: <span class="math display">\[RSS = \displaystyle \sum_{i=1}^{n}(y_i - \{\hat{\beta_0} + \hat{\beta}_1x_{1i} + \dots + \hat{\beta}_px_{pi}\}) = \sum_{i=1}^{n}(y_i - \hat{y_i})^2\]</span> and corresponds to the squared distances between the observed values <span class="math inline">\(y_i, \dots,y_n\)</span> to fitted values <span class="math inline">\(\hat{y_1}, \dots \hat{y_n}\)</span>, i.e.&nbsp;distances to the red fitted line</p></li>
</ul>
</div>
</div>
</section>
<section id="r2-summary-of-the-fitted-model-3" class="slide level2">
<h2><span class="math inline">\(R^2\)</span>: summary of the fitted model</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell">
<div class="cell-output-display">
<p><img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-32-1.png" width="768"></p>
</div>
</div>
</div><div class="column" style="width:60%;">
<p><br></p>
<div id="def-r2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (<span class="math inline">\(R^2\)</span>) </strong></span>A simple but useful measure of model fit is given by <span class="math display">\[R^2 = 1 - \frac{RSS}{TSS}\]</span> where:</p>
<ul>
<li>RSS is the residual sum-of-squares for Model 1, the fitted model of interest</li>
<li>TSS is the sum of squares of the <strong>null model</strong></li>
<li><span class="math inline">\(R^2\)</span> is also referred as <strong>coefficient of determination</strong></li>
<li><span class="math inline">\(R^2\)</span> is expressed on a scale, as a proportion between 0 and 1 of the total variation in the data.</li>
</ul>
</div>
</div>
</div>
<!-- ## $R^2$: summary of the fitted model -->
<!-- ::: incremental -->
<!-- -   $R^2$ quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model -->
<!-- -   $R^2$ is also referred as **coefficient of determination** -->
<!-- -   It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data -->
<!-- -   Values of $R^2$ approaching 1 indicate the model to be a good fit -->
<!-- -   Values of $R^2$ less than 0.5 suggest that the model gives rather a poor fit to the data -->
<!-- ::: -->
</section>
<section id="r2-and-correlation-coefficient" class="slide level2">
<h2><span class="math inline">\(R^2\)</span> and correlation coefficient</h2>
<div class="fragment">
<div id="thm-r2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (<span class="math inline">\(R^2\)</span>) </strong></span>In the case of simple linear regression:</p>
<p>Model 1: <span class="math inline">\(Y_i = \beta_0 + \beta_1x + \epsilon_i\)</span> <span class="math display">\[R^2 = r^2\]</span> where:</p>
<ul>
<li><span class="math inline">\(R^2\)</span> is the coefficient of determination</li>
<li><span class="math inline">\(r^2\)</span> is the sample correlation coefficient</li>
</ul>
</div>
<aside class="notes">
<p>What other coefficient is expressed on a 0-1 scale?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>
<section id="r2adj" class="slide level2">
<h2><span class="math inline">\(R^2(adj)\)</span></h2>
<div>
<ul>
<li class="fragment">In the case of multiple linear regression we are using the <strong>adjusted version of</strong> <span class="math inline">\(R^2\)</span> to assess the model fit as the number of explanatory variables increase, <span class="math inline">\(R^2\)</span> also increases.</li>
</ul>
</div>
<div class="fragment">
<div id="thm-r2adj" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (<span class="math inline">\(R^2(adj)\)</span>) </strong></span>For any multiple linear regression <span class="math display">\[Y_i = \beta_0 + \beta_1x_{1i} + \dots + \beta_{p-1}x_{(p-1)i} +  \epsilon_i\]</span> <span class="math inline">\(R^2(adj)\)</span> is defined as <span class="math display">\[R^2(adj) = 1-\frac{\frac{RSS}{n-p-1}}{\frac{TSS}{n-1}}\]</span></p>
<ul>
<li><span class="math inline">\(p\)</span> is the number of independent predictors, i.e.&nbsp;the number of variables in the model, excluding the constant and <span class="math inline">\(n\)</span> is number of observations.</li>
</ul>
<p><span class="math inline">\(R^2(adj)\)</span> can also be calculated from <span class="math inline">\(R^2\)</span>: <span class="math display">\[R^2(adj) = 1 - (1-R^2)\frac{n-1}{n-p-1}\]</span></p>
</div>
<!-- ## $R^2$ -->
<!-- *Live demo* -->
<!-- ``` r -->
<!-- htwtgen <- read.csv("data/lm/heights_weights_gender.csv") -->
<!-- head(htwtgen) -->
<!-- attach(htwtgen) -->
<!-- ## Simple linear regression -->
<!-- model.simple <- lm(Height ~ Weight, data=htwtgen) -->
<!-- # TSS -->
<!-- TSS <- sum((Height - mean(Height))^2) -->
<!-- # RSS -->
<!-- # residuals are returned in the model type names(model.simple) -->
<!-- RSS <- sum((model.simple$residuals)^2) -->
<!-- R2 <- 1 - (RSS/TSS) -->
<!-- print(R2) -->
<!-- print(summary(model.simple)) -->
<!-- ``` -->
<!-- ## $R^2$ -->
<!-- *Live demo* -->
<!-- ``` r -->
<!-- htwtgen <- read.csv("data/lm/heights_weights_gender.csv") -->
<!-- head(htwtgen) -->
<!-- attach(htwtgen) -->
<!-- ## Multiple regression -->
<!-- model.multiple <- lm(Height ~ Weight + Gender, data=htwtgen) -->
<!-- n <- length(Weight) -->
<!-- p <- 1 -->
<!-- RSS <- sum((model.multiple$residuals)^2) -->
<!-- R2_adj <- 1 - (RSS/(n-p-1))/(TSS/(n-1)) -->
<!-- print(R2_adj) -->
<!-- print(summary(model.multiple)) -->
<!-- ``` -->
</div>
</section></section>
<section>
<section id="checking-model-assumptions" class="title-slide slide level1 center">
<h1>Checking model assumptions</h1>

</section>
<section id="the-assumptions-of-a-linear-model" class="slide level2">
<h2>The assumptions of a linear model</h2>
<div>
<ul>
<li class="fragment">Before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data.</li>
<li class="fragment">Informally we have been using box plots and scatter plots to look at the data.</li>
<li class="fragment">There are however formal assumptions that should be met when fitting linear models.</li>
</ul>
</div>
</section>
<section id="the-assumptions-of-a-linear-model-1" class="slide level2">
<h2>The assumptions of a linear model</h2>
<div class="fragment">
<p><br></p>
<p><strong>Linearity:</strong></p>
<ul>
<li>The relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is linear</li>
</ul>
</div>
<div class="fragment">
<p><strong>Independence of errors</strong></p>
<ul>
<li><span class="math inline">\(Y\)</span> is independent of errors, there is no relationship between the residuals and <span class="math inline">\(Y\)</span></li>
</ul>
</div>
<div class="fragment">
<p><strong>Normality of errors</strong></p>
<ul>
<li>The residuals must be approximately normally distributed</li>
</ul>
</div>
<div class="fragment">
<p><strong>Equal variances</strong></p>
<ul>
<li>The variance of the residuals is the same for all values of <span class="math inline">\(X\)</span></li>
</ul>
</div>
</section>
<section id="checking-assumptions" class="slide level2 smaller">
<h2>Checking assumptions</h2>
<p><strong>Residuals</strong>, <span class="math inline">\(\hat{\epsilon_i} = y_i - \hat{y_i}\)</span> are the <strong>main ingredient to check model assumptions</strong>.</p>
<div class="fragment">
<div class="cell" data-fig-cap-location="margin">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-33-1.png" width="960"></p>
<figcaption>Diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Modeling BMI with waist explanatory variable. Model assumptions seems to be met with no noticible patterns based on residuals. Few potential outliers could be considered to be removed.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="checking-assumptions-1" class="slide level2 smaller">
<h2>Checking assumptions</h2>
<p><strong>Residuals</strong>, <span class="math inline">\(\hat{\epsilon_i} = y_i - \hat{y_i}\)</span> are the <strong>main ingredient to check model assumptions</strong>.</p>

<img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-34-1.png" width="960" class="r-stretch quarto-figure-center"><p class="caption">Diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Modeling glucose with age. Model assumptions seems to be violated, e.g.&nbsp;with residulas not following normal distributions.</p></section></section>
<section id="exercises" class="title-slide slide level1 center">
<h1>Exercises</h1>
<p>Let’s practice in exercises fitting linear models, hypothesis testing and assessing model fit.</p>
</section>

<section>
<section id="linear-models-regression-and-classification-tasks" class="title-slide slide level1 center">
<h1>Linear models: regression and classification tasks</h1>

</section>
<section id="linear-models-in-ml-context" class="slide level2">
<h2>Linear models in ML context</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:55%;">
<div>
<ul>
<li class="fragment">We can think of linear models in machine learning context, as they are both used for <strong>regression</strong> and <strong>classification</strong>.</li>
<li class="fragment">Often some or many of the variables used in linear models <strong>are not</strong> associated with the response.</li>
<li class="fragment">There are <strong>feature selection</strong> methods for excluding <strong>irrelevant</strong> variables and improving prediction results.
<ul>
<li class="fragment">subset selection, <strong>Shrinkage methods</strong> and dimension reduction</li>
<li class="fragment">or grouped by filter methods, wrapper methods and <strong>embedded methods</strong>.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:40%;">
<div class="cell">
<div class="cell-output-display">
<p><img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-35-1.png" width="960"></p>
</div>
</div>
</div>
</div>
</section>
<section id="evaluating-linear-models" class="slide level2">
<h2>Evaluating linear models</h2>
<p><br></p>
<p>Regression models can be evaluated by assessing a model fit or by directly evaluating the prediction error via using data splitting strategies.</p>
<div class="fragment">
<div class="columns">
<div class="column" style="width:55%;">
<p><strong>Model fit</strong></p>
<p><strong>Adjusted R-squared</strong> <span class="math display">\[R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1}\]</span></p>
<p><strong>Akaike information criterion (AIC)</strong> <span class="math display">\[\text{AIC} = n \ln(\text{RSS}/n) + 2p\]</span></p>
<p><strong>Bayesian information criterion (BIC)</strong> <span class="math display">\[\text{BIC} = n \ln(\text{RSS}/n) + p \ln(n)\]</span></p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:40%;">
<p><strong>Prediction error</strong></p>
<p><strong>Mean Squared Error (MSE)</strong> <span class="math display">\[MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2\]</span> <strong>Root Mean Squared Error (RMSE)</strong> <span class="math display">\[RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}\]</span></p>
<p><strong>Mean Absolute Error (MAE)</strong> <span class="math display">\[MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|\]</span></p>
</div>
</div>
</div>
</section>
<section id="feature-selection" class="slide level2">
<h2>Feature selection</h2>
<p><em>Group discussion</em> <br></p>
<p>It is time to try to find the best model to explain <code>BMI</code> using <code>diabetes</code> data. Given from what we have learnt so far about linear regression models, how would you find the best model?</p>
<p>As a reminder, we have below variables in the data:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1] "id"       "chol"     "stab.glu" "hdl"      "ratio"    "glyhb"   
 [7] "location" "age"      "gender"   "height"   "weight"   "frame"   
[13] "bp.1s"    "bp.1d"    "bp.2s"    "bp.2d"    "waist"    "hip"     
[19] "time.ppn" "BMI"      "obese"    "diabetic"</code></pre>
</div>
</div>
</section>
<section id="feature-selection-1" class="slide level2">
<h2>Feature selection</h2>
<p><em>Definition</em> <br></p>
<p>Feature selection is the process of selecting the most relevant and informative subset of features from a larger set of potential features in order to improve the performance and interpretability of a machine learning model.</p>
<h3 id="there-are-generally-three-main-groups-of-feature-selection-methods">There are generally three main groups of feature selection methods:</h3>
<div>
<ul>
<li class="fragment"><strong>Filter methods</strong> use statistical measures to score the features and select the most relevant ones, e.g.&nbsp;based on correlation coefficient. Computationally efficient but may overlook complex interactions between features.</li>
<li class="fragment"><strong>Wrapper methods</strong> use ML algorithm to evaluate the performance of different subsets of features, e.g.&nbsp;forward/backward feature selection. Computationally heavy.</li>
<li class="fragment"><strong>Embedded methods</strong> incorporate feature selection as part of the ML algorithm itself, e.g.&nbsp;<strong>regularized regression</strong> or <strong>Random Forest</strong>. These methods are computationally efficient and can be more accurate than filter methods.</li>
</ul>
</div>
</section>
<section id="regularized-regression" class="slide level2">
<h2>Regularized regression</h2>
<p><em>definition</em> <br></p>
<div>
<ul>
<li class="fragment">Regularized regression expands on the regression by adding a <strong>penalty term(s)</strong> to <strong>shrink the model coefficients</strong> of less important features towards zero.</li>
<li class="fragment">This can help to prevent overfitting and improve the accuracy of the predictive model.</li>
<li class="fragment">Depending on the penalty added, we talk about <strong>Ridge</strong>, <strong>Lasso</strong> or <strong>Elastic Nets regression</strong>.</li>
</ul>
</div>
</section>
<section id="regularized-regression-1" class="slide level2">
<h2>Regularized regression</h2>
<p><em>Ridge regression</em> <br></p>
<ul>
<li>Previously we saw that the least squares fitting procedure estimates model coefficients <span class="math inline">\(\beta_0, \beta_1, \cdots, \beta_p\)</span> using the values that minimize the residual sum of squares: <span id="eq-lm"><span class="math display">\[RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 \qquad(3)\]</span></span></li>
</ul>
<div class="fragment">
<ul>
<li>In <strong>regularized regression</strong> the coefficients are estimated by minimizing slightly different quantity. Specifically, in <strong>Ridge regression</strong> we estimate <span class="math inline">\(\hat\beta^{L}\)</span> that minimizes <span id="eq-ridge"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \qquad(4)\]</span></span></li>
</ul>
<p>where:</p>
<p><span class="math inline">\(\lambda \ge 0\)</span> is a <strong>tuning parameter</strong> to be determined separately e.g.&nbsp;via cross-validation</p>
</div>
</section>
<section id="regularized-regression-2" class="slide level2">
<h2>Regularized regression</h2>
<p><em>Ridge regression</em> <br></p>
<p><span id="eq-ridge"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \qquad(5)\]</span></span></p>
<p><a href="#/regularized-regression-1">Equation&nbsp;5</a> trades two different criteria:</p>
<ul>
<li>lasso regression seeks coefficient estimates that fit the data well, by making RSS small</li>
<li>however, the second term <span class="math inline">\(\lambda \sum_{j=1}^{p}\beta_j^2\)</span>, called <strong>shrinkage penalty</strong> is small when <span class="math inline">\(\beta_1, \cdots, \beta_p\)</span> are close to zero, so it has the effect of <strong>shrinking</strong> the estimates of <span class="math inline">\(\beta_j\)</span> towards zero.</li>
<li>the tuning parameter <span class="math inline">\(\lambda\)</span> controls the relative impact of these two terms on the regression coefficient estimates
<ul>
<li>when <span class="math inline">\(\lambda = 0\)</span>, the penalty term has no effect</li>
<li>as <span class="math inline">\(\lambda \rightarrow \infty\)</span> the impact of the shrinkage penalty grows and the ridge regression coefficient estimates approach zero</li>
</ul></li>
</ul>
</section>
<section id="regularized-regression-3" class="slide level2">
<h2>Regularized regression</h2>
<p><em>Ridge regression</em> <br></p>

<img data-src="session-lm-presentation_files/figure-revealjs/ridge-run-1.png" width="960" class="r-stretch quarto-figure-center"><p class="caption">Example of Ridge regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero for larger lambda values.</p></section>
<section id="bias-variance-trade-off" class="slide level2">
<h2>Bias-variance trade-off</h2>
<p><br></p>
<p>Ridge regression’s advantages over least squares estimates stems from <strong>bias-variance trade-off</strong></p>
<div>
<ul>
<li class="fragment">The bias-variance trade-off describes the relationship between model complexity, prediction accuracy, and the ability of the model to generalize to new data.</li>
<li class="fragment"><strong>Bias</strong> refers to the error that is introduced by approximating a real-life problem with a simplified model
<ul>
<li class="fragment">e.g.&nbsp;a high bias model is one that makes overly simplistic assumptions about the underlying data, resulting in <em>under-fitting</em> and poor accuracy.</li>
</ul></li>
<li class="fragment"><strong>Variance</strong> refers to the sensitivity of a model to fluctuations in the training data.
<ul>
<li class="fragment">e.g.&nbsp;a high variance model is one that is overly complex and captures noise in the training data, resulting in <em>overfitting</em> and poor generalization to new data.</li>
</ul></li>
</ul>
</div>
</section>
<section id="bias-variance-trade-off-1" class="slide level2">
<h2>Bias-variance trade-off</h2>
<p><br></p>
<p>The goal of machine learning is to find a model with <strong>the right balance between bias and variance</strong>.</p>
<div>
<ul>
<li class="fragment">The bias-variance trade-off can be visualized in terms of MSE, means squared error of the model. The <strong>MSE</strong> can be decomposed into: <span class="math display">\[MSE(\hat\beta) := bias^2(\hat\beta) + Var(\hat\beta) + noise\]</span>
<ul>
<li class="fragment">The irreducible error is the inherent noise in the data that cannot be reduced by any model</li>
<li class="fragment">The bias and variance terms can be reduced by choosing an appropriate model complexity.</li>
<li class="fragment">The trade-off lies in finding the right balance between bias and variance that minimizes the total MSE.</li>
</ul></li>
<li class="fragment">In practice, this trade-off can be addressed by <strong>regularizing the model</strong>, selecting an appropriate model complexity, or by using ensemble methods that combine multiple models to reduce the variance.</li>
</ul>
</div>
</section>
<section id="bias-variance-trade-off-2" class="slide level2">
<h2>Bias-variance trade-off</h2>
<p><br></p>

<img data-src="images/bias-variance.png" style="width:100.0%" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;3: Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. al, A Introduction to statistical learning</p></section>
<section id="ridge-vs.-lasso" class="slide level2">
<h2>Ridge vs.&nbsp;Lasso</h2>
<p>In <strong>Ridge</strong> regression we minimize: <span id="eq-ridge2"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \qquad(6)\]</span></span> where <span class="math inline">\(\lambda \sum_{j=1}^{p}\beta_j^2\)</span> is also known as <strong>L2</strong> regularization element or <span class="math inline">\(l_2\)</span> penalty</p>
<p>In <strong>Lasso</strong> regression, that is Least Absolute Shrinkage and Selection Operator regression we change penalty term to absolute value of the regression coefficients: <span id="eq-lasso"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| = RSS + \lambda \sum_{j=1}^{p}|\beta_j| \qquad(7)\]</span></span> where <span class="math inline">\(\lambda \sum_{j=1}^{p}|\beta_j|\)</span> is also known as <strong>L1</strong> regularization element or <span class="math inline">\(l_1\)</span> penalty</p>
<p>Lasso regression was introduced to help model interpretation. With Ridge regression we improve model performance but unless <span class="math inline">\(\lambda = \infty\)</span> all beta coefficients are non-zero, hence all variables remain in the model. By using <span class="math inline">\(l_1\)</span> penalty we can force some of the coefficients estimates to be exactly equal to 0, hence perform <strong>variable selection</strong></p>
</section>
<section id="ridge-vs.-lasso-1" class="slide level2">
<h2>Ridge vs.&nbsp;Lasso</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="session-lm-presentation_files/figure-revealjs/ridge-run-02-1.png" width="960"></p>
<figcaption>Example of Ridge regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero for larger lambda values.</figcaption>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="session-lm-presentation_files/figure-revealjs/lasso-run-1.png" width="960"></p>
<figcaption>Example of Lasso regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values, showing how initially for small lambda values all variables are part of the model and how they gradually shrink towards zero and are also set to zero for larger lambda values.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="elastic-net" class="slide level2">
<h2>Elastic Net</h2>
<p><br></p>
<p><strong>Elastic Net</strong> use both L1 and L2 penalties to try to find middle grounds by performing parameter shrinkage and variable selection. <span id="eq-elastic-net"><span class="math display">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2  \qquad(8)\]</span></span></p>

<img data-src="session-lm-presentation_files/figure-revealjs/elastic-net-1.png" width="960" class="r-stretch quarto-figure-center"><p class="caption">Example of Elastic Net regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values and alpha value 0.1, showing the changes of model coefficients as a function of lambda being somewhere between those for Ridge and Lasso regression.</p></section>
<section id="elastic-net-1" class="slide level2">
<h2>Elastic Net</h2>
<p><em>In R with <code>glmnet</code></em> <br></p>
<p>In the <code>glmnet</code> library we can fit Elastic Net by setting parameters <span class="math inline">\(\alpha\)</span>. Under the hood <code>glmnet</code> minimizes a cost function: <span class="math display">\[\sum_{i_=1}^{n}(y_i-\hat y_i)^2 + \lambda \left ( (1-\alpha) \sum_{j=1}^{p}\beta_j^2 + \alpha \sum_{j=1}^{p}|\beta_j|\right )\]</span> where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of samples</li>
<li><span class="math inline">\(p\)</span> is the number of parameters</li>
<li><span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\alpha\)</span> hyperparameters control the shrinkage</li>
</ul>
<p>When <span class="math inline">\(\alpha = 0\)</span> this corresponds to Ridge regression and when <span class="math inline">\(\alpha=1\)</span> this corresponds to Lasso regression. A value of <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span> gives us <strong>Elastic Net regularization</strong>, combining both L1 and L2 regularization terms.</p>
</section></section>
<section>
<section id="generalized-linear-models" class="title-slide slide level1 center">
<h1>Generalized Linear Models</h1>

</section>
<section id="why-generalized-linear-models" class="slide level2">
<h2>Why Generalized Linear Models</h2>
<p><em>GLM</em></p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>GLMs extend linear model framework to outcome variables that do not follow normal distribution.</li>
<li>They are most frequently used to model binary, categorical or count data.</li>
<li>For instance, fitting a regression line to binary data yields predicted values that could take any value, including <span class="math inline">\(&lt;0\)</span>,</li>
<li>not to mention that it is hard to argue that the values of 0 and 1s are normally distributed.</li>
</ul>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:45%;">
<div class="cell" data-fig-cap-location="bottom">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="session-lm-presentation_files/figure-revealjs/log-example-1.png" width="960"></p>
<figcaption>Example of fitting linear model to binary data, to model obesity status coded as 1 (Yes) and 0 (No) with waist variable. Linear model does not fit the data well in this case and the predicted values can take any value, not only 0 and 1.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="logistic-regression" class="slide level2">
<h2>Logistic regression</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Since the response variable takes only two values (Yes/No) we use GLM model</li>
<li>to fit <strong>logistic regression</strong> model for the <strong>probability of suffering from obesity (Yes).</strong></li>
<li>We let <span class="math inline">\(p_i=P(Y_i=1)\)</span> denote the probability of suffering from obesity (success)</li>
<li>and we assume that the response follows binomial distribution: <span class="math inline">\(Y_i \sim Bi(1, p_i)\)</span> distribution.</li>
<li>We can then write the regression model now as: <span class="math display">\[log(\frac{p_i}{1-p_i})=\beta_0 + \beta_1x_i\]</span> and given the properties of logarithms this is also equivalent to: <span class="math display">\[p_i = \frac{exp(\beta_0 + \beta_1x_i)}{1 + exp(\beta_0 + \beta_1x_i)}\]</span></li>
</ul>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:45%;">
<div class="cell">
<div class="cell-output-display">
<div id="fig-obesity" class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="session-lm-presentation_files/figure-revealjs/fig-obesity-1.png" width="960"></p>
<figcaption>Figure&nbsp;4: <strong>?(caption)</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="logistic-regression-1" class="slide level2">
<h2>Logistic regression</h2>
<p><br></p>
<ul>
<li>In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a <strong>link function</strong>.</li>
<li>Here, the <strong>link function</strong> <span class="math inline">\(log(\frac{p_i}{1-p_i})\)</span> provides the link between the binomial distribution of <span class="math inline">\(Y_i\)</span> (suffering from obesity) and the linear predictor (waist)</li>
<li>Thus the <strong>GLM model</strong> can be written as <span class="math display">\[g(\mu_i)=\mathbf{X}\boldsymbol\beta\]</span> where <code>g()</code> is the link function.</li>
</ul>
</section>
<section id="logistic-regression-2" class="slide level2">
<h2>Logistic regression</h2>
<p><em>R</em> <br></p>
<p>In R we can use <code>glm()</code> function to fit GLM models:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># fit logistic regression model</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>logmodel_1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(obese <span class="sc">~</span> waist, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span><span class="st">"logit"</span>), <span class="at">data =</span> data_diabetes)</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># print model summary</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="fu">print</span>(<span class="fu">summary</span>(logmodel_1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<pre><code>## 
## Call:
## glm(formula = obese ~ waist, family = binomial(link = "logit"), 
##     data = data_diabetes)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -18.29349    1.83541  -9.967   &lt;2e-16 ***
## waist         0.18013    0.01843   9.774   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 518.24  on 394  degrees of freedom
## Residual deviance: 282.93  on 393  degrees of freedom
##   (8 observations deleted due to missingness)
## AIC: 286.93
## 
## Number of Fisher Scoring iterations: 6</code></pre>
</div>
</section>
<section id="logistic-regression-3" class="slide level2">
<h2>Logistic regression</h2>
<p><em>R</em> <br></p>

<img data-src="session-lm-presentation_files/figure-revealjs/unnamed-chunk-47-1.png" width="960" class="r-stretch quarto-figure-center"><p class="caption">Fitted logistic model to diabetes data given the 130 study participants and using waist as explantatory variable to model obesity status.</p></section>
<section id="logistic-regression-4" class="slide level2">
<h2>Logistic regression</h2>
<p><em>Hypothesis testing</em> <br></p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li>Similarly to linear models, we can determine whether each variable is related to the outcome of interest by testing the null hypothesis that the relevant logistic regression coefficient is zero.</li>
<li>This can be performed by <strong>Wald test</strong> which is equals to estimated logistic regression coefficient divided by its standard error and follows the Standard Normal distribution: <span class="math display">\[W = \frac{\hat\beta-\beta}{e.s.e.(\hat\beta)} \sim N(0,1)\]</span></li>
</ul>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:55%;">
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># fit logistic regression model</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>logmodel_1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(obese <span class="sc">~</span> waist, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span><span class="st">"logit"</span>), <span class="at">data =</span> data_diabetes)</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="fu">summary</span>(logmodel_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = obese ~ waist, family = binomial(link = "logit"), 
    data = data_diabetes)

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -18.29349    1.83541  -9.967   &lt;2e-16 ***
waist         0.18013    0.01843   9.774   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 518.24  on 394  degrees of freedom
Residual deviance: 282.93  on 393  degrees of freedom
  (8 observations deleted due to missingness)
AIC: 286.93

Number of Fisher Scoring iterations: 6</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="logistic-regression-5" class="slide level2">
<h2>Logistic regression</h2>
<p><em>Deviance</em> <br></p>
<ul>
<li>Deviance is the number that measures the goodness of fit of a logistic regression model.</li>
<li>We use saturated and residual deviance to assess model, instead of <span class="math inline">\(R^2\)</span> or <span class="math inline">\(R^2(adj)\)</span>.</li>
<li>We can also use deviance to check the association between explanatory variable and the outcome</li>
<li>In the <strong>likelihood ratio test</strong> the test statistics is the <strong>deviance</strong> for the full model minus the deviance for the full model excluding the relevant explanatory variable. This test statistics follow a Chi-squared distribution with 1 degree of freedom.</li>
</ul>
</section>
<section id="logistic-regression-6" class="slide level2">
<h2>Logistic regression</h2>
<p><em>Odds ratio</em> <br></p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li>In logistic regression we often interpret the model coefficients by taking <span class="math inline">\(e^{\hat{\beta}}\)</span></li>
<li>and we talk about <strong>odd ratios</strong>.</li>
<li>For instance we can say, given our above model, <span class="math inline">\(e^{0.18} = 1.2\)</span> that for each unit increase in <code>waist</code> the odds of suffering from obesity get multiplied by 1.2.</li>
</ul>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:55%;">
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># fit logistic regression model</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>logmodel_1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(obese <span class="sc">~</span> waist, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span><span class="st">"logit"</span>), <span class="at">data =</span> data_diabetes)</span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="co"># print model summary</span></span>
<span id="cb15-5"><a href="#cb15-5"></a><span class="fu">print</span>(<span class="fu">summary</span>(logmodel_1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = obese ~ waist, family = binomial(link = "logit"), 
    data = data_diabetes)

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -18.29349    1.83541  -9.967   &lt;2e-16 ***
waist         0.18013    0.01843   9.774   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 518.24  on 394  degrees of freedom
Residual deviance: 282.93  on 393  degrees of freedom
  (8 observations deleted due to missingness)
AIC: 286.93

Number of Fisher Scoring iterations: 6</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="common-glm-models" class="slide level2">
<h2>Common GLM models</h2>
<p><br></p>
<div class="cell">
<div class="cell-output-display">
<table class="lightable-paper lightable-hover table" data-quarto-postprocess="true" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; margin-left: auto; margin-right: auto; font-size: 20px; margin-left: auto; margin-right: auto;">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Type.of.outcome</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">Type.of.GLM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Continous numerical</td>
<td style="text-align: left;">Simple or multiple linear</td>
</tr>
<tr class="even">
<td style="text-align: left;">Binary</td>
<td style="text-align: left;">Logistic</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Categorical outcome with more than two groups</td>
<td style="text-align: left;">Multinomial or ordinal logistic regression</td>
</tr>
<tr class="even">
<td style="text-align: left;">Event rate or count</td>
<td style="text-align: left;">Poisson</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Time to event</td>
<td style="text-align: left;">Exponential</td>
</tr>
</tbody>
</table>


</div>
</div>
</section></section>
<section>
<section id="logistic-lasso" class="title-slide slide level1 center">
<h1>Logistic Lasso</h1>

</section>
<section id="logistic-lasso-1" class="slide level2">
<h2>Logistic Lasso</h2>
<p><em>Logistic regression + Lasso regularization</em></p>
<p><br></p>
<ul>
<li>Logistic Lasso combines logistic regression with Lasso regularization to analyze binary outcome data while simultaneously performing variable selection and regularization.</li>
<li>We estimate set of coefficients <span class="math inline">\(\hat \beta\)</span> that minimize the combined logistic loss function and the Lasso penalty:</li>
</ul>
<p><span class="math display">\[
\left( \sum_{i=1}^n [y_i \log(p_i) + (1-y_i) \log(1-p_i)] \right) + \lambda \sum_{j=1}^p |\beta_j|
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> represents the binary outcome (0 or 1) for the ( i )-th observation.</li>
<li><span class="math inline">\(p_i\)</span> is the predicted probability of <span class="math inline">\(y_i = 1\)</span> given by the logistic model <span class="math inline">\(p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip})}}\)</span>.</li>
<li><span class="math inline">\(\lambda\)</span> is the regularization parameter that controls the strength of the Lasso penalty <span class="math inline">\(\lambda \sum_{j=1}^p |\beta_j|\)</span>, which encourages sparsity in the coefficients <span class="math inline">\(\beta_j\)</span> by shrinking some of them to zero.</li>
<li><span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(p\)</span> is the number of predictors.</li>
</ul>
</section></section>
<section id="common-cases" class="title-slide slide level1 center">
<h1>Common cases</h1>
<ul>
<li>Let’s go over common linear models cases and some exercises.</li>
</ul>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="session-lm-presentation_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="session-lm-presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="session-lm-presentation_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="session-lm-presentation_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="session-lm-presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="session-lm-presentation_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="session-lm-presentation_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="session-lm-presentation_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="session-lm-presentation_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="session-lm-presentation_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="session-lm-presentation_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>