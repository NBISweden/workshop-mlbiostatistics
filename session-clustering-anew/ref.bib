@article{Traag2019,
   abstract = {Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25% of the communities are badly connected and up to 16% are disconnected. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees.},
   author = {V. A. Traag and L. Waltman and N. J. van Eck},
   doi = {10.1038/s41598-019-41695-z},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   title = {From Louvain to Leiden: guaranteeing well-connected communities},
   volume = {9},
   year = {2019},
}
@article{Suzuki2006,
   abstract = {Summary: Pvclust is an add-on package for a statistical software R to assess the uncertainty in hierarchical cluster analysis. Pvclust can be used easily for general statistical problems, such as DNA microarray analysis, to perform the bootstrap analysis of clustering, which has been popular in phylogenetic analysis. Pvclust calculates probability values (p-values) for each cluster using bootstrap resampling techniques. Two types of p-values are available: approximately unbiased (AU) p-value and bootstrap probability (BP) value. Multiscale bootstrap resampling is used for the calculation of AU p-value, which has superiority in bias over BP value calculated by the ordinary bootstrap resampling. In addition the computation time can be enormously decreased with parallel computing option. © 2006 Oxford University Press.},
   author = {Ryota Suzuki and Hidetoshi Shimodaira},
   doi = {10.1093/bioinformatics/btl117},
   issn = {13674811},
   issue = {12},
   journal = {Bioinformatics},
   title = {Pvclust: An R package for assessing the uncertainty in hierarchical clustering},
   volume = {22},
   year = {2006},
}
@misc{Gao2023,
   abstract = {Cluster analyzes have been widely used in mental health research to decompose inter-individual heterogeneity by identifying more homogeneous subgroups of individuals. However, despite advances in new algorithms and increasing popularity, there is little guidance on model choice, analytical framework and reporting requirements. In this paper, we aimed to address this gap by introducing the philosophy, design, advantages/disadvantages and implementation of major algorithms that are particularly relevant in mental health research. Extensions of basic models, such as kernel methods, deep learning, semi-supervised clustering, and clustering ensembles are subsequently introduced. How to choose algorithms to address common issues as well as methods for pre-clustering data processing, clustering evaluation and validation are then discussed. Importantly, we also provide general guidance on clustering workflow and reporting requirements. To facilitate the implementation of different algorithms, we provide information on R functions and libraries.},
   author = {Caroline X. Gao and Dominic Dwyer and Ye Zhu and Catherine L. Smith and Lan Du and Kate M. Filia and Johanna Bayer and Jana M. Menssink and Teresa Wang and Christoph Bergmeir and Stephen Wood and Sue M. Cotton},
   doi = {10.1016/j.psychres.2023.115265},
   issn = {18727123},
   journal = {Psychiatry Research},
   title = {An overview of clustering methods with guidelines for application in mental health research},
   volume = {327},
   year = {2023},
}
@article{Lloyd1982,
   abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quantization schemes for 2b quanta, b = 1,2, ···, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes. ©1982 IEEE},
   author = {Stuart P. Lloyd},
   doi = {10.1109/TIT.1982.1056489},
   issn = {15579654},
   issue = {2},
   journal = {IEEE Transactions on Information Theory},
   title = {Least Squares Quantization in PCM},
   volume = {28},
   year = {1982},
}
@article{Hartigan1979,
   abstract = {The K-means clustering algorithm is described indetail by Hartigan(1975). An efficient version of the algorithm is presented here.\nThe aim of the K-means algorithm is to divide M points in N dimensions into K clusters so that the within-cluster sum of squares is minimized. It is not practical to require that the solution has minimal sum of squares against all partitions except when M,N are small and K = 2. We seek instead "local" optima, solution such that no movement of a point from one cluster to another will reduce the within cluster sum of squares.},
   author = {J. A. Hartigan and M. A. Wong},
   doi = {10.2307/2346830},
   issn = {00359254},
   issue = {1},
   journal = {Applied Statistics},
   title = {Algorithm AS 136: A K-Means Clustering Algorithm},
   volume = {28},
   year = {1979},
}
@article{Forgy1965,
   abstract = {In recent years a number of procedures to cluster, group, or classify a sample of data points have been advanced, but such methods are still not generally accepted as useful and dependable tools for data analysis. Much of the reluctance to rely on these methods may be due to uncertainty as to just what the end result of a cluster analysis means. This of course depends on the particular method used. In the literature two rather different purposes of clustering methods may be discerned. (1) To establish a maximally efficient way of partitioning the sample into given numbers of classes. The classical measure of efficiency is the pooled variance within classes, which is to be minimized. (2) To discover and describe the 'natural' way of classifying the sample. Here the data determine the number of classes, so there is the possibility of there being no partitioning at all. Examples of methods advanced for each purpose are given, and experimental results of several of these upon samples from known multivariate populations with and without 'natural' partitionings are presented. The most striking finding is that some procedures having the second purpose will almost always suggest 'natural' (i.e. interpretable) classifications when in fact there are none (e.g. in data from a joint normal population). A user would thus be exposed to virtual certainty of Type I error. Improved clustering procedures are suggested for both purposes.},
   author = {E. Forgy},
   issue = {3},
   journal = {Biometrics},
   title = {Cluster analysis of multivariate data: Efficiency vs. interpretability of classifications},
   volume = {21},
   year = {1965},
}
@article{Preud2021,
   abstract = {The choice of the most appropriate unsupervised machine-learning method for “heterogeneous” or “mixed” data, i.e. with both continuous and categorical variables, can be challenging. Our aim was to examine the performance of various clustering strategies for mixed data using both simulated and real-life data. We conducted a benchmark analysis of “ready-to-use” tools in R comparing 4 model-based (Kamila algorithm, Latent Class Analysis, Latent Class Model [LCM] and Clustering by Mixture Modeling) and 5 distance/dissimilarity-based (Gower distance or Unsupervised Extra Trees dissimilarity followed by hierarchical clustering or Partitioning Around Medoids, K-prototypes) clustering methods. Clustering performances were assessed by Adjusted Rand Index (ARI) on 1000 generated virtual populations consisting of mixed variables using 7 scenarios with varying population sizes, number of clusters, number of continuous and categorical variables, proportions of relevant (non-noisy) variables and degree of variable relevance (low, mild, high). Clustering methods were then applied on the EPHESUS randomized clinical trial data (a heart failure trial evaluating the effect of eplerenone) allowing to illustrate the differences between different clustering techniques. The simulations revealed the dominance of K-prototypes, Kamila and LCM models over all other methods. Overall, methods using dissimilarity matrices in classical algorithms such as Partitioning Around Medoids and Hierarchical Clustering had a lower ARI compared to model-based methods in all scenarios. When applying clustering methods to a real-life clinical dataset, LCM showed promising results with regard to differences in (1) clinical profiles across clusters, (2) prognostic performance (highest C-index) and (3) identification of patient subgroups with substantial treatment benefit. The present findings suggest key differences in clustering performance between the tested algorithms (limited to tools readily available in R). In most of the tested scenarios, model-based methods (in particular the Kamila and LCM packages) and K-prototypes typically performed best in the setting of heterogeneous data.},
   author = {Gregoire Preud’homme and Kevin Duarte and Kevin Dalleau and Claire Lacomblez and Emmanuel Bresso and Malika Smaïl-Tabbone and Miguel Couceiro and Marie Dominique Devignes and Masatake Kobayashi and Olivier Huttin and João Pedro Ferreira and Faiez Zannad and Patrick Rossignol and Nicolas Girerd},
   doi = {10.1038/s41598-021-83340-8},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   title = {Head-to-head comparison of clustering methods for heterogeneous data: a simulation-driven benchmark},
   volume = {11},
   year = {2021},
}
@article{Blanco2023,
   abstract = {Many machine learning and data mining tasks are based on distance measures, so a large amount of literature addresses this aspect somehow. Due to the broad scope of the topic, this paper aims to provide an overview of the use of these measures in the most common machine learning problems, pointing out those aspects to consider to choose the most appropriate measure for a particular task. For this purpose, the most recent works addressing the subject were reviewed and seven of the most commonly used measures were analyzed, investigating in detail their main properties and applications. Different experiments were carried out to study their relationships and compare their performance. The degradation of the results in the presence of noise was also considered, as well as the execution time required by each measure.},
   author = {Eva Blanco-Mallo and Laura Morán-Fernández and Beatriz Remeseiro and Verónica Bolón-Canedo},
   doi = {10.1016/j.patcog.2023.109646},
   issn = {00313203},
   journal = {Pattern Recognition},
   title = {Do all roads lead to Rome? Studying distance measures in the context of machine learning},
   volume = {141},
   year = {2023},
}
@article{Ghaderyan2020,
   abstract = {Gait rhythm fluctuations are of great importance for automatic neurodegenerative diseases (NDDs) detection. They provide a cost-effective and noninvasive monitoring tool in which their parameters are related to neuromuscular function. This study investigated a new solution based on a set of new symmetric features and sparse non-negative least squares (NNLS) coding classifier. Dynamic gait series warping (DGSW), Euclidean, Manhattan, Minkowski, Chebyshev, Canberra distances, and cosine function were used to quantify the amount of divergence between the left and right stride, swing, and stance intervals. The algorithm was evaluated using the gait signals of 20 healthy control subjects, 20 patients with amyotrophic lateral sclerosis (ALS), 15 patients with Parkinson's disease (PD) and 20 patients with Huntington's disease (HD). The proposed new approach using symmetric features and NNLS technique achieved outstanding accuracies of 98%, 97%, and 95% on the patients with PD, ALS, and HD, respectively. The findings also suggested that the new DGSW, cosine function, and Chebyshev distance, which are designed to dynamically, geometrically, or nonlinearly quantify the similarity between two time series, provide the discriminatory measures to describe how NDDs alter the gait symmetry. In comparison with other studies, combining symmetric features with a sparse NNLS coding classifier can improve the detection accuracy providing an efficient and cost-effective framework for the development of a NDDs detection system.},
   author = {Peyvand Ghaderyan and Seyede Marziyeh Ghoreshi Beyrami},
   doi = {10.1016/j.compbiomed.2020.103736},
   issn = {18790534},
   journal = {Computers in Biology and Medicine},
   title = {Neurodegenerative diseases detection using distance metrics and sparse coding: A new perspective on gait symmetric features},
   volume = {120},
   year = {2020},
}
@article{Wilcoxon1945,
   author = {Frank Wilcoxon},
   issue = {6},
   journal = {Biometrics Bulletin},
   pages = {80-83},
   title = {Individual Comparisions by Ranking Methods},
   volume = {1},
   year = {1945},
}
