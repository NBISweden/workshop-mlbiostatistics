---
title: "Introduction"
subtitle: "Machine learning and Biostatistics course"
author: "Bengt Sennblad"
institute: "NBIS"
date: "11/16/2020"
output: 
  xaringan::moon_reader:
    encoding: 'UTF-8'
    self_contained: false
    css: [default, metropolis, metropolis-fonts, customColumns.css]
    lib_dir: 'libs'
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)

library(reticulate)
library(knitr)
knit_engines$set(python = reticulate::eng_python)  

library(ggplot2)
library(qqman)
```

# Content/aim



.pull-left[
## What do we want you to gain from this course?
- Learn a selection statistical methods

{{content}}
]
--

#### ...but, more importantly, be able to
- by yourselves explore statistics
  - find alternative methods
  - get a intuitive understanding
  - evaluate assumptions
      + know when to apply
      + know when not to apply

--
.pull-right[
### How?
{{content}}
]
--
- Go from <span style="background-color:black"><span style="color:white">_black box_</span></span> to understanding
    - several levels
      - models = assumptions
      - tests -- test statistics
      - overfitting
{{content}}

???
Go from treating statistical methods as black box to actually understanding what they do...
--
- Appreciate theory as a ❤️*friend*❤️
    - help understanding assumptions
    - help when evaluating methods
{{content}}


--
- Provide a _#framework#_ for learning
    - statistical thinking
    - basic math language
      - not afraid of equations
      - know some basic terminology

???

---
class: inverse, middle, center
<html><div style='float:left'></div><hr color='#EB811B' size=1px width=800px></html> 

# Why Statistics?
## Examples

???
- So, why do we need statistics?
- We will illustrate with some examples

---

# Example: Mouse knockout experiment

### Setup
Hypothesis: The low density lipoprotein (LDL) receptor gene *LDLR* affects the prevalence of hypercholsterolaemia.

- 10 wildtype (wt) mice
- 10 *LDLR* knockout (KO) mice

Measure plasma concentration of total cholesterol at one time point after feeeding on high fat diet.

???

- low-density lipoprotein 
- cholesterol -> vascular plaques -> cardiovascualr disease (CVD)
- experiment design using mice  model
---
layout: true
# Example: Mouse knockout experiment
.pull-left[
```{r mice1}
set.seed(85)

mean = c(0.55, 0.45)
stddev = c(0.025, 0.025)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10)),
  names = c(paste0("KO", 1:10), paste0("wt", 1:10))
)
row.names(mice)=mice$names

# to get randomly ordered x-axis
mice$names=factor(as.character(mice$names), levels=sample(row.names(mice)))
ggplot(mice, aes(y=prevalence, x=names))+
  xlab("mice") +
  geom_point() +
  theme_bw()

```
]
---
.pull-right[
### Visualize results in a plot

]
---
.pull-right[
### Visualize results in a plot
- Not so informative!
- Let's improve
]
---
layout: false
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice1a}
ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a **boxplot**
]

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a boxplot
  ![black box warning](assets/blackbox.jpg)
]

???

- Whooa! Black box right there -- what is hiding behind "boxplot"

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a boxplot
  - Visualize **distribution** of sample values using **descriptive statistics**
      - *mean* 
      - *quartiles*
]

???

- black line = mean
- box = quartiles -- indicate variation in data
- More about descriptive statistics in the course

---
.pull-right[

### Interpretation

Intuitively, there is a clear difference between KO and wt in this plot...
]

-- 
.pull-right[
What if the result had looked different?

]

???

- but...

---
layout: false
# Example: Mouse knockout experiment

.pull-left[
```{r mice3}
set.seed(85)
mean = c(0.5, 0.5)
stddev = c(0.025, 0.025)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]
.pull-right[

### Interpretation

...equally clearly, here is no difference between KO and wt...

]
---
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice4}
set.seed(85)
mean = c(0.525, 0.475)
stddev = c(0.05, 0.05)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]

---
.pull-right[

### Interpretation

... Now, the difference is less clear

{{content}}
]
--

... we get uncertain

{{content}}

--
... Uncertainty best measured as probabilities!

{{content}}
--

... with probabilities we can perform a **statistical test** for difference

{{content}}
--
  ![black box warning](assets/blackbox.jpg)]

???
- Another black box
- What do we mean by that?


---
.pull-right[
### Statistical tests

#### Model
- Abstraction of reality
  - computation of  probabilities 
- Simplifying assumptions
  - Variation approximately $N(\mu,\sigma)$
    
#### Hypotheses
- H0: $\mu$ and $\sigma$ same for KO and wt 
- H1: $\mu$ might be different 

#### Test
- **P-value** $(p)$ =  probability of the observed or more extreme difference  under H0
- **significance level** $p\leq \alpha$
]

???

- We will go through this in the course, but briefly...
- We set up statistical models, 
  - making simplifying assumptions
  - allowing us to compute probabilities
  - Here, we could make the simplifying assumption 
- Based on the model, we can set up hypotheses relating to the problem at hand, here that variation is normally distributed with given mean and variance
  - particularly NULL model H0
- Using the model, we can compute probabilities under the NULL model and test this
- P-value
- significance

---
layout: false 
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice5}
set.seed(22)
mean = c(0.5, 0.5)
stddev = c(0.05, 0.5)
mult = 0.5
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2],  stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x = model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()
```
]
---
.pull-right[

### Interpretation

... what about here?

]


---
.pull-right[

### Interpretation

... what about here?

#### Tentative answer
This might require testing hypotheses that also look at the difference in  *variance*

]

???

- Another example...

---
layout: false
# Example: Protein expression

#### Setup

**Aim:** Investigate the relation between the Breast cancer transcriptomic and proteomics landscape

- Breast cancer tumour samples experiment
    + measure RNA expression
    + measure protein expression

???

- biopsies from patients

```{r, regression1}
alpha = 1.5
beta = 1.5
beta2 = 0.5
stddev = 1
rlin<-function(x, n=1, a=alpha, b=beta, s=stddev){
  m = a+b*x
  return(rnorm(n, m, s))
}

rlog<-function(x, n=1, a=alpha, b=beta){
  p = exp(-(a+b*x))
  return(rnorm(n, 1/(1+p), 0.2))
#  return(rbinom(n, 1, 1/(1+p)))
}

log<-function(x, n=1, a=alpha, b=beta){
  p = exp(-(a+b*x))
  return(1/(1+p))
}

x = rnorm(10000, 0, 2)
ylin1 = unlist(lapply(x, rlin))
ylin2 = unlist(lapply(x, function(x) rlin(x, s=10)))
ylog = unlist(lapply(x, rlog))
ylin3 = unlist(lapply(x, function(x) rlin(x, b=beta2)))

df = data.frame(x=x, ylin1  = ylin1, ylin2=ylin2,  ylog=ylog, ylin3=ylin3)
p1=ggplot(df, aes(x=x, y=ylin1)) +
  xlab("[RNA]") +
  ylab("[protein]") + 
  geom_point()

p2 = p1 + geom_abline(intercept=alpha, slope=beta, col="red")

p3 = ggplot(df, aes(x=x, y=ylin2)) +
  xlab("[RNA]") +
  ylab("[protein]") + 
  geom_point() + 
  geom_abline(intercept=alpha, slope=beta, col="red")

p4=ggplot(df, aes(x=x, y=ylog)) +
  xlab("RNA variation") +
  ylab("protein-RNA correlation") + 
  geom_point() + 
  stat_function(fun=log, col="red")

# p5 = ggplot(df) +
#   xlab("[RNA]") +
#   ylab("[protein]") + 
#   geom_point(aes(x=x, y=ylin1)) + 
#   geom_point(aes(x=x, y=ylin3)) 
# 
# 
# p6 = g= ggplot(df) +
#   xlab("[RNA]") +
#   ylab("[protein]") + 
#   geom_point(aes(x=x, y=ylin1), col="red") + 
#   geom_point(aes(x=x, y=ylin3), col="blue")
# 
# p7 = p6 +
#   geom_abline(intercept=alpha, slope=beta,  col="red") +
#   geom_abline(intercept=alpha, slope=beta2, col="blue") 
# 
# p8 = p6 
# for(b in seq(-0.75, 0.75, 0.25)){
#   p8 = p8 +
#     geom_abline(intercept=alpha, slope=beta+b,  col="red")+
#     geom_abline(intercept=alpha, slope=beta2+b, col="blue") 
# }
# 
# p9 = p6 
# for(b in seq(-0.05, 0.05, 0.05)){
#   p9 = p9 +
#     geom_abline(intercept=alpha, slope=beta+b,  col="red")+
#     geom_abline(intercept=alpha, slope=beta2+b, col="blue") 
# }
```
---

layout: true
# Example: Protein expression

.pull-left[
```{r, regression2}

p1
```
]
---
.pull-right[
### Discovery: 
- **Correlation** between Protein and RNA expression

{{content}}
]
--
  ![black box warning](assets/blackbox.jpg)

???

Actually, not going to through what correlation is here .. have to wait until the course session on this
---

.pull-right[
### Discovery: 
- **Correlation** between Protein and RNA expression

### Hypothesis: 
- RNA expression governs protein expression

{{content}}
]

???

- Common hypothesis that you don't need to measure protein expr because it just follows RNA expr
- These researchers work with proteomics and want to investigate this further

--

### Test
- **regression**
    + **fit** a **linear model** 
{{content}}

--
  ![black box warning](assets/blackbox.jpg)

???

- another black box -- what's a linea model
---
layout: false
layout: true
# Example: Protein expression

.pull-left[
```{r, regression3}
p2
```
]
---
.pull-right[
### Linear model

- Extend our previous model
    + Variation of $[Prot]$ approximately $N(\mu,\sigma)$  
    *and*
    + $\mu = \alpha +\beta [RNA]$ 
{{content}}
]

???
- recognize the equation for a straight line
  - $\beta$ is the slope of the line
  - $\alpha$ is intercept (where the line crosses the y-axis)
--
- Uses
  1. Simulation
      - validating methods
{{content}}
--
  2. Inference 
      - **regression**
          - optimal $\alpha$ and $\beta$ values
      - **significance test**
      
???
- $\beta$ tells how strongly y depend on x
- not always this simple
---
layout: false
layout: true
# Example: Protein expression

.pull-left[
```{r, regression4}
p3
```
]

---
.pull-right[
### Linear model

- Extend our previous model
    + Variation of $[Prot]$ approximately $N(\mu,\sigma)$  
    *and*
    + $\mu = \alpha +\beta [RNA]$ 
- Uses
  1. Simulation
      - validating methods
  2. Inference 
      - **regression**
          - optimal $\alpha$ and $\beta$ values
      - **significance test**
]


???
More realistic

These researchers found that there were indeed a portion of genes with consistently low correlation RNA  and protein expression



---
layout: false
layout: true
# Example: Protein expression

.pull-left[
```{r, regression5}
p4
```
]
---
.pull-right[
### Another hypothesis: 
- RNA-protein correlation is governed by RNA variation

### $\Leftarrow$ Result
- How should this be interpreted?

{{content}}
]

???

- another common hypothesis is that low correlation is just an effect of low RNA variation
- investigate 
- This is the plot
- But that's not a strainght line
- can't fit linear model

--
### Non-linear dependencies
- **Generalized linear models (GLMs)**
{{content}}
--
  ![black box warning](assets/blackbox.jpg)

]

??? 

- Another black box, which we leave to the the course session on this.
- Turn to yet another example

---
layout: false
# Example: GWAS

## Setup: 

Aim: Elucidate the genetic setup of IED condition

- Case-control cohort (N = 1000)
- Genome-wide SNP assay (P= 5000)

???

- [IED = Invented example disease .. revealed below]
- genome-wide association analysis
- setup...

---
layout: true
# Example: GWAS


.pull-left[
```{r, gwas, cache=TRUE, warning=FALSE}
chromlengths=c(
  247249719,
  242951149,
  199501827,
  191273063,
  180857866,
  170899992,
  158821424,
  146274826,
  140273252,
  135374737,
  134452384,
  132349534,
  114142980,
  106368585,
  100338915,
  88827254,
  78774742,
  76117153,
  63811651,
  62435964,
  46944323,
  49691432,
  154913754,
  57772954
)

m = length(chromlengths)
n=5000
gwas = data.frame(
  P=unlist(lapply(1:(n*m), function(x) runif(1,0,1))),
  BP=unlist(lapply(chromlengths, function(x) sample(1:x, n, replace =FALSE))),
  CHR = unlist(lapply(seq(1,m), function(x) rep(x, 5000))),
  SNP =  unlist(lapply(seq(1,m), function(x) "dummy"))
  )

manhattan(gwas, suggestiveline=FALSE, genomewideline=-log10(0.05))

```

]
---
.pull-right[

## Manhattan plot

- Individual association test for each SNP to the disease
- $-log_{10}$ P-value against genomic position
{{content}}
]

???
- individual association tests, e.g.,  using linear models, 
- standard significance level log(0.05) = 1.30103 (line)
- several hundred or even thousand associations
--


#### *Multiple tests*
- Repeated experiments are more likely to succeed under H0
{{content}}


???

- Discuss repeated dice throws
  - Getting a Yatzy on one throw vs getting it on a 1000 throws
- This manhattan plot is actually generated under the null model
- IED = invented example disease
--

#### Multiple test correction
- **Bonferroni, False Discovery Rate (FDR)**
{{content}}

--

  ![black box warning](assets/blackbox.jpg)

]

???

- There we have it a black box
- more on this in the course
- Now Go back to protein expression exampe

---
layout: false
# Example: Protein expression cntd.


### What other variables affect protein expression?

- How can we investigate that?

???

- Not only protein generation from RNA
- For example, maybe the breakdown of proteins by proteinases affect protein  expression
- How can we investigate?

*Comments for coming slides*: 

- We can make a 3-D plot
- include protease conc in our linear model 


---
layout: true
# Example: Protein expression cntd.

.pull-left[
```{r, multivariate}
library(plot3D)

nrow = 10000
alpha = 0.0
beta=1.2
gamma = -1.5
stddev = 1
rmul<-function(x, y, n=1, a=alpha, b=beta, c = gamma, s=stddev){
  m = a + b*x + c*y
  return(rnorm(n, m, s))
}

x = runif(nrow, 0, 10)
y = runif(nrow, 0, 10)
z = unlist(lapply(1:nrow, function(i) rmul(x[i], y[i])))
df = data.frame(x=x,y=y, z=z)
#df = data.frame(x=c(x,x),y=c(rep(1, length(x)), rep(2, length(x))), z=c(ylin1, ylin3))
#df = data.frame(x=c(x,x),y=c(rnorm(length(x), mean=1, sd=0.00001), rnorm(length(x), 2, 0.00001)), z=c(ylin1, ylin3))

scatter3D(x=df$x, z=df$z, y=df$y, xlab="[RNA]", ylab="[proteases]", zlab="[protein]", phi=20, theta=-50, zlim=c(-20,20))
#scatter3D(x=df$x, z=df$z, y=df$y)



# gamma=0.5
# rmlin<-function(x, z, n=1, a=alpha, b=beta, c=gamma, s=2){
#   m = a+b*x+c*x
#   return(rnorm(n, m, s))
# }
# 
# z= rnorm(10000, 1,1)
# ylin4 = unlist(lapply(1:length(z), function(n) rmlin(x[n], z[n])))
# df = data.frame(x=x,z=z, y=ylin4)
# 
# scatter3D(x=x,y=z, z=ylin4,xlab="x", ylab="[protein]", zlab="age", phi=10, theta=40)
```
]

---
.pull-right[
## Multivariate regression

{{content}} 
]

--

 ![black box warning](assets/blackbox.jpg) 
]

---
.pull-right[
## Multivariate regression
- We can visualize models with 3 variables
- and write an equation for this:  
$\begin{cases}[prot] &\sim N(\mu, \sigma) \\\mu = &\alpha+\beta [RNA] \\&\quad + \gamma [proteases] \end{cases}$
{{content}}

]

--
- For >3 variables, visualization breaks...
{{content}}
--
- but we can still write it as an equation:  
$\begin{cases}[prot] \sim& N(\mu, \sigma) \\\mu =& \alpha+\beta [RNA] \\&\quad+ \gamma [proteases]\\ &\quad + \delta [miRNA]\end{cases}$

---
layout: false
# Example: Protein expression cntd.
.pull-left[
```{r, multivariate2}
library(plot3D)

z = unlist(lapply(1:nrow, function(i) rmul(x[i], y[i], c=0)))
df = data.frame(x=x,y=y, z=z)
scatter3D(x=df$x, z=df$z, y=df$y, xlab="x", ylab="age", zlab="[protein]", phi=20, theta=-50, zlim=c(-20,20))

```
]
.pull-right[
#### **Issues with multivariate analysis**

1. **Feature selection**
    - Include only the most relevant variables in out model
{{content}}
]
--
2. **Overfitting**
    - Risk of modeling random noise in data
    - Solution: **regularization**
{{content}}

--
![black box warning](assets/blackbox.jpg) 

{{content}}

???

- All of this is of course black boxes
- that will be discussed further in the course

--

#### Really Big Data 
- **Machine Learning** on all data
{{content}}

???

- big data, i.e., extremely many variables
- analyze simultaneously
- Let's look at an example

---
layout: false
layout: true
# Example: Single cell expression data

### Unsupervised Machine Learning - Dimensional reduction
.pull-left[
```{r, pca}

pca = prcomp(iris[,1:4], scale=T)
pca_plot <- data.frame(x = pca$x[,"PC1"], y = pca$x[,"PC2"], Groups = factor(x=(iris$Species=="setosa"), labels=c("cluster 1","cluster 2")))
p=ggplot(pca_plot) +
  geom_point(aes(x=x, y=y, color=Groups)) +
  #scale_color_manual(values=mycolors) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw() +
  theme(title=element_text(face="bold")) 
print(p)


```
]

???

** comments for coming black box slide**
- ML is almost by nature a major black box that we will try to open in the course
- Another example...

---
.pull-right[

### Feature selection with PCA

- **linear transformation** of original variables into new *hidden variables*, components/factors

### Clustering
- identify groups based on hidden variables
]


---
.pull-right[

### Feature selection with PCA

- **linear transformation** of original variables into new *hidden variables*, components/factors

### Clustering
- identify groups based on hidden variables
  ![black box warning](assets/blackbox.jpg)

]

---
layout: false

# Example: Mass spectrometry
### Supervised Machine Learning -- classification

.left-custom[
- **Data**
  - *X*: mass spec proteins data
  - *Y*: disease progression outcome
- **Random Forest classifier**
  - Machine Learning algorithm
- **Training**
  - Learn to predict *Y* from *X*
]
.right-custom[
```{r RF1a, message=FALSE}
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)
plotfile = "assets/RF1.png"
if(!file.exists(plotfile)){
  DiagrammeR::grViz(paste0("digraph {

graph [layout = dot, rankdir = LR ]
node [shape = rectangle, style = filled, fontname = 'Helvetica-bold', fontsize = 24]
edge [ penwidth= 3, fontname = 'Helvetica-bold', fontsize = 32, labeldistance = 2, arrowsize = 2, penwidth = 5, minlen = 2 ] 

# label nodes
input [label = 'X \n Training \n Mass Spec Data \n N x P', fillcolor = PaleGreen, height = 5, width= 5 ]
RF [label = '\n Random Forest \n Classifier', shape = ovale, height = 3, width = 4, fillcolor = Black, fontcolor= White ]
output [label =  'Y \n Known \n outcome \n N x D', height = 5, width = 3, fillcolor = LightSalmon ]
select [label = 'Z \n Selected \n features \n N x Q', height = 5, width = 2, fillcolor = White, color = White, fontcolor = White ] 

subgraph {
rank = same; RF; select;
}
input -> RF -> output;
RF -> select [ headlabel = 'Feature Selection \n ', minlen = 6, color = White, fontcolor = White ];

}"))  %>%
    export_svg %>%
    charToRaw %>%
    rsvg_png(plotfile)
}

knitr::include_graphics(plotfile)
```
]

---

# Example: Mass spectrometry
### Supervised Machine Learning -- classification

.left-custom[
- **Data**
  - *X*: mass spec proteins data
  - *Y*: disease progression outcome
- **Random Forest classifier**
  - Machine Learning algorithm
- **Training**
  - Learn to predict *Y* from *X*
- **Feature selection**
  - *Z* = most important variables in *X*
]
.right-custom[
```{r RF3}
plotfile = "assets/RF3.png"
if(!file.exists(plotfile)){
  DiagrammeR::grViz(paste0("digraph {

graph [layout = dot, rankdir = LR ]
node [shape = rectangle, style = filled, fontname = 'Helvetica-bold', fontsize = 24]
edge [ penwidth= 3, fontname = 'Helvetica-bold', fontsize = 32, labeldistance = 2, arrowsize = 2, penwidth = 5, minlen = 2 ] 

# label nodes
input [label = 'X \n Training \n Mass Spec Data \n N x P', fillcolor = PaleGreen, height = 5, width= 5 ]
RF [label = '\n Random Forest \n Classifier', shape = ovale, height = 3, width = 4, fillcolor = Black, fontcolor= White ]
output [label =  'Y \n Known \n outcome \n N x D', height = 5, width = 3, fillcolor = LightSalmon ]
select [label = 'Z \n Selected \n features \n N x Q', height = 5, width = 2, fillcolor = AquaMarine, color = Black, fontcolor = Black ] 

subgraph {
rank = same; RF; select;
}
input -> RF -> output;
RF -> select [ headlabel = 'Feature Selection \n ', minlen = 6, color = Black, fontcolor = red ];

}"))  %>%
    export_svg %>%
    charToRaw %>%
    rsvg_png(plotfile)
}
knitr::include_graphics(plotfile)
```
]
---
# Example: Mass spectrometry
### Supervised Machine Learning -- classification

.left-custom[
- **Data**
  - *X*: mass spec proteins data
  - *Y*: disease progression outcome
- **Random Forest classifier**
  - Machine Learning algorithm
- **Training**
  - Learn to predict *Y* from *X*
- **Feature selection**
  - most important variables in *X*
- **Prediction**
  - outcome Y' from new data X'
]
.right-custom[
```{r RF2}
plotfile = "assets/RF2.png"
if(!file.exists(plotfile)){
  DiagrammeR::grViz(paste0("digraph {

graph [layout = dot, rankdir = LR ]
node [shape = rectangle, style = filled, fontname = 'Helvetica-bold', fontsize = 24]
edge [ penwidth= 3, fontname = 'Helvetica-bold', fontsize = 32, labeldistance = 2, arrowsize = 2, penwidth = 5, minlen = 2 ] 

# label nodes
input [label = 'X' \n NEW \n Mass Spec Data \n N x P', fillcolor = Chartreuse, height = 5, width= 5 ]
RF [label = 'TRAINED\n Random Forest \n Classifier', shape = ovale, height = 3, width = 4, fillcolor = Black, fontcolor= White ]
output [label =  'Y' \n PREDICTED \n outcome \n N x D', height = 5, width = 3, fillcolor = Coral ]
select [label = 'Z' \n Selected \n features \n N x Q', height = 5, width = 2, fillcolor = White, color = White, fontcolor = White ] 

subgraph {
rank = same; RF; select;
}
input -> RF -> output;
RF -> select [ headlabel = 'Feature Selection \n ', minlen = 6, color = White, fontcolor = White ];

}"))  %>%
    export_svg %>%
    charToRaw %>%
    rsvg_png(plotfile)
}
knitr::include_graphics(plotfile)
```
]

--
![black box warning](assets/blackbox.jpg)

???

And again this is a black box that you will learn more about in the course

---
layout: false
# Welcome to the course!

<br>
<br>

<center>
![black box warning](assets/blackboxgift.png)

???

So, once again, Welcome to the course -- this is our black gift box to you!
