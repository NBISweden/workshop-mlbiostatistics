---
title: "Introduction"
subtitle: "Machine learning and Biostatistics course"
author: "Bengt Sennblad"
institute: "NBIS"
date: "11/16/2020"
output: 
  xaringan::moon_reader:
    encoding: 'UTF-8'
    self_contained: false
    css: [default, metropolis, metropolis-fonts]
    lib_dir: 'libs'
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)

library(reticulate)
library(knitr)
knit_engines$set(python = reticulate::eng_python)  

library(ggplot2)
library(qqman)
```

# Content/aim



.pull-left[
## What do we want you to gain from this course?
- Learn a selection statistical methods

{{content}}
]
--

#### ...but, more importantly, be able to
- by yourselves explore statistics
  - find alternative methods
  - get a intuitive understanding
  - evaluate assumptions
      + know when to apply
      + know when not to apply

--
.pull-right[
### How?
{{content}}
]
--
- Go from <span style="background-color:black"><span style="color:white">_black box_</span></span> to understanding
    - several levels
      - models = assumptions
      - tests -- test statistics
      - overfitting
{{content}}
--
- Appreciate theory as a ❤️*friend*❤️
    - help understanding assumptions
    - help when evaluating methods
{{content}}
--
- Provide a _#framework#_ for learning
    - statistical thinking
    - basic math language
      - not afraid of equations
      - know some basic terminology

---
class: inverse, middle, center
<html><div style='float:left'></div><hr color='#EB811B' size=1px width=800px></html> 

# Why Statistics?
## Examples


---

# Example: Mouse knockout experiment

### Setup
Hypothesis: The low density lipoprotein (LDL) receptor gene *LDLR* affects the prevalence of hypercholsterolaemia.

- 10 wildtype (wt) mice
- 10 *LDLR* knockout (KO) mice

Measure plasma concentration of total cholesterol at one time point after feeeding on high fat diet.

???

low-density lipoprotein
---
layout: true
# Example: Mouse knockout experiment
.pull-left[
```{r mice1}
set.seed(85)

mean = c(0.55, 0.45)
stddev = c(0.025, 0.025)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)
row.names(mice)=c(paste0("KO", 1:10), paste0("wt", 1:10))

ggplot(mice, aes(y=prevalence, x=sample(row.names(mice))))+
  xlab("mice") +
  geom_point() +
  theme_bw()

```
]
---
.pull-right[
### Visualize results in a plot

]
---
.pull-right[
### Visualize results in a plot
- Not so informative!
- Let's improve
]
---
layout: false
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice1a}
ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a **boxplot**
]

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a boxplot
  ![black box warning](assets/blackbox.jpg)
]

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a boxplot
  - Visualize **distribution** of sample values using **descriptive statistics**
      - *mean* 
      - *quartiles*
]
---
.pull-right[

### Interpretation

Intuitively, there is a clear difference between KO and wt in this plot...
]
???
- 

-- 
.pull-right[
What if the result had looked different?

]
---
layout: false
# Example: Mouse knockout experiment

.pull-left[
```{r mice3}
set.seed(85)
mean = c(0.5, 0.5)
stddev = c(0.025, 0.025)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]
.pull-right[

### Interpretation

...equally clearly, here is no difference between KO and wt...

]
---
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice4}
set.seed(85)
mean = c(0.525, 0.475)
stddev = c(0.05, 0.05)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]

---
.pull-right[

### Interpretation

... Now, the difference is less clear

{{content}}
]
--

... we get uncertain

{{content}}

--
... Uncertainty best measured as probabilities!

{{content}}
--

... with probabilities we can perform a **statistical test** for difference

{{content}}
--
  ![black box warning](assets/blackbox.jpg)
]

---
.pull-right[
### Statistical tests

#### Model
- Abstractions of reality  probabilities
- Simplifying assumptions
  - Variation approximately $N(\mu,\sigma)$
    
#### Hypotheses
- H0: $\mu$ and $\sigma$ same for KO and wt 
- H1: $\mu$ might be different 

#### Test
- **P-value** $(p)$ =  probability of the observed or more extreme difference  under H0
- **significance level** $p\leq \alpha$
]

---
layout: false 
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice5}
set.seed(22)
mean = c(0.5, 0.5)
stddev = c(0.05, 0.5)
mult = 0.5
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2],  stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x = model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()
```
]
---
.pull-right[

### Interpretation

... what about here?

]


---
.pull-right[

### Interpretation

... what about here?

#### Tentative answer
This might require testing hypotheses that also look at the difference in  *variance*

]


---
layout: false
# Example: Protein expression

#### Setup

**Aim:** Investigate the relation between the Breast cancer transcriptomic and proteomics landscape

- Breast cancer tumour samples experiment
    + measure RNA expression
    + measure protein expression



```{r, regression1}
alpha = 1.5
beta = 1.5
beta2 = 0.5
stddev = 1
rlin<-function(x, n=1, a=alpha, b=beta, s=stddev){
  m = a+b*x
  return(rnorm(n, m, s))
}

rlog<-function(x, n=1, a=alpha, b=beta, p){
  p = exp(-(a+b*x))
  return(rnorm(n, 1/(1+p), 0.2))
#  return(rbinom(n, 1, 1/(1+p)))
}

log<-function(x, n=1, a=alpha, b=beta, p){
  p = exp(-(a+b*x))
  return(1/(1+p))
}

x = rnorm(10000, 0, 2)
ylin1 = unlist(lapply(x, rlin))
ylin2 = unlist(lapply(x, function(x) rlin(x, s=10)))
ylog = unlist(lapply(x, rlog))
ylin3 = unlist(lapply(x, function(x) rlin(x, b=beta2)))

df = data.frame(x=x, ylin1  = ylin1, ylin2=ylin2,  ylog=ylog, ylin3=ylin3)
p1=ggplot(df, aes(x=x, y=ylin1)) +
  xlab("[RNA]") +
  ylab("[protein]") + 
  geom_point()

p2 = p1 + geom_abline(intercept=alpha, slope=beta, col="red")

p3 = ggplot(df, aes(x=x, y=ylin2)) +
  xlab("[RNA]") +
  ylab("[protein]") + 
  geom_point() + 
  geom_abline(intercept=alpha, slope=beta, col="red")

p4=ggplot(df, aes(x=x, y=ylog)) +
  xlab("protein variation") +
  ylab("protein-RNA correlation") + 
  geom_point() + 
  stat_function(fun=log, col="red")

# p5 = ggplot(df) +
#   xlab("[RNA]") +
#   ylab("[protein]") + 
#   geom_point(aes(x=x, y=ylin1)) + 
#   geom_point(aes(x=x, y=ylin3)) 
# 
# 
# p6 = g= ggplot(df) +
#   xlab("[RNA]") +
#   ylab("[protein]") + 
#   geom_point(aes(x=x, y=ylin1), col="red") + 
#   geom_point(aes(x=x, y=ylin3), col="blue")
# 
# p7 = p6 +
#   geom_abline(intercept=alpha, slope=beta,  col="red") +
#   geom_abline(intercept=alpha, slope=beta2, col="blue") 
# 
# p8 = p6 
# for(b in seq(-0.75, 0.75, 0.25)){
#   p8 = p8 +
#     geom_abline(intercept=alpha, slope=beta+b,  col="red")+
#     geom_abline(intercept=alpha, slope=beta2+b, col="blue") 
# }
# 
# p9 = p6 
# for(b in seq(-0.05, 0.05, 0.05)){
#   p9 = p9 +
#     geom_abline(intercept=alpha, slope=beta+b,  col="red")+
#     geom_abline(intercept=alpha, slope=beta2+b, col="blue") 
# }
```
---

layout: true
# Example: Protein expression

.pull-left[
```{r, regression2}

p1
```
]
---
.pull-right[
### Discovery: 
- **Correlation** between Protein and RNA expression

{{content}}
]
--
  ![black box warning](assets/blackbox.jpg)

???

Actually, not going to through this here .. have to wait until the session on this
---

.pull-right[
### Discovery: 
- **Correlation** between Protein and RNA expression

### Hypothesis: 
- RNA expression governs protein expression

{{content}}
]
--

### Test
- **regression**
    + **fit** a **linear model** 
{{content}}

--
  ![black box warning](assets/blackbox.jpg)

---
layout: false
layout: true
# Example: Protein expression

.pull-left[
```{r, regression3}
p2
```
]
---
.pull-right[
### Linear model

- Extend our previous model
    + Variation of $[Prot]$ approximately $N(\mu,\sigma)$  
    *and*
    + $\mu = \alpha +\beta [RNA]$ 
{{content}}
]

???
Maybe this can suffice as hierarchical model example instead...?
--
- Uses
  1. Simulation
      - validating methods
{{content}}
--
  2. Inference 
      - **regression**
          - optimal $\alpha$ and $\beta$ values
      - **significance test**
      
???
- $\beta$ tells how strongly y depend on x
- not always this simple
---
layout: false
layout: true
# Example: Protein expression

.pull-left[
```{r, regression4}
p3
```
]

---
.pull-right[
### Linear model

- Extend our previous model
    + Variation of $[Prot]$ approximately $N(\mu,\sigma)$  
    *and*
    + $\mu = \alpha +\beta [RNA]$ 
- Uses
  1. Simulation
      - validating methods
  2. Inference 
      - **regression**
          - optimal $\alpha$ and $\beta$ values
      - **significance test**
]


???
More realistic

These researchers found that there were a portion of genes low correlation RNA  and protein expression



---
layout: false
layout: true
# Example: Protein expression

.pull-left[
```{r, regression5}
p4
```
]
---
.pull-right[
### Another hypothesis: 
- RNA-protein correlation is governed by RNA variation

### $\Leftarrow$ Result
- How should this be interpreted?

{{content}}
]
--
### Non-linear dependencies
- **Generalized linear models (GLMs)**
{{content}}
--
  ![black box warning](assets/blackbox.jpg)

]

---
layout: false
# Example: GWAS

## Setup: 

Aim: Elucidate the genetic setup of IMD condition

- Case-control cohort (N = 1000)
- Genome-wide SNP assay (P= 5000)



---
layout: true
# Example: GWAS


.pull-left[
```{r, gwas, cache=TRUE, warning=FALSE}
chromlengths=c(
  247249719,
  242951149,
  199501827,
  191273063,
  180857866,
  170899992,
  158821424,
  146274826,
  140273252,
  135374737,
  134452384,
  132349534,
  114142980,
  106368585,
  100338915,
  88827254,
  78774742,
  76117153,
  63811651,
  62435964,
  46944323,
  49691432,
  154913754,
  57772954
)

m = length(chromlengths)
n=5000
gwas = data.frame(
  P=unlist(lapply(1:(n*m), function(x) runif(1,0,1))),
  BP=unlist(lapply(chromlengths, function(x) sample(1:x, n, replace =FALSE))),
  CHR = unlist(lapply(seq(1,m), function(x) rep(x, 5000))),
  SNP =  unlist(lapply(seq(1,m), function(x) "dummy"))
  )

manhattan(gwas, suggestiveline=FALSE, genomewideline=-log10(0.05))

```

]
---
.pull-right[

## Manhattan plot

- Individual association test for each SNP and disease
- $-log_{10}$ P-value against genomic position
{{content}}
]

???
-log(0.05) = 1.30103 (line)
- several houndred or even thousand associations
--


#### *Multiple tests*
- Repeated experiments are more likely to succeed under H0
{{content}}


???

- Discuss repeated dice throws
- This manhattan plot is actually generated under the null model
--

#### Multiple test correction
- **Bonferroni, False Discovery Rate (FDR)**
{{content}}

--

  ![black box warning](assets/blackbox.jpg)

]


---
layout: false
# Example: Protein expression cntd.


### What other variables affect protein expression?

- How can we investigate that?

---
layout: true
# Example: Protein expression cntd.

.pull-left[
```{r, multivariate}
library(plot3D)

nrow = 10000
alpha = 0.0
beta=1.2
gamma = -1.5
stddev = 1
rmul<-function(x, y, n=1, a=alpha, b=beta, c = gamma, s=stddev){
  m = a + b*x + c*y
  return(rnorm(n, m, s))
}

x = runif(nrow, 0, 10)
y = runif(nrow, 0, 10)
z = unlist(lapply(1:nrow, function(i) rmul(x[i], y[i])))
df = data.frame(x=x,y=y, z=z)
#df = data.frame(x=c(x,x),y=c(rep(1, length(x)), rep(2, length(x))), z=c(ylin1, ylin3))
#df = data.frame(x=c(x,x),y=c(rnorm(length(x), mean=1, sd=0.00001), rnorm(length(x), 2, 0.00001)), z=c(ylin1, ylin3))

scatter3D(x=df$x, z=df$z, y=df$y, xlab="[RNA]", ylab="[proteases]", zlab="[protein]", phi=20, theta=-50, zlim=c(-20,20))
#scatter3D(x=df$x, z=df$z, y=df$y)



# gamma=0.5
# rmlin<-function(x, z, n=1, a=alpha, b=beta, c=gamma, s=2){
#   m = a+b*x+c*x
#   return(rnorm(n, m, s))
# }
# 
# z= rnorm(10000, 1,1)
# ylin4 = unlist(lapply(1:length(z), function(n) rmlin(x[n], z[n])))
# df = data.frame(x=x,z=z, y=ylin4)
# 
# scatter3D(x=x,y=z, z=ylin4,xlab="x", ylab="[protein]", zlab="age", phi=10, theta=40)
```
]

---
.pull-right[
## Multivariate regression

{{content}} 
]

--

 ![black box warning](assets/blackbox.jpg) 
]

---
.pull-right[
## Multivariate regression
- We can visualize models with 3 variables
- and write an equation for this:  
$\begin{cases}[prot] &\sim N(\mu, \sigma) \\\mu = &\alpha+\beta [RNA] \\&\quad + \gamma [proteases] \end{cases}$
{{content}}
]

--
- For >3 variables, visualization breaks...
{{content}}
--
- but we can still write it as an equation:  
$\begin{cases}[prot] \sim& N(\mu, \sigma) \\\mu =& \alpha+\beta [RNA] \\&\quad+ \gamma [proteases]\\ &\quad + \delta [miRNA]\end{cases}$

---
layout: false
# Example: Protein expression cntd.
.pull-left[
```{r, multivariate2}
library(plot3D)

z = unlist(lapply(1:nrow, function(i) rmul(x[i], y[i], c=0)))
df = data.frame(x=x,y=y, z=z)
scatter3D(x=df$x, z=df$z, y=df$y, xlab="x", ylab="age", zlab="[protein]", phi=20, theta=-50, zlim=c(-20,20))

```
]
.pull-right[
#### Issues with multivariate analysis

1. **Feature selection**
    - Include only the most relevant variables in out model
{{content}}
]
--
2. **Overfitting**
    - Risk of modeling random noise in data
    - Solution: **regularization** or **Bayesian approaches**
{{content}}
--
![black box warning](assets/blackbox.jpg) 

{{content}}
--

#### Really Big Data 
- **Machine Learning** on all data
{{content}}


---
layout: false
layout: true
# Example: Single cell expression data

### Machine learning - Dimensional reduction
.pull-left[
```{r, pca}

pca = prcomp(iris[,1:4], scale=T)
pca_plot <- data.frame(x = pca$x[,"PC1"], y = pca$x[,"PC2"], Groups = factor(x=(iris$Species=="setosa"), labels=c("cluster 1","cluster 2")))
p=ggplot(pca_plot) +
  geom_point(aes(x=x, y=y, color=Groups)) +
  #scale_color_manual(values=mycolors) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw() +
  theme(title=element_text(face="bold")) 
print(p)


```
]
---
.pull-right[

### Feature selection with PCA

- **linear transformation** of original variables into new *hidden variables*, components/factors

### Clustering
- identify groups based on hidden variables
]
---
.pull-right[

### Feature selection with PCA

- **linear transformation** of original variables into new *hidden variables*, components/factors

### Clustering
- identify groups based on hidden variables
  ![black box warning](assets/blackbox.jpg)

]
---
layout: false
layout: true

# Example: Species classification

## Machine learning -- Random forests

- **Train** ANN to learn feature selection and model selection to predict the right answer.

.pull-left[

{{content}}

]
.pull-right[

```{python pyann1, echo=F, out.height="100%", fig.align='center', eval=FALSE}
import numpy as np
import matplotlib.pyplot as plt
from draw_neural_net import draw_neural_net

#--------[1] Input data
#dataset = np.mat('-1 -1 -1; -1 1 1; 1 -1 1; 1 1 -1')
#X_train = dataset
#y_train = np.mat('0; 1; 1; 0')
#-----2-2-1
layer_sizes= (4,5,5,5,3)

output = [ "COW=0", "DOG=1", "CAT=0" ]

fig = plt.figure(figsize=(12, 12))

ax = fig.gca()
ignore=ax.axis('off')
nodeFontSize = 35

draw_neural_net(ax,
                layerSizes=layer_sizes, 
                outPrefix = output,
                nodePrefix = "",
                inNodePrefix = "",
                nodeFontSize=nodeFontSize,
                hideBias = True)
plt.show()
```


```{r, ann2, fig.height=4,eval=FALSE, EVAL=FALSE}
layers =list("I"=4, "H1"=5, "H2"=4,"O"=3)
ylab=c("cat=0", "dog=1", "cow=0")
plotAnn(layers, cex=1, withY=T, ylab=ylab)
```
]

---

```{r, ann1, results='asis', eval=FALSE} 
for(row in 1:2){
  cat("<br>")
}
cat("\n")
cat('<p style="text-align: center; font-size:2em;">')
  cat("Some Big Data \nmultivariate input X!\n")
cat('</p>')
cat("\n")
```

---
##### Training input  could be, e.g., images (pixels)
```{r, ann3, out.width=200, fig.align='right', eval=FALSE} 
#, out.width="50%", fig.align='center', fig.asp=0.5, dpi=600}
knitr::include_graphics('assets/Nora.jpeg')

```

---

#####  ... or training input could be a genome sequence...
```{r, ann5, results='asis', eval=FALSE} 
cat(paste0("...", paste0(sample(c("A", "C","G","T"), 27, replace=TRUE), collapse="")))
cat("\n")
for(row in 1:7){
  cat(paste0(sample(c("A", "C","G","T"), 30, replace=TRUE), collapse=""))
  cat("\n")
}
cat(paste0(paste0(sample(c("A", "C","G","T"), 27, replace=TRUE), collapse=""), "..."))
```
---

#####  This is of course a major...
![black box warning](assets/blackbox.jpg)

???
This is of course a major black box

---
layout: false
# Welcome to the course!

<br>
<br>

<center>
![black box warning](assets/blackboxgift.png)
