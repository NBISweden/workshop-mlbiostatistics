---
editor: source
---

# Parametric tests

In the previous chapter we computed the sampling distribution using resampling techniques to be able to perform hypothesis tests. If the null distribution was already known, or could be computed based on a few assumptions, resampling would not be necessary.

We can follow the same steps as before to perform a hypothesis test:
  
  1.  Define $H_0$ and $H_1$
  2.  Select an appropriate significance level, $\alpha$
  3.  Select appropriate test statistic, $T$, and compute the observed value, $t_{obs}$
  4.  Assume that the $H_0$ and derive the null distribution of the test statistic based on appropriate assumptions.
5.  Compare the observed value, $t_{obs}$, with the null distribution and compute a p-value. The p-value is the probability of observing a value at least as extreme as the observed value, if $H_0$ is true.
6.  Based on the p-value either accept or reject $H_0$.

In this section we will present a few common situation in which the null distribution can be described parametrically.

## One sample, mean

A one sample test of means compares the mean of one sample to a prespecified value.

For example, we might know that the weight of a mouse on normal diet is normally distributed with mean 24.0 g and standard deviation 3 g and want to compare the weight of a sample of 10 mice on high-fat diet to the known mean value for mice on normal diet.

The hypotheses:

$$H_0: \mu = \mu_0$$
$$H_1: \mu \neq \mu_0$$

The alternative hypothesis, $H_1,$ above is for the two-sided hypothesis test. Other options are the one-sided $H_1$; $H_1: \mu > \mu_0$ or $H_1: \mu < \mu_0$.

If $$X \sim N(\mu, \sigma)$$ (this could for example be the weight of a mouse on high-fat diet) then the sample mean $$\bar X \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right).$$

If $\sigma$ is known under the null hypothesis, then the test statistic

$$Z = \frac{\bar X - \mu_0}{\frac{\sigma}{\sqrt{n}}}$$ is normally distributed, $\sim N(0,1)$.

In many situations the the population standard deviation is not known, but can instead be estimated from the sample. For large sample size $n$, $\sigma$ can be replaced by the sample standard deviation $s$ and the test statistic will still be normally distributed according to the **central limiit theorem***.

For small $n$ and unknown $\sigma$, we can use the **one-sample t-test**, that uses the test statistic

$$T = \frac{\bar X - \mu_0}{\frac{s}{\sqrt{n}}},$$

which is t-distributed with $df=n-1$ degrees of freedom.

Once we have an appropriate test statistic, $T$ with known distribution, we can compute the observed value, $t_{obs}$ and use the null distribution to compute the p-value, $P(|T| \geq |t_{obs}|)$.

In R, the functions `pnorm` and `pt` are useful for computing these probabilities. The **one-sample t-test** can also be performed using the function `t.test`like this;

```{r, echo=TRUE}
##The observed 10 mouse weights;
x <- c(25, 30, 23, 18, 31, 24, 39, 26, 36, 29, 23, 32)
## Under the null hypothesis mu=24
##Perform the t-test to investigate if the null hypotheis can be accepted, i.e.
##if the sample comes from a normal distribution with expected value 24.
t.test(x, mu=24)
```

## Two samples, mean

A two sample test of means is used to determine if two population means are equal.

Two independent samples are collected (one from each population) and the means are compared. This test can for example be used to determine if a treatment group is different compared to a control group, in terms of the mean of a property of interest.

The null hypothesis;

$$H_0: \mu_2 = \mu_1$$ The alternative hypothesis can either be two sided

$$H_1: \mu_2 \neq \mu_1$$ or one sided

$$H_1: \mu_2 > \mu_1 \textrm{ or } H_1: \mu_2 < \mu_1.$$

Assume observations from both populations are normally distributed;

$$
\begin{aligned}
X_1 \sim N(\mu_1, \sigma_1) \\
X_2 \sim N(\mu_2, \sigma_2)
\end{aligned}
$$ 
Then it follows that the sample means will also be normally distributed;

$$
\begin{aligned}
\bar X_1 \sim N(\mu_1, \sigma_1/\sqrt{n_1}) \\
\bar X_2 \sim N(\mu_2, \sigma_2/\sqrt{n_2})
\end{aligned}
$$

The mean difference $D = \bar X_2 - \bar X_1$ is thus also normally distributed:

$$D = \bar X_2 - \bar X_1 = N\left(\mu_2-\mu_1, \sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}\right)$$

If $H_0$ is true: $$D = \bar X_2 - \bar X_1 = N\left(0, \sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}\right)$$

The test statistic: $$Z = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}}$$ is standard normal, i.e. $Z \sim N(0,1)$.

However, note that the test statistic require the standard deviations $\sigma_1$ and $\sigma_2$ to be known.

What if the population standard deviations are not known?

If the sample sizes are large, we can replace the known standard deviations with our sample standard deviations and according to the **central limit theorem assume** that

$$Z = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{s_2^2}{n_2} + \frac{s_1^2}{n_1}}} \sim N(0,1)$$

and proceed as before.

<!-- Here $n_1=n_2=12$ which is not very large. -->

For small sample sizes the test statistic will be t-distributed.

$$t = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{s_2^2}{n_2} + \frac{s_1^2}{n_1}}}$$

If it can be assumed that $X_1$ and $X_2$ both are normally distributed and have equal variances, Studen't t-test can be used. For equal variances the pooled sample variance can be computed;

$$s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}$$
  
and the test statistic

$$t = \frac{\bar X_1 - \bar X_2}{\sqrt{s_p^2(\frac{1}{n_1} + \frac{1}{n_2})}}$$
  
is t-distributed with $n_1+n_2-2$ degrees of freedom.

For unequal variances Welch's t-test can instead be used.

The t-test is implemented in R, e.g. in the function `t.test` in the R-package `stats`, both Student's t-test for equal variances and Welch's t-test for unequal variances.

## One sample, proportions

:::: {#exm-parampollen}
# Pollen allergy

Let's get back to the pollen example!

Assume that the proportion of pollen allergy in Sweden is known to be $0.3$. Observe 100 people from Uppsala, 42 of these were allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala $\pi > 0.3$?
::::

1. Null and alternative hypotheses

   $$H_0:\, \pi=\pi_0$$

    $$H_1:\, \pi>\pi_0,$$

    where $\pi_0=0.30$ in this example. Other potential alternative hypothesis are $H_1: \pi<\pi_0$ or $H_1:\pi \neq \pi_0,$ but in this particular example we are only interested in the alternative that $\pi > \pi_0$.

2. Significance level

    Set the signifgicance level to $\alpha=0.05$.

3. Test statistic

    Here, we will use $X$, the number of allergic persons in a random sample of size $n=100$. The observed value is $x_{obs} = 42$.

4. Null distribution

    $X$ is binomially distributed, so there is no need to use resampling here, we can use the binomial distribution to answer the question.

5. p-value

    The probability of $x_{obs}$ or something higher,

    $P(X \geq 42) = 1 - P(X \leq 41)$ = \[`1-pbinom(41,100,0.3)`\] = `r format(1-pbinom(41,100,0.3), digits=4)`

6. Accept or reject $H_0$?

    As $p<0.05$ $H_0$ is rejected and we conclude that there is reason to believe that the proportion of pollen allergic in Uppsala is higher than 0.3.

This p-value can also be computed using the exact binomial test;

```{r, echo=TRUE}
binom.test(42, 100, 0.3, alternative="greater")
```

An alternative approach is to use the **central limit theorem** and use the normal approximation.

<!-- # The central limit theorem -->
<!-- The sum of $n$ independent and equally distributed random variables -->
<!-- is normally distributed, if $n$ is large enough. -->

As a result of the central limit theorem, the distribution of number or proportion of allergic individuals in a sample of size $n$ is approximately normal. At least if the sample is large enough. A rule of thumb is that the sample size should be $n>30$.

Here, the sample size is 100!

This test of proportions using the normal approximation is implemented in in the r-function `prop.test`.

```{r}
prop.test(42, 100, 0.3, alternative="greater")
```

But can of course also be calculated using only the normal distribution table.

The normal distribution has two parameters, mean and standard deviation.

From the binomial distribution we know that $E[X] = n\pi$ and $var(X) = n\pi(1-\pi)$.

The standard error of $X$ is

$$SE=\sqrt{n\pi(1-\pi)}$$

If $H_0$ is true $\pi=\pi_0$ and

$$X \sim N\left(n\pi_0, \sqrt{n\pi_0(1-\pi_0)}\right)$$ <!-- An appropriate test statistic is --> <!-- $$Z = \frac{X-n\pi_0}{\sqrt{n\pi_0(1-\pi_0)}}$$ --> <!-- $Z \in N(0,1)$ which makes probabilities easy to compute. -->

With this null distribution and ur observed value $x_{obs} = 42$, the p-value can be computed.

$p = P(X \geq 42)$

As we are now approximating a discrete distribution using a continuous distribution we need to use a trick called *continuiuty correction*, which simply means that we let each integer be represented by y a region $\pm 0.5$ from its value. Hence,

`r zobs <- (41.5-30)/sqrt(100*0.3*(1-0.3))`

$$p = P(X \geq 42) = [continuity \; correction] = $$ $$P(X \geq 41.5) = P\left(Z \geq \frac{41.5 - n\pi_0}{\sqrt{n\pi_0(1-\pi_0)}}\right) = $$ $$P\left(Z \geq \frac{41.5-30}{\sqrt{100*0.3*(1-0.3)}}\right) = P(Z \geq 2.51) = 1 - P(Z \leq 2.51) = [Z-table] = 0.0060$$

<!-- The p-value is the probability of the observed value, or something more extreme, if the null hypothesis is true. If the computed probability is below $\alpha=0.05$ our significance threshold, $H_0$ will be rejected. -->

<!-- $$p = P(P \geq \pi_0) = P(Z \geq Z_{obs}) = P(Z \geq` `r format(zobs, digits=3)`) = 1 - P(Z \leq `r format(zobs, digits=3)`) = [table] = 1 - `r format(pnorm(2.62), digits=3)` = `r format(pnorm(2.62, lower.tail=FALSE), digits=3)`$$ -->

As `r format(pnorm(zobs, lower.tail=FALSE), digits=3)`\<0.05 we reject $H_0$ and conclude that there is reason to believe that the proportion of allergic in Uppsala is greater than 0.3.





## Two samples, proportions

$$
\begin{aligned}
H_0: \pi_1 - \pi_2 = 0\\
H_1: \pi_1 - \pi_2 \neq 0
\end{aligned}
$$

Alternatively, a one sided alternative hypothesis can be used; 
$$H_1: \pi_1 - \pi_2 >0\textrm{ or }H_1: \pi_1 - \pi_2 < 0.$$ 

<!-- $$P_1 - P_2 \sim N\left(0, \sqrt{\pi(1-\pi)\left (\frac{1}{n_1} + \frac{1}{n_2}\right)} \right)$$ -->

<!-- where $\pi$ is the  -->

If n is large the normal approximation can be used, an appropriate test statistic is

$$Z = \frac{P_1 - P_2}{\sqrt{P(1-P)\left (\frac{1}{n_1} + \frac{1}{n_2}\right)}},$$

where $P$ is the proportion in the merged sample of size $n_1 + n_2$. $Z \in N(0,1)$ and p-value can be computed using the standard normal distribution.

Also the two sample proportions test is implemented in the function `prop.test`.

## Variance

The test of equal variance in two groups is based on the null hypothesis

$$H_0: \sigma_1^2 = \sigma_2^2$$
  
If the two samples both come from populations with normal distributions, the sample variances (also random variables) are

$$S_1^2 = \frac{1}{n_1-1} \sum_{i=1}^{n_1} (X_{1i}-\bar X_1)^2\\
S_2^2 = \frac{1}{n_2-1} \sum_{i=1}^{n_2} (X_{2i}-\bar X_2)^2$$
  
It can be shown that $\frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2(n_1-1)$ and $\frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)$.

Hence, the test statistic for comparing the variances of two groups

$$F = \frac{S_1^2}{S_2^2}$$ is $F$-distributed with $n_1-1$ and $n_2-1$ degrees of freedom.

In R a test of equal variances can be performed using the function `var.test`.
