---
editor: source
title: "Statistical inference, part I"
author: "Eva Freyhult"
date: "2024-04-23"
date-format: long
institute: NBIS, SciLifeLab
format: 
  revealjs:
    slide-number: true
    self-contained: true
    theme: [default, custom.scss]
---

```{r, include=FALSE}
require(tidyverse)
require(ggplot2)
require(reshape2)
require(knitr)
require(kableExtra)
library(gt)
require(latex2exp)
knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, cache=TRUE, error=FALSE, warnings=FALSE, dpi=600)
options(digits=2)
```

## Introduction to hypothesis tests 

::: {.r-fit-text}

**Statistical inference** is to draw conclusions regarding properties of a population based on observations of a random sample from the population.

A **hypothesis test** is a type of inference about evaluating if a hypothesis about a population is supported by the observations of a random sample (i.e by the data available).

Typically, the hypotheses that are tested are assumptions about properties of a population, such as proportion, mean, mean difference, variance etc.
:::

## The null and alternative hypothesis 

::: {.columns}
::: {.column width="50%"}
### Null hypothesis, $H_0$

$H_0$ is in general neutral

* *no change*
* *no difference between groups*
* *no association*

In general we want to show that $H_0$ is false.

```{r H0box}
#| fig.width: 2.5
#| fig.height: 2.5
#| out.width: 50%
set.seed(123)
df <- data.frame(x=rnorm(100,30,5), group=sample(c("A", "B"), size = 100, replace=TRUE, prob=c(0.5, 0.5))) 
df |> ggplot(aes(x=group, y=x, color=group)) + geom_boxplot() + theme_bw() + xlab("Group") + ylab("") + theme(legend.position="none")
```

:::

::: {.column width="50%"}
:::{.fragment}

### Alternative hypothesis, $H_1$

$H_1$ expresses what the researcher is interested in 

* *the treatment has an effect*
* *there is a difference between groups*
* *there is an association*

The alternative hypothesis can also be directional

* *the treatment has a positive effect*

```{r H1box}
#| fig.width: 2.5
#| fig.height: 2.5
#| out.width: 50%
set.seed(123)
df1 <- df |> mutate(x=x+rnorm(nrow(df),c(A=15, B=0)[group],1))
df1 |> ggplot(aes(x=group, y=x, color=group)) + geom_boxplot() + theme_bw() + xlab("Group") + ylab("") + theme(legend.position="none")
```
:::
:::
::::

## To perform a hypothesis test

::: {.smaller style="font-size: 35px" .incremental}
1.  Define $H_0$ and $H_1$
2.  Select an appropriate significance level, $\alpha$
3.  Select appropriate test statistic, $T$, and compute the observed value, $t_{obs}$
4.  Assume that the $H_0$ is true and compute the sampling distribution of $T$.
5.  Compare the observed value, $t_{obs}$, with the computed sampling distribution under $H_0$ and compute a p-value. The p-value is the probability of observing a value at least as extreme as the observed value, if $H_0$ is true.
6.  Based on the p-value either accept or reject $H_0$.
:::



## Null distribution

A **sampling distribution** is the distribution of a sample statistic. The sampling distribution can be obtained by drawing a large number of samples from a specific population.

The **null distribution** is a sampling distribution when the null hypothesis is true.

```{r examplenull, out.width="70%", fig.show="hold", fig.width=5, fig.align="center"}
x<-seq(-3,3,0.01)
df <- data.frame(x=x, f=dnorm(x, 0, 1))
plot(ggplot(df, aes(x=x, y=f)) + geom_line() + theme_bw() + xlab("x") + ylab("f(x)"))
```

## p-value

The p-value is the probability of the observed value, or something more extreme, if the null hypothesis is true.

```{r examplepval, out.width="70%", fig.align="center", fig.show="hold", fig.width=5, warning=FALSE}
pl <- ggplot(df, aes(x=x, y=f)) + geom_line() + theme_bw() + xlab("x") + ylab("f(x)") + geom_area(data=df %>% filter(x>1.5)) + annotate("label",label=TeX("P( X$\\geq$xobs )"), x=1.8, y=0.11, hjust=0)
plot(pl + scale_x_continuous(breaks=c(-2,0,1.5,2), labels=c("-2","0","xobs", "2")) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_line(color = c("grey92", "grey92", NA, "grey92"))))
```

## p-value

The p-value is the probability of the observed value, or something more extreme, if the null hypothesis is true.

```{r examplepval2, out.width="70%", fig.align="center", fig.show="hold", fig.width=5, warning=FALSE}
pl <- pl + geom_area(data=df %>% filter(x<(-1.5))) + annotate("label",label=TeX("P(X$\\leq$-xobs)"), x=-1.8, y=0.11, hjust=1)
plot(pl + scale_x_continuous(breaks=c(-2,-1.5,0,1.5,2), labels=c("-2", "-xobs","0","xobs", "2")) +
     theme(panel.grid.minor = element_blank(),
              panel.grid.major.x = element_line(color = c("grey92", NA, "grey92", NA, "grey92"))))
```

## Error types {.auto-animate}

A hypothesis test is used to draw inference about a population based on a random sample. The inference made might of course be wrong. There are two types of errors;

::: {.fragment .fade-in}
**Type I error** is a false positive, a false alarm that occurs when $H_0$ is rejected when it is actually true. Examples: *"The test says that you are covid-19 positive, when you actually are not", "The test says that the drug has a positive effect on patient symptoms, but it actually has not"*.
:::

::: {.fragment .fade-in}
**Type II error** is a false negative, a miss that occurs when $H_0$ is accepted, when it is actually false. Examples: *"The test says that you are covid-19 negative, when you actually have covid-19", "The test says that the drug has no effect on patient symptoms, when it actually has"*.
:::

## Probability of type I and II errors

The probability of type I and II errors are denoted $\alpha$ and $\beta$, respectively.


$$\alpha = P(\textrm{type I error}) = P(\textrm{false alarm}) = P(\textrm{Reject }H_0|H_0 \textrm{ is true})$$
$$\beta = P(\textrm{type II error}) = P(\textrm{miss}) = P(\textrm{Accept }H_0|H_1 \textrm{ is true})$$

The significance level, $\alpha$, is the risk of false alarm.
<!-- , i.e. to say *"I have a hit"*, *"I found a difference"*, when the the null hypothesis (*"there is no difference"*) is true. -->

:::{.notes}
On white board:

```{r}
kable(matrix(c("", "H0 is true", "H0 is false", "Accept H0", "", "Type II error, miss", "Reject H0", "Type I error, false alarm", ""), byrow=F, ncol=3)) %>% kable_styling(full_width = FALSE, bootstrap_options = "bordered", position="center")
```
:::


## Probability of type I and II errors

```{r}
#| label: fig-alphabeta
#| fig-cap: "The probability density functions under H0 and H1, respectively. The probability  of type I error ($\\alpha$) and type II error ($\\beta$) are indicated."
#| fig-width: 5
x<-seq(-3,6,0.01)
d <- 2.5
q=qnorm(0.975)
df <- data.frame(x=x, h=rep(c("H0", "H1"), each=length(x))) |> mutate(m=c(H0=0, H1=d)[h], f=dnorm(x, m, 1))
ggplot(df, aes(x=x, y=f, color=h, fill=h)) + geom_line()  + geom_area(data=df %>% filter(x>q, h=="H0"), alpha=0.5)  + geom_area(data=df %>% filter(x<q, h=="H1"), alpha=0.5) + xlab("x") + ylab("f(x)") + theme_bw() + scale_color_manual("", guide="none", values=c("black", "orange")) + scale_fill_manual("", guide="none", values=c("black", "orange")) + annotate("label", x=d+1, y=dnorm(1,0,1), label="H1", color="orange") + annotate("label", x=-1, y=dnorm(1,0,1), label="H0", color="black") + annotate("label", x=2.2, y=0.02, label="alpha", parse=TRUE) + annotate("label", x=1.2, y=0.08, label="beta", parse=TRUE, color="orange") + geom_vline(xintercept=q)
```

:::{.notes}
The risk of false alarm is controlled by setting the significance level to a desired value. We do want to keep the risk of false alarm (type I error) low, but at the same time we don't want many missed hits (type II error).
:::


## Significance level

The **significance level**, $\alpha$, is the risk of false alarm.

$\alpha$ should be set before the hypothesis test is performed. 

Common values to use are $\alpha=0.05$ or 0.01.



* If the p-value is above the significance level, $p>\alpha$, $H_0$ is accepted.
* If the p-value is below the significance level, $p \leq \alpha$, $H_0$ is rejected.




## Statistical power

**Statistical power** is defined as

$$\textrm{power} = 1 - \beta = P(\textrm{Reject }H_0 | H_1\textrm{ is true}).$$



```{r}
x<-seq(-3,6,0.01)
d <- 2.5
q=qnorm(0.975)
df <- data.frame(x=x, h=rep(c("H0", "H1"), each=length(x))) |> mutate(m=c(H0=0, H1=d)[h], f=dnorm(x, m, 1))
ggplot(df, aes(x=x, y=f, color=h, fill=h)) + geom_line()  + geom_area(data=df %>% filter(x>q, h=="H1"), alpha=0.5) + xlab("x") + ylab("f(x)") + theme_bw() + scale_color_manual("", guide="none", values=c("black", "orange")) + scale_fill_manual("", guide="none", values=c("black", "orange")) + annotate("label", x=d+1, y=dnorm(1,0,1), label="H1", color="orange") + annotate("label", x=-1, y=dnorm(1,0,1), label="H0", color="black") + annotate("label", x=3, y=0.13, label="power", parse=TRUE, color="orange") + geom_vline(xintercept=q)
```

## Perform a hypothesis test

You suspect that a dice is loaded, i.e. showing 'six' more often than expected of a fair dice. To test this you throw the dice 10 times and count the total number of sixes. You got 5 sixes. Is there reason to belive that the dice is loaded?

1.  Define $H_0$ and $H_1$
2.  Select an appropriate significance level, $\alpha$
3.  Select appropriate test statistic, $T$, and $t_{obs}$
4.  Compute the sampling distribution of $T$ when $H_0$ is true
5.  Compare the observed value, $t_{obs}$, with the computed sampling distribution under $H_0$ and compute a p-value. The p-value is the probability of observing a value at least as extreme as the observed value, if $H_0$ is true.
6.  Based on the p-value either accept or reject $H_0$.

:::{.notes}
$$H0: \pi=1/6, H1: \pi>1/6$$
$\alpha=0.05$

N - number of sixes as test statistic, nobs=5

Simulate of use Binomial to get null distribution

p= P(N>=5)

Reject H0!

```{r dicec, echo=TRUE}
##Simulate thowing 10 dice and counting the number of sixes.
N <- replicate(100000, sum(sample(1:6, size=10, replace=TRUE)==6))
table(N)
##The probability mass function
table(N)/length(N)
hist(N, breaks=(0:11)-0.5)
##
mean(N>=5)
```
:::


## Hypothesis testing using resampling

The null distribution, the sampling distribution of a test statistic under the null hypothesis, is sometimes known or can be approximated. When the null distribution is unknown, another option is to estimate the null distribution using resampling.

* Simulate from a known $H_0$
* Bootstrap
* Permutation

## Bootstrap

**Bootstrap** is a resampling technique that resamples with replacement from the available data (random sample) to construct new simulated samples.

Bootstrapping can be used to simulate a sampling distribution and estimate properties such as standard error and interval estimates, but also to perform hypothesis testing.


## Bootstrap example

Men with a waist circumference greater than 94 cm have been shown to have an increased risk of cardiovascular disease. Based on the following waist circumferences of 12 diabetic men, is there reason to believe that the mean waist circumference of diabetic men is greater than 94 cm?

```{r bootdata, fig.height=.3, fig.width=7}
set.seed(11)
df=data.frame(i=1:12,x=c(97, 97, 89, 84, 124, 107, 99, 122, 102, 114, 109, 84), boot=0)
df |> ggplot(aes(x=i, y=1, label=sprintf("%i",x))) + geom_label() + theme_void() + theme(legend.position="none")
mobs <- mean(df$x)
```

:::{.fragment .fade.in}
#### Hypotheses

$$H_0: \mu=94$$
$$H_1: \mu>94$$
:::
:::{.fragment .fade.in}
#### Significance level {.unnumbered}

let $\alpha = 0.05$.
:::
:::{.fragment .fade.in}
##### Test statistic {.unnumbered}

use the sample mean, $m$, as test statistic.

$mobs = `r mean(df$x)`$
:::

## Bootstrap example

```{r, fig.height=.3, fig.width=7}
df |> ggplot(aes(x=i, y=1, label=sprintf("%i",x))) + geom_label() + theme_void() + theme(legend.position="none")
```

:::{.fragment .fade-in}

#### Null distribution

Assume $H_0$ is true and compute sampling distribution. This can be done by first modifying the observed values;

$xnull = x - mobs + 94$

```{r bootxnull, fig.height=.25, fig.width=7}
df0=df |> mutate(x=x-mean(x)+94)
dfboot <- lapply(1:5, function(k) df0 |> mutate(x=sample(x, replace=TRUE), boot=k)) |> bind_rows()

dfm <- dfboot |> group_by(boot) |> summarize(m=mean(x))
df0 |> ggplot(aes(x=i, y=1, label=sprintf("%.0f",x))) + geom_label() + theme_void() + theme(legend.position="none")
```
:::

:::::{.fragment .fade-in}
<!-- ::::{.columns} -->
<!-- :::{.column width="90%} -->
Bootstrap!

```{r fig.height=1.5, fig.width=7}
dfboot |> ggplot(aes(x=i, y=boot, label=sprintf("%.0f",x))) + geom_label() + theme_void() 
```
<!-- ::: -->

<!-- <!-- :::{.column width="20%} --> -->
<!-- ```{r} -->
<!-- gt(dfm)  -->
<!-- ``` -->

<!-- ::: -->
<!-- :::: -->
:::::

## Bootstrap example
::::{.columns}
:::{.column width="50%}
#### Null distribution

```{r}
mboot <- replicate(1000, sample(df0$x, replace=TRUE)|> mean())
data.frame(mboot=mboot)|>ggplot(aes(x=mboot)) + geom_histogram(color="white") + theme_bw() + geom_vline(xintercept=mobs)
```
:::
:::{.column width="50%}

#### p-value

Probability of $mobs$ or something more extreme; $p = `r mean(mboot>=mobs)`$

#### Accept or reject $H_0$?
:::
::::

## Permutation

A permutation test answers the question whether an observed effect could be due only to the random sampling, how samples are assigned to different groups for example.

Permutation tests are commonly used in clinical settings where a treatment group is compared to a control group.

:::::{.columns}
::::{.column width="60%"}

:::{.fragment .fade-in}
```{r permdata,, fig.height=.3, fig.width=5}
set.seed(11)
df=data.frame(i=1:12,x=c(rnorm(6,27,5), rnorm(6, 22,5)), gr=rep(c("A", "B"), each=6), perm=0)
dfperm <- lapply(1:5, function(k) df |> mutate(x=sample(x), perm=k)) |> bind_rows()
df <- bind_rows(df, dfperm)
dfm <- df |> group_by(perm) |> summarize(mA=mean(x[gr=="A"]), mB=mean(x[gr=="B"]))|> mutate(diff=mA-mB)
df |> filter(perm==0) |> ggplot(aes(x=i, y=1, label=sprintf("%.1f",x), color=gr)) + geom_label() + theme_void() + theme(legend.position="none")
```
:::

:::{.fragment .fade-in}
Permute!

```{r perm, fig.height=1.4, fig.width=5}
dfperm |> ggplot(aes(x=i, y=perm, label=sprintf("%.1f",x), color=gr)) + geom_label() + theme_void() + theme(legend.position="none")
```
:::
::::

::::{.column width="30%"}
:::{.fragment .fade-in}
```{r}
gt(dfm)
```
:::
::::
:::::


:::{.notes}
In a permutation test the null distribution is computed by permuting (rearranging) the observations so that they are no longer associated to the outcome (e.g. group identity).

The null distribution is a distribution of test statistics that could be explained by the random sampling. If the observed effect is much more extreme than what could be explained just by the random sampling, we can conclude that there is a difference between the groups (the treatment has an effect).
:::

## Permutation example {auto-animate="true" auto-animate-easing="ease-in-out"}

**Do high fat diet lead to increased body weight?**

. . .

Study setup:

1.  Order 24 female mice from a lab.

::: r-hstack
```{r results="asis"}
set.seed(111)
n <- 12
xHF <- c(25, 30, 23, 18, 31, 24, 39, 26, 36, 29, 23, 32)
xN <- c(27, 25, 22, 23, 25, 37, 24, 26, 21, 26, 30, 24)
name <- paste0("mus", 1:(2*n))
o <- sample(1:(2*n))
#x <- c(xHF-rnorm(n, 5, 2), xN-rnorm(n, 2, 2))
x <- rnorm(2*n, 25, 3)
sc <- 4
for (i in o) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}', round(x[i]*sc), name[i]))
}
```

:::

## Permutation example {auto-animate="true" auto-animate-easing="ease-in-out"}

**Do high fat diet lead to increased body weight in mice?**

Study setup:

1.  Order 24 female mice from a lab.

2.  Randomly assign 12 of the 24 mice to receive high-fat diet, the remaining 12 are controls (ordinary diet).

::: {.columns}
::: {.column width="50%"}
High-fat diet

::: {r-vstack}
::: r-hstack
```{r results="asis"}
for (i in 1:4) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}', round(x[i]*sc), name[i])) 
}
```
:::
::: r-hstack
```{r results="asis"}
for (i in 5:8) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}', round(x[i]*sc), name[i])) 
}
```
:::
::: r-hstack
```{r results="asis"}
for (i in 9:12) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}', round(x[i]*sc), name[i])) 
}
```
:::
:::
:::
::: {.column width="50%"}
Ordinary diet

::: {r-vstack}
::: r-hstack
```{r results="asis"}
for (i in 13:16) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}\n', round(x[i]*sc), name[i])) 
}
```
:::
::: r-hstack
```{r results="asis"}
for (i in 17:20) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}\n', round(x[i]*sc), name[i])) 
}
```
:::
::: r-hstack
```{r results="asis"}
for (i in 21:24) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}\n', round(x[i]*sc), name[i])) 
}
```
:::
:::
:::
::::

## Permutation example {auto-animate="true" auto-animate-easing="ease-in-out"}

**Do high fat diet lead to increased body weight in mice?**

Study setup:

1.  Order 24 female mice from a lab.

2.  Randomly assign 12 of the 24 mice to receive high-fat diet, the remaining 12 are controls (ordinary diet).

3.  Measure body weight after three weeks.

::: {.columns}
::: {.column width="50%"}
High-fat diet

::: {r-vstack}
::: r-hstack
```{r results="asis"}
for (i in 1:4) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}', round(xHF[i]*sc), name[i]))
}
```
:::
::: r-hstack
```{r results="asis"}
for (i in 5:8) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}', round(xHF[i]*sc), name[i]))
}
```
:::
::: r-hstack
```{r results="asis"}
for (i in 9:12) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}', round(xHF[i]*sc), name[i]))
}
```
:::
:::
:::

::: {.column width="50%"}
Ordinary diet

::: {r-vstack}
::: r-hstack
```{r results="asis"}
for (i in 1:4) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}\n', round(xN[i]*sc), name[12+i]))
}
```
:::
::: r-hstack
```{r results="asis"}
for (i in 5:8) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}\n', round(xN[i]*sc), name[12+i]))
}
```
:::
::: r-hstack
```{r results="asis"}
for (i in 9:12) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}\n', round(xN[i]*sc), name[12+i]))
}
```
:::
:::
:::
::::


## Permutation example

**Do high fat diet lead to increased body weight in mice?**

Study setup:

1.  Order 24 female mice from a lab.

2.  Randomly assign 12 of the 24 mice to receive high-fat diet, the remaining 12 are controls (ordinary diet).

3.  Measure body weight after three weeks.

The observed values, mouse weights in grams, are summarized below;

```{r miceobs, echo=FALSE}
## 12 HF mice
xHF <- c(25, 30, 23, 18, 31, 24, 39, 26, 36, 29, 23, 32)
## 12 control mice
xN <- c(27, 25, 22, 23, 25, 37, 24, 26, 21, 26, 30, 24)
```

```{r}
kable(rbind("high-fat"=xHF, "ordinary"=xN), digits=1) %>% kable_styling()
```

## Simulation example

**1. Null and alternative hypotheses**

$$
\begin{aligned}
H_0: \mu_2 = \mu_1 \iff \mu_2 - \mu_1 = 0\\
H_1: \mu_2>\mu_1 \iff \mu_2-\mu_1 > 0
\end{aligned}
$$

where $\mu_2$ is the (unknown) mean body weight of the high-fat mouse population and $\mu_1$ is the mean body-weight of the control mouse population.

Studied population: Female mice that can be ordered from a lab.

## Simulation example

**2. Select appropriate significance level** $\alpha$

$$\alpha = 0.05$$

## Simulation example

**3. Test statistic**

Of interest; the mean weight difference between high-fat and control mice 

$$D = \bar X_2 - \bar X_1$$

Mean weight of 12 (randomly selected) mice on ordinary diet, $\bar X_1$. $E[\bar X_1] = E[X_1] = \mu_1$

Mean weight of 12 (randomly selected) mice on high-fat diet, $\bar X_2$. $E[\bar X_2] = E[X_2] = \mu_2$

Observed values;

```{r, echo=FALSE}
## 12 HF mice
xHF <- c(25, 30, 23, 18, 31, 24, 39, 26, 36, 29, 23, 32)
## 12 control mice
xN <- c(27, 25, 22, 23, 25, 37, 24, 26, 21, 26, 30, 24)

##Compute mean body weights of the two samples
mHF <- mean(xHF)
mN <- mean(xN) 
## Compute mean difference
dobs <- mHF - mN
```

$\bar x_1 = `r sprintf("%.2f", mN)`$, mean weight of control mice (ordinary diet)

$\bar x_2 = `r sprintf("%.2f", mHF)`$, mean weight of mice on high-fat diet

$d_{obs} = \bar x_2 - \bar x_1 = `r dobs`$, difference in mean weights 

## Simulation example

**4. Null distribution**

If high-fat diet has no effect, i.e. if $H_0$ was true, the result would be as if all mice were given the same diet.

The 24 mice were initially from the same population, depending on how the mice are randomly assigned to high-fat and normal group, the mean weights would differ, even if the two groups were treated the same.

::: r-hstack
```{r results="asis"}
for (i in o) {
  cat(sprintf('![](../figures/Mus.jpg){width=%i data-id="%s"}', round(c(CHF, xN)[i]*sc), name[i]))
}
```

Random reassignment to two groups can be accomplished using permutation.

Assume $H_0$ is true, i.e. assume all mice are equivalent and

1.  Randomly reassign 12 of the 24 mice to 'high-fat' and the remaining 12 to 'control'.
2.  Compute difference in mean weights

If we repeat 1-2 many times we get the sampling distribution when $H_0$ is true, the so called null distribution, of difference in mean weights.

## Simulation example

**4. Null distribution**

```{r permtest, echo=FALSE, out.width="50%"}
## All 24 body weights in a vector
x <- c(xHF, xN)
## Mean difference
dobs <- mean(x[1:12]) - mean(x[13:24])

## Permute once
y <- sample(x)
##Compute mean difference
#mean(y[1:12]) - mean(y[13:24])
dnull.perm <- replicate(n = 100000, {
  y <- sample(x)
  ##Mean difference
  mean(y[1:12]) - mean(y[13:24])
})
ggplot(data.frame(d=dnull.perm), aes(x=d)) + geom_histogram(bins=25, color="white") + theme_bw() + geom_vline(xintercept=dobs, color="red")
##Alternatively plot using hist
## hist(dnull.perm)
```

## Simulation example

**5. Compute p-value**

What is the probability to get an at least as extreme mean difference as our observed value, $d_{obs}$, if $H_0$ was true?

```{r micepval, echo=FALSE, eval=FALSE}
## Compute the p-value
sum(dnull.perm>dobs)/length(dnull.perm)
mean(dnull.perm>dobs)
```

$P(\bar X_2 - \bar X_2 \geq d_{obs} | H_0) =$ `r sprintf("%.3g",mean(dnull.perm>=dobs))`

## Simulation example

**6. Conclusion?**


