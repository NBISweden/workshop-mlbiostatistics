[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Preface\nThis repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://uppsala.instructure.com/courses/74597"
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Statistical Inference",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nto define null and alternative hypothesis\nto perform a hypothesis test using resampling\nto perform a t-test\nto understand and define sampling distribution and standard error\nto compute standard error of mean and proportions\nto compute confidence interval of mean and proportions using the normal approximation\nto compute confidence interval of mean using the t-distribution"
  },
  {
    "objectID": "infe_01hypothesis.html#the-null-and-alternative-hypothesis",
    "href": "infe_01hypothesis.html#the-null-and-alternative-hypothesis",
    "title": "1  Introduction to hypothesis tests",
    "section": "1.1 The null and alternative hypothesis",
    "text": "1.1 The null and alternative hypothesis\nThere are two hypotheses involved in a hypothesis test, the null hypothesis, \\(H_0\\), and the alternative hypothesis, \\(H_1\\).\nThe null hypothesis is in general neutral; “no change”, “no difference between the groups”, “no association”. In general we want to show that \\(H_0\\) is false.\nThe alternative hypothesis expresses what the researcher is interested in, such as “the treatment has an effect”, “there is a difference between the groups”, “there is an association”. The alternative hypothesis can also be directional, e.g. “the treatment has a positive effect”."
  },
  {
    "objectID": "infe_01hypothesis.html#to-perform-a-hypothesis-test",
    "href": "infe_01hypothesis.html#to-perform-a-hypothesis-test",
    "title": "1  Introduction to hypothesis tests",
    "section": "1.2 To perform a hypothesis test",
    "text": "1.2 To perform a hypothesis test\n\nDefine \\(H_0\\) and \\(H_1\\)\nSelect an appropriate significance level, \\(\\alpha\\)\nSelect an appropriate test statistic, \\(T\\), and compute the observed value, \\(t_{obs}\\)\nAssume that the \\(H_0\\) is true and compute the sampling distribution of \\(T\\).\nCompare the observed value, \\(t_{obs}\\), with the computed sampling distribution under \\(H_0\\) (the so called null distribution) and compute a p-value.\nBased on the p-value either accept or reject \\(H_0\\).\n\nThe sampling distribution is the distribution of a sample statistic (e.g mean or proportion). The sampling distribution can be obtained by drawing a large number of samples from a population.\nA null distribution is a sampling distribution when the null hypothesis is true.\n\n\n\n\n\nFigure 1.1: A null distribution"
  },
  {
    "objectID": "infe_01hypothesis.html#the-p-value",
    "href": "infe_01hypothesis.html#the-p-value",
    "title": "1  Introduction to hypothesis tests",
    "section": "1.3 The p-value",
    "text": "1.3 The p-value\nThe p-value is the probability of observing a value at least as extreme as the observed value, if \\(H_0\\) is true.\n\n\n\n\n\n\n\n(a) One-tailed\n\n\n\n\n\n\n\n(b) Two-tailed\n\n\n\n\nFigure 1.2: The p-value is the probability to observe \\(x_{obs}\\) or something more extreme, if the null hypothesis is true. The p-value is illustrated for a one-tailed test (left) and for a two-tailed test (right)."
  },
  {
    "objectID": "infe_01hypothesis.html#sec-errortype",
    "href": "infe_01hypothesis.html#sec-errortype",
    "title": "1  Introduction to hypothesis tests",
    "section": "1.4 Significance level and error types",
    "text": "1.4 Significance level and error types\nA hypothesis test is used to draw inference about a population based on a random sample. The inference made might of course be wrong. There are two types of errors;\nType I error is a false positive, a false alarm that occurs when \\(H_0\\) is rejected when it is actually true. Examples: “The test says that you are covid-19 positive, when you actually are not”, “The test says that the drug has a positive effect on patient symptoms, but it actually has not”.\nType II error is a false negative, a miss that occurs when \\(H_0\\) is accepted, when it is actually false. Examples: “The test says that you are covid-19 negative, when you actually have covid-19”, “The test says that the drug has no effect on patient symptoms, when it actually has”.\n\n\n\n\nTable 1.1:  Error types. \n\n  \n     \n    Accept H0 \n    Reject H0 \n  \n  \n    H0 is true \n     \n    Type I error, false alarm \n  \n  \n    H0 is false \n    Type II error, miss \n     \n  \n\n\n\n\n\n\nThe probability of type I and II errors are denoted \\(\\alpha\\) and \\(\\beta\\), respectively.\n\\[\\alpha = P(\\textrm{type I error}) = P(\\textrm{false alarm}) = P(\\textrm{Reject }H_0|H_0 \\textrm{ is true})\\] \\[\\beta = P(\\textrm{type II error}) = P(\\textrm{miss}) = P(\\textrm{Accept }H_0|H_1 \\textrm{ is true})\\]\nThe significance level, \\(\\alpha\\), is the risk of false alarm, i.e. to say “I have a hit”, “I found a difference”, when the the null hypothesis (“there is no difference”) is true.\n\n\n\n\n\nFigure 1.3: The probability density functions under H0 and H1, respectively. The probability of type I error (\\(\\alpha\\)) and type II error (\\(\\beta\\)) are indicated.\n\n\n\n\nThe risk of false alarm is controlled by setting the significance level to a desired value. We do want to keep the risk of false alarm (type I error) low, but at the same time we don’t want many missed hits (type II error).\nThe significance level should be set before the hypothesis test is performed. Common values to use are \\(\\alpha=0.05\\) or 0.01.\nIf the p-value is above the significance level, \\(p>\\alpha\\), \\(H_0\\) is accepted.\nIf the p-value is below the significance level, \\(p \\leq \\alpha\\), \\(H_0\\) is rejected.\nAnother property of a statistical test is the statistical power, defined as\n\\[\\textrm{power} = 1 - \\beta = P(\\textrm{Reject }H_0 | H_1\\textrm{ is true}).\\]"
  },
  {
    "objectID": "infe_02hypresamp.html#resampling-from-a-known-population-under-h_0",
    "href": "infe_02hypresamp.html#resampling-from-a-known-population-under-h_0",
    "title": "2  Hypothesis testing using resampling",
    "section": "2.1 Resampling from a known population under \\(H_0\\)",
    "text": "2.1 Resampling from a known population under \\(H_0\\)\nIf it is possible to simulate sampling under the null hypothesis, resampling can be used to estimate the null distribution, just like we did in the previous session about estimating probabilties and probability distributions using resampling.\nThis is done by setting up a model under the null hypothesis (e.g. using an urn model) and drawing random samples from this model repeatedly.\n\nExample 2.1 (Proportions, pollen allergy) Let’s assume we know that the proportion of pollen allergy in Sweden is \\(0.3\\). We suspect that the number of pollen allergic has increased in Uppsala in the last couple of years and want to investigate this.\nObserve 100 people from Uppsala, 42 of these were allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala \\(\\pi > 0.3\\)?\n\nNull and alternative hypotheses\n\\(H_0:\\) The proportion of pollen allergy in Uppsala is the same as in Sweden as a whole.\n\\(H_1:\\) The proportion of pollen allergy in Uppsala is greater than in Sweden as a whole.\nor expressed differently;\n\\[H_0:\\, \\pi=\\pi_0\\]\n\\[H_1:\\, \\pi>\\pi_0\\] where \\(\\pi\\) is the unknown proportion of pollen allergy in the Uppsala population and \\(\\pi_0 = 0.3\\) is the proportion of pollen allergy in Sweden.\n\n\nSignificance level\nHere we let \\(\\alpha = 0.05\\).\n\n\nTest statistic\nHere we are interested in the proportion of pollen allergic in Uppsala. An appropriate test statistic could be the number of pollen allergic in a sample of size \\(n=100\\), \\(X\\). As an alternative we can use the proportion of pollen allergic in a sample of size \\(n\\),\n\\[P = \\frac{X}{n}\\]\nLet’s use \\(P\\) as our test statistic and compute the observed value, \\(p_{obs}\\). In our sample of 100 people from Uppsala, the proportion allergic to pollen is \\(p_{obs}=42/100=0.42\\).\n\n\nNull distribution\nThe sampling distribution of \\(P\\) under \\(H_0\\) (i.e. when the null hypothesis is true) is what we call the null distribution.\n\\(H_0\\) state that \\(\\pi=0.3\\). We can model this using an urn model as follows;\n\n\n\n\n\nFigure 2.1: An urn model of the null hypothesis \\(\\pi=0.3\\). The black balls represent allergic and the white balls non-allergic.\n\n\n\n\nUsing this model, we can simulate taking a sample of size 100 many times.\n\n## Urn\nurn <- rep(c(0, 1), c(7, 3))\n## Sample 100 times with replacement\nx <- sample(urn, 100, replace=TRUE)\n## Compute proportion of samples that are allergic (1)\nmean(x)\n\n[1] 0.24\n\n## Set the seed to get the same result if we redo the analysis\nset.seed(13)\n## Repeat drawing sample of size 100 and computing proporion allergic 100000 times\np <- replicate(100000, mean(sample(rep(c(0, 1), c(7, 3)), 100, replace=TRUE)))\n\nPlot the distribution\n\n\n\n\n\nFigure 2.2: The sampling distribution.\n\n\n\n\n\n\nCompute p-value\nCompare the observed value, \\(p_{obs} = 0.42\\) to the null distribution.\n\n\n\n\n\nFigure 2.3: The sampling distribution. The observed value is marked by a red vertical line.\n\n\n\n\nThe p-value is the probability of getting the observed value or higher, if the null hypothesis is true.\nUse the null distribution to calculate the p-value, \\(P(P \\geq 0.42|H_0)\\).\n\n## How many times \nsum(p >= 0.42)\n\n[1] 703\n\n## p-value\npval <- mean(p >= 0.42)\n\np = \\(P(P \\geq 0.42|H_0)\\) = 0.00703\n\n\nAccept or reject \\(H_0\\)?\nAs the p-value is \\(< \\alpha = 0.05\\) the null hypotheis is rejected. This means that we can conclude that there is reason to belive that the porportion of pollen allergic in Uppsala is greater than 30%."
  },
  {
    "objectID": "infe_02hypresamp.html#bootstrap-resampling",
    "href": "infe_02hypresamp.html#bootstrap-resampling",
    "title": "2  Hypothesis testing using resampling",
    "section": "2.2 Bootstrap resampling",
    "text": "2.2 Bootstrap resampling\nBootstrap is a resampling technique that resamples with replacement from the available data (random sample) to construct new simulated samples. These bootstrapped samples can be used to estimate sampling distributions that can be used to estimate properties such as standard error and interval estimates, but also to perform hypothesis testing.\nWe will get back to constructing bootstrap interval estimates in Section 5.3.1. Here we will focus on hypothesis testing using bootstrap.\nBootstrapping can be used to construct the null distribution in hypothesis testing. To do so, we first need to modify the observed sample according to the null hyothesis, before resampling with replacement.\nFor a one sample test of means the hypotheses can be;\n\\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu>\\mu_0\\] Let our observed random sample of size \\(n\\) be \\(x_1, x_2, \\dots, x_n\\), with sample mean \\(m_{obs} = \\bar x\\). For simplicity we can use sample mean as out test statistic.\nTo contruct a sampling distribution when \\(H_0\\) is true we would need a modified sample with mean \\(\\mu_0\\), which can be constructed by computing \\(x_i'=x_i-m_{obs} + \\mu_0\\). A null distribution can then be estimated using bootstrapping as follows;\n\nTake a random sample of size \\(n\\) with replacement from \\(x_1', x_2', \\dots, x_n'\\). Note that the same value might be selected more than once as we sample with replacement. Denote our bootstrap resample \\(x^*\\).\nCompute the mean value of the bootstrap resample \\(x^*\\) and denote the mean \\(m^*\\).\n\nRepeat 1 and 2 many times to get a distribution of mean values \\(m^*\\), a bootstrap estimate of the null distribution of sample means. Many times can e.g. be \\(B=1000\\).\nThe p-value can be estimated as the number of times \\(m^* \\geq m_{obs}\\) divided by \\(B\\)\n\\[p = \\frac{\\sum_{k=1}^B I(m^*_k \\geq m_{obs})}{B}.\\]\n\nExample 2.2 (Mean value, bootstrap) Men with a waist circumference greater than 94 cm have been shown to have an increased risk of cardiovascular disease. Based on the following waist circumferences of 12 diabetic men, is there reason to believe that the mean waist circumference of diabetic men is greater than 94 cm?\n\n\n\n97, 97, 89, 84, 124, 107, 99, 122, 102, 114, 109, 84\n\nx <- c(97, 97, 89, 84, 124, 107, 99, 122, 102, 114, 109, 84)\n\n\nNull and alternartive hypotheses\n\\[H_0: \\mu=94\\] \\[H_1: \\mu>94\\]\n\n\nSignificance level\nHere we let \\(\\alpha = 0.05\\).\n\nTest statistic\nHere we will use the sample mean, \\(m\\), as test statistic.\n\\(m_{obs} = 102.3\\)\n\nmobs <- mean(x)\n\n\n\n\nNull distribution\nAssume that the \\(H_0\\) is true and compute the sampling distribution of sample means \\(m\\). We will estimate the null distributiuon using bootstrap resampling.\nFirst modify the sample we have to have mean 94 cm (according to the null distribution), we do this by subtracting \\(m_{obs}\\) and adding 94 to the observations.\n\nxnull <- x - mobs + 94\n\nResample with replacement to get a bootstrap sample of size 12 (same as the original sample) and compute the mean.\n\nmean(sample(xnull, replace=TRUE))\n\n[1] 91.33\n\n\nRepeat 1000 times.\n\nmnull <- replicate(1000, mean(sample(xnull, replace=TRUE)))\n\nPlot the null distribution in a histogram and compare with the observed value \\(m_{obs}\\).\n\nggplot(data.frame(m=mnull), aes(x=m)) +\n  geom_histogram(bins=25, color=\"white\") +\n  theme_bw() +\n  geom_vline(xintercept=mobs, color=\"red\")\n\n\n\n\nFigure 2.4: Null distribution of mean waist circumferencs in cm. Observed mean is marked with a red vertical line.\n\n\n\n\n\n\nCompute p-value\nThe p-value is the probability of seeing the observed value or something more extreme.\n\np <- mean(mnull>=mobs)\n\nAs the computed p-value \\(<\\alpha=0.05\\) the null hypothesis is rejected and we conclude that there is reason to belive that diabetic men have a mean waist circumference greater than 94 cm.\n\n\n\n2.2.1 Permutation\nAnother resampling techinque is permutation. A permutation test answers the question whether an observed effect could be due only to the random sampling. Permutations are commonly used in clinical settings where a treatment group is compared to a control group.\nA permutation test involves comparing two (or more) groups, usually by computing the difference of a property of the two groups, e.g. a mean difference, where the null hypotheis is that there is no difference between the groups (with regards to the studied property).\nIn a permutation test the null distribution is computed by permuting (rearranging) the observations so that they are randomly assigned to the two groups and computing the test statistic (e.g. mean difference). This can be achieved by keeping the order of the group labels and resampling (without replacement) the observations (o the other way arround).\nThe null distribution is a distribution of test statistics that could be explained by the random sampling. If the observed effect is much more extreme than what could be explained just by the random sampling, we can conclude that there is a difference between the groups (the treatment has an effect).\n\nExample 2.3 (Do high fat diet lead to increased body weight?) The effect of high-fat diet on the body weight of mice is studied in an experiment. The study setup is as follows:\n\n24 female mice are ordered from a lab.\nRandomly, 12 of the 24 mice are assigned to receive high-fat diet, the remaining 12 are controls (ordinary diet).\nBody weight is measured after one week.\n\n\n\n\nThe observed values, mouse weights in grams, are summarized below;\n\n\n\n\n\n\n\n\n  \n    high-fat \n    25 \n    30 \n    23 \n    18 \n    31 \n    24 \n    39 \n    26 \n    36 \n    29 \n    23 \n    32 \n  \n  \n    ordinary \n    27 \n    25 \n    22 \n    23 \n    25 \n    37 \n    24 \n    26 \n    21 \n    26 \n    30 \n    24 \n  \n\n\n\n\n\n\nNull and alternative hypotheses\n\\[\n\\begin{aligned}\nH_0: \\mu_d = \\mu_c \\iff \\mu_d - \\mu_c = 0\\\\\nH_1: \\mu_d>\\mu_c \\iff \\mu_d-\\mu_c > 0\n\\end{aligned}\n\\]\nwhere \\(\\mu_d\\) is the (unknown) mean body weight of the high-fat mouse population and \\(\\mu_c\\) is the mean body-weight of the control mouse population.\nStudied population: Female mice that can be ordered from a lab.\n\n\nTest statistic\nHere we are interested in the mean difference between high-fat and control mice and an appropriate test statistic can be the mean diffrence, \\(D = \\bar X_d - \\bar X_c\\), where\n\n\\(\\bar X_d\\) is a random variable describing the mean weight of 12 (randomly selected) mice on high-fat diet. \\(E[\\bar X_d] = E[X_d] = \\mu_d\\)\n\\(\\bar X_c\\) is a random variable describing the Mean weight of 12 (randomly selected) mice on ordinary diet. \\(E[\\bar X_c] = E[X_c] = \\mu_c\\)\n\nThe mean difference \\(D = \\bar X_d - \\bar X_c\\) is also a random variable.\nObserved values;\n\n## 12 HF mice\nxD <- c(25, 30, 23, 18, 31, 24, 39, 26, 36, 29, 23, 32)\n## 12 control mice\nxC <- c(27, 25, 22, 23, 25, 37, 24, 26, 21, 26, 30, 24)\n\n##Compute mean body weights of the two samples\nmD <- mean(xD)\nmC <- mean(xC) \n## Compute mean difference\ndobs <- mD - mC\n\nMean weight of sample control mice (ordinary diet): \\(\\bar x_c = 25.83\\)\nMean weight of sample mice on high-fat diet: \\(\\bar x_d = 28.00\\)\nDifference in sample mean weights: \\(d_{obs} = \\bar x_d - \\bar x_c = 2.1667\\)\n\n\nNull distribution\nIf high-fat diet has no effect, i.e. if \\(H_0\\) was true, the result would be as if all mice were given the same diet. What can we expect if all mice are fed with the same type of food?\nThis can be accomplished using permutation\nThe 24 mice were initially from the same population, depending on how the mice are randomly assigned to high-fat and normal group, the mean weights would differ, even if the two groups were treated the same.\nAssume \\(H_0\\) is true, i.e. assume all mice are equivalent and\n\nRandomly reassign 12 of the 24 mice to ‘high-fat’ and the remaining 12 to ‘control’.\nCompute difference in mean weights\n\nIf we repeat 1-2 many times we get the null distribution of difference in mean weights.\n\n## All 24 body weights in a vector\nx <- c(xHF, xN)\n## Mean difference\ndobs <- mean(x[1:12]) - mean(x[13:24])\n## Permute once\ny <- sample(x)\n##Compute mean difference\nmean(y[1:12]) - mean(y[13:24])\n\n[1] -3.5\n\n##Repeat the above many times\ndnull.perm <- replicate(n = 100000, {\ny <- sample(x)\n##Mean difference\nmean(y[1:12]) - mean(y[13:24])\n})\n\nPlot the null distribution and the observed difference.\n\n\n\n\n\nFigure 2.5: Null distribution of the mean difference \\(D\\).\n\n\n\n\n\n\nCompute p-value\nWhat is the probability to get an at least as extreme mean difference as our observed value, \\(d_{obs}\\), if \\(H_0\\) was true?\n\n## Compute the p-value\npval <- mean(dnull.perm>=dobs)\n\n\\[P(\\bar X_2 - \\bar X_1 \\geq d_{obs} | H_0) = 0.169\\]\n\n\nAccept or reject \\(H_0\\)?\nAs \\(0.169>0.05\\) the null hypothesis is accepted, there is no evidence that the high-fat diet increase body weight in mice."
  },
  {
    "objectID": "infe_exr1_hyporesampling_solutions.html",
    "href": "infe_exr1_hyporesampling_solutions.html",
    "title": "Exercises: Hypothesis tests, resampling",
    "section": "",
    "text": "Exercise 1 (Pollen) You believe that the proportion of Swedish students allergic to pollen is greater than 0.3 (the proportion allergic to pollen in Sweden). To test this you observe 20 people in a student group at BMC in Uppsala, 9 or them are allergic to pollen.\nIs this reason to believe that the proportion of Swedish students allergic to pollen i greater than 0.3? Perform a hypothesis test to answer the question.\nCan you identify any problems with this study setup?\n\n​HintSolution\n\n\n\n\n\nxx\n\n\n\\(H_0: \\pi=0.3\\) \\(H_1: \\pi>0.3\\)\nSet the significance level to \\(\\alpha=0.05\\).\nTest statistic, \\(X\\), the number of allergic people in a sample of size 20.\n\\(x_{obs} = 9\\)\nSimulate null distribution\n\n\n\n\n\nCompute p-value, i.e. if null is true what is the probability to observe \\(x_{obs}\\) or higher?\n\nxobs <- 9\n(x <- mean(xnull>=xobs))\n\n[1] 0.1158\n\n\nAs \\(p>\\alpha\\) we will accept the null hypothesis, i.e. there is no reason to belive that the students are more allergic than the general Swedish population.\nProblems with the study: Discuss in your group! Is it reasonable to select 20 students at BMC to answer a question about all students in Sweden?\n\n\n\n\n\nExercise 2 (Diet) A diet study aims to study how the hemoglobin (Hb) levels in blood are affected by an iron-rich diet consisting of tofu, soybeans, broccoli, lentils and peas. To perform the study the dietician has recruited 40 male participants, who are randomly assigned to the iron-rich diet or control group (no change in participants diet), 20 participant in each group.\nThe observed Hb levels (in g/L);\n\nctrl <- c(197, 186, 157, 170, 193, 188, 175, 186, 177, 191, 168, 193, 191, 189, 188, 192, 179, 186, 197, 203)\niron <- c(187, 218, 196, 210, 206, 178, 181, 193, 172, 202, 169, 221, 183, 222, 185, 174, 192, 192, 162, 211)\n\nPerform a hypothesis test to investigate if the Hb level is affected (increased or decreased) by the iron-rich diet.\n\n​Solution\n\n\n\n\n\nDefine \\(H_0\\) and \\(H_1\\)\n\\(H_0: \\mu_{diet} = \\mu_{ctrl}\\) No difference in mean iron level between control group and iron rich group\n\\(H_1: \\mu_{diet} \\neq \\mu_{ctrl}\\)\nWill use the significance level, \\(\\alpha=0.05\\)\nSelect test statistic \\(D = \\bar X_d - \\bar X_c\\), where \\(\\bar X_c\\) is the mean Hb level in a control group of 20 people and \\(\\bar X_d\\) is the mean Hb level in a diet group of 20 people.\nThe observed value; \\(d_{obs}\\)\n\nmdiet <- mean(iron)\nmctrl <- mean(ctrl)\n(dobs <- mdiet - mctrl)\n\n[1] 7.4\n\n\nCompute null distribution using permutation.\n\n## Under null all observations are equivalent\nallobs <- c(iron, ctrl)\ndnull <- replicate(10000, {\n  ##Permute the 40 observations and assign the 20 first to the iron group\n  x <- sample(allobs)\n  d <- mean(x[1:20]) - mean(x[21:40])\n})\nhist(dnull)\n\n\n\n\nCompute p-value;\n\n(p <- mean(abs(dnull) >= abs(dobs)))\n\n[1] 0.1258\n\n\nAs \\(p>\\alpha\\), the null hypothesis is accepted, i.e. there is no reason to believe that the iron-rich diet affects the blood Hb level."
  },
  {
    "objectID": "infe_03hypparm.html#one-sample-mean",
    "href": "infe_03hypparm.html#one-sample-mean",
    "title": "3  Parametric tests",
    "section": "3.1 One sample, mean",
    "text": "3.1 One sample, mean\nA one sample test of means compares the mean of one sample to a prespecified value.\nFor example, we might know that the weight of a mouse on normal diet is normally distributed with mean 24.0 g and standard deviation 3 g and want to compare the weight of a sample of 10 mice on high-fat diet to the known mean value for mice on normal diet.\nThe hypotheses:\n\\[H_0: \\mu = \\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\]\nThe alternative hypothesis, \\(H_1,\\) above is for the two-sided hypothesis test. Other options are the one-sided \\(H_1\\); \\(H_1: \\mu > \\mu_0\\) or \\(H_1: \\mu < \\mu_0\\).\nIf \\[X \\sim N(\\mu, \\sigma)\\] (this could for example be the weight of a mouse on high-fat diet) then the sample mean \\[\\bar X \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right).\\]\nIf \\(\\sigma\\) is known under the null hypothesis, then the test statistic\n\\[Z = \\frac{\\bar X - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] is normally distributed, \\(\\sim N(0,1)\\).\nIn many situations the the population standard deviation is not known, but can instead be estimated from the sample. For large sample size \\(n\\), \\(\\sigma\\) can be replaced by the sample standard deviation \\(s\\) and the test statistic will still be normally distributed according to the central limiit theorem*.\nFor small \\(n\\) and unknown \\(\\sigma\\), we can use the one-sample t-test, that uses the test statistic\n\\[T = \\frac{\\bar X - \\mu_0}{\\frac{s}{\\sqrt{n}}},\\]\nwhich is t-distributed with \\(df=n-1\\) degrees of freedom.\nOnce we have an appropriate test statistic, \\(T\\) with known distribution, we can compute the observed value, \\(t_{obs}\\) and use the null distribution to compute the p-value, \\(P(|T| \\geq |t_{obs}|)\\).\nIn R, the functions pnorm and pt are useful for computing these probabilities. The one-sample t-test can also be performed using the function t.testlike this;\n\n##The observed 10 mouse weights;\nx <- c(25, 30, 23, 18, 31, 24, 39, 26, 36, 29, 23, 32)\n## Under the null hypothesis mu=24\n##Perform the t-test to investigate if the null hypotheis can be accepted, i.e.\n##if the sample comes from a normal distribution with expected value 24.\nt.test(x, mu=24)\n\n\n    One Sample t-test\n\ndata:  x\nt = 2.3153, df = 11, p-value = 0.04092\nalternative hypothesis: true mean is not equal to 24\n95 percent confidence interval:\n 24.19742 31.80258\nsample estimates:\nmean of x \n       28"
  },
  {
    "objectID": "infe_03hypparm.html#two-samples-mean",
    "href": "infe_03hypparm.html#two-samples-mean",
    "title": "3  Parametric tests",
    "section": "3.2 Two samples, mean",
    "text": "3.2 Two samples, mean\nA two sample test of means is used to determine if two population means are equal.\nTwo independent samples are collected (one from each population) and the means are compared. This test can for example be used to determine if a treatment group is different compared to a control group, in terms of the mean of a property of interest.\nThe null hypothesis;\n\\[H_0: \\mu_2 = \\mu_1\\] The alternative hypothesis can either be two sided\n\\[H_1: \\mu_2 \\neq \\mu_1\\] or one sided\n\\[H_1: \\mu_2 > \\mu_1 \\textrm{ or } H_1: \\mu_2 < \\mu_1.\\]\nAssume observations from both populations are normally distributed;\n\\[\n\\begin{aligned}\nX_1 \\sim N(\\mu_1, \\sigma_1) \\\\\nX_2 \\sim N(\\mu_2, \\sigma_2)\n\\end{aligned}\n\\] Then it follows that the sample means will also be normally distributed;\n\\[\n\\begin{aligned}\n\\bar X_1 \\sim N(\\mu_1, \\sigma_1/\\sqrt{n_1}) \\\\\n\\bar X_2 \\sim N(\\mu_2, \\sigma_2/\\sqrt{n_2})\n\\end{aligned}\n\\]\nThe mean difference \\(D = \\bar X_2 - \\bar X_1\\) is thus also normally distributed:\n\\[D = \\bar X_2 - \\bar X_1 = N\\left(\\mu_2-\\mu_1, \\sqrt{\\frac{\\sigma_2^2}{n_2} + \\frac{\\sigma_1^2}{n_1}}\\right)\\]\nIf \\(H_0\\) is true: \\[D = \\bar X_2 - \\bar X_1 = N\\left(0, \\sqrt{\\frac{\\sigma_2^2}{n_2} + \\frac{\\sigma_1^2}{n_1}}\\right)\\]\nThe test statistic: \\[Z = \\frac{\\bar X_2 - \\bar X_1}{\\sqrt{\\frac{\\sigma_2^2}{n_2} + \\frac{\\sigma_1^2}{n_1}}}\\] is standard normal, i.e. \\(Z \\sim N(0,1)\\).\nHowever, note that the test statistic require the standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\) to be known.\nWhat if the population standard deviations are not known?\nIf the sample sizes are large, we can replace the known standard deviations with our sample standard deviations and according to the central limit theorem assume that\n\\[Z = \\frac{\\bar X_2 - \\bar X_1}{\\sqrt{\\frac{s_2^2}{n_2} + \\frac{s_1^2}{n_1}}} \\sim N(0,1)\\]\nand proceed as before.\n\nFor small sample sizes the test statistic will be t-distributed.\n\\[t = \\frac{\\bar X_2 - \\bar X_1}{\\sqrt{\\frac{s_2^2}{n_2} + \\frac{s_1^2}{n_1}}}\\]\nIf it can be assumed that \\(X_1\\) and \\(X_2\\) both are normally distributed and have equal variances, Studen’t t-test can be used. For equal variances the pooled sample variance can be computed;\n\\[s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}\\]\nand the test statistic\n\\[t = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s_p^2(\\frac{1}{n_1} + \\frac{1}{n_2})}}\\]\nis t-distributed with \\(n_1+n_2-2\\) degrees of freedom.\nFor unequal variances Welch’s t-test can instead be used.\nThe t-test is implemented in R, e.g. in the function t.test in the R-package stats, both Student’s t-test for equal variances and Welch’s t-test for unequal variances."
  },
  {
    "objectID": "infe_03hypparm.html#one-sample-proportions",
    "href": "infe_03hypparm.html#one-sample-proportions",
    "title": "3  Parametric tests",
    "section": "3.3 One sample, proportions",
    "text": "3.3 One sample, proportions\n\nExample 3.1 (Pollen allergy) Let’s get back to the pollen example!\nAssume that the proportion of pollen allergy in Sweden is known to be \\(0.3\\). Observe 100 people from Uppsala, 42 of these were allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala \\(\\pi > 0.3\\)?\n\n\nNull and alternative hypotheses\n\\[H_0:\\, \\pi=\\pi_0\\]\n\\[H_1:\\, \\pi>\\pi_0,\\]\nwhere \\(\\pi_0=0.30\\) in this example. Other potential alternative hypothesis are \\(H_1: \\pi<\\pi_0\\) or \\(H_1:\\pi \\neq \\pi_0,\\) but in this particular example we are only interested in the alternative that \\(\\pi > \\pi_0\\).\nSignificance level\nSet the signifgicance level to \\(\\alpha=0.05\\).\nTest statistic\nHere, we will use \\(X\\), the number of allergic persons in a random sample of size \\(n=100\\). The observed value is \\(x_{obs} = 42\\).\nNull distribution\n\\(X\\) is binomially distributed, so there is no need to use resampling here, we can use the binomial distribution to answer the question.\np-value\nThe probability of \\(x_{obs}\\) or something higher,\n\\(P(X \\geq 42) = 1 - P(X \\leq 41)\\) = [1-pbinom(41,100,0.3)] = 0.007174\nAccept or reject \\(H_0\\)?\nAs \\(p<0.05\\) \\(H_0\\) is rejected and we conclude that there is reason to believe that the proportion of pollen allergic in Uppsala is higher than 0.3.\n\nThis p-value can also be computed using the exact binomial test;\n\nbinom.test(42, 100, 0.3, alternative=\"greater\")\n\n\n    Exact binomial test\n\ndata:  42 and 100\nnumber of successes = 42, number of trials = 100, p-value = 0.007174\nalternative hypothesis: true probability of success is greater than 0.3\n95 percent confidence interval:\n 0.3364797 1.0000000\nsample estimates:\nprobability of success \n                  0.42 \n\n\nAn alternative approach is to use the central limit theorem and use the normal approximation.\n\n\n\nAs a result of the central limit theorem, the distribution of number or proportion of allergic individuals in a sample of size \\(n\\) is approximately normal. At least if the sample is large enough. A rule of thumb is that the sample size should be \\(n>30\\).\nHere, the sample size is 100!\nThis test of proportions using the normal approximation is implemented in in the r-function prop.test.\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  42 out of 100, null probability 0.3\nX-squared = 6.2976, df = 1, p-value = 0.006045\nalternative hypothesis: true p is greater than 0.3\n95 percent confidence interval:\n 0.3372368 1.0000000\nsample estimates:\n   p \n0.42 \n\n\nBut can of course also be calculated using only the normal distribution table.\nThe normal distribution has two parameters, mean and standard deviation.\nFrom the binomial distribution we know that \\(E[X] = n\\pi\\) and \\(var(X) = n\\pi(1-\\pi)\\).\nThe standard error of \\(X\\) is\n\\[SE=\\sqrt{n\\pi(1-\\pi)}\\]\nIf \\(H_0\\) is true \\(\\pi=\\pi_0\\) and\n\\[X \\sim N\\left(n\\pi_0, \\sqrt{n\\pi_0(1-\\pi_0)}\\right)\\]   \nWith this null distribution and ur observed value \\(x_{obs} = 42\\), the p-value can be computed.\n\\(p = P(X \\geq 42)\\)\nAs we are now approximating a discrete distribution using a continuous distribution we need to use a trick called continuiuty correction, which simply means that we let each integer be represented by y a region \\(\\pm 0.5\\) from its value. Hence,\n\\[p = P(X \\geq 42) = [continuity \\; correction] = \\] \\[P(X \\geq 41.5) = P\\left(Z \\geq \\frac{41.5 - n\\pi_0}{\\sqrt{n\\pi_0(1-\\pi_0)}}\\right) = \\] \\[P\\left(Z \\geq \\frac{41.5-30}{\\sqrt{100*0.3*(1-0.3)}}\\right) = P(Z \\geq 2.51) = 1 - P(Z \\leq 2.51) = [Z-table] = 0.0060\\]\n\n\n\nAs 0.00605<0.05 we reject \\(H_0\\) and conclude that there is reason to believe that the proportion of allergic in Uppsala is greater than 0.3."
  },
  {
    "objectID": "infe_03hypparm.html#two-samples-proportions",
    "href": "infe_03hypparm.html#two-samples-proportions",
    "title": "3  Parametric tests",
    "section": "3.4 Two samples, proportions",
    "text": "3.4 Two samples, proportions\n\\[\n\\begin{aligned}\nH_0: \\pi_1 - \\pi_2 = 0\\\\\nH_1: \\pi_1 - \\pi_2 \\neq 0\n\\end{aligned}\n\\]\nAlternatively, a one sided alternative hypothesis can be used; \\[H_1: \\pi_1 - \\pi_2 >0\\textrm{ or }H_1: \\pi_1 - \\pi_2 < 0.\\]\n\n\nIf n is large the normal approximation can be used, an appropriate test statistic is\n\\[Z = \\frac{P_1 - P_2}{\\sqrt{P(1-P)\\left (\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}},\\]\nwhere \\(P\\) is the proportion in the merged sample of size \\(n_1 + n_2\\). \\(Z \\in N(0,1)\\) and p-value can be computed using the standard normal distribution.\nAlso the two sample proportions test is implemented in the function prop.test."
  },
  {
    "objectID": "infe_03hypparm.html#variance",
    "href": "infe_03hypparm.html#variance",
    "title": "3  Parametric tests",
    "section": "3.5 Variance",
    "text": "3.5 Variance\nThe test of equal variance in two groups is based on the null hypothesis\n\\[H_0: \\sigma_1^2 = \\sigma_2^2\\]\nIf the two samples both come from populations with normal distributions, the sample variances (also random variables) are\n\\[S_1^2 = \\frac{1}{n_1-1} \\sum_{i=1}^{n_1} (X_{1i}-\\bar X_1)^2\\\\\nS_2^2 = \\frac{1}{n_2-1} \\sum_{i=1}^{n_2} (X_{2i}-\\bar X_2)^2\\]\nIt can be shown that \\(\\frac{(n_1-1)S_1^2}{\\sigma_1^2} \\sim \\chi^2(n_1-1)\\) and \\(\\frac{(n_2-1)S_2^2}{\\sigma_2^2} \\sim \\chi^2(n_2-1)\\).\nHence, the test statistic for comparing the variances of two groups\n\\[F = \\frac{S_1^2}{S_2^2}\\] is \\(F\\)-distributed with \\(n_1-1\\) and \\(n_2-1\\) degrees of freedom.\nIn R a test of equal variances can be performed using the function var.test."
  },
  {
    "objectID": "infe_04multiple.html#error-types",
    "href": "infe_04multiple.html#error-types",
    "title": "4  Multiple testing",
    "section": "4.1 Error types",
    "text": "4.1 Error types\n\n\n\n\nTable 4.1:  The outcome of a statistical test is either to accept or reject the null hypothesis H0. The test result might agree with the truth or not, either H0 is true or false. TN - true negative, TP - true positive, FN - false negative, FP - false positive. \n\n  \n     \n    Accept H0 \n    Reject H0 \n  \n  \n    H0 is true \n    TN \n    Type I error, false alarm, FP \n  \n  \n    H0 is false \n    Type II error, miss, FN \n    TP \n  \n\n\n\n\n\n\nRemember from Section 1.4 that the probability of type I and II errors are denoted \\(\\alpha\\) and \\(\\beta\\), respectively;\n\\[\\alpha = P(\\textrm{type I error}) = P(\\textrm{false alarm}) = P(\\textrm{Reject }H_0|H_0 \\textrm{ is true})\\] \\[\\beta = P(\\textrm{type II error}) = P(\\textrm{miss}) = P(\\textrm{Accept }H_0|H_1 \\textrm{ is true})\\] and the statistical power\n\\[\\textrm{power} = 1 - \\beta = P(\\textrm{Reject }H_0 | H_1\\textrm{ is true}).\\]\n\n\n\n\n\nFigure 4.1: The probability density functions under H0 and H1, respectively. The probability of type I error (\\(\\alpha\\)) and type II error (\\(\\beta\\)) are indicated."
  },
  {
    "objectID": "infe_04multiple.html#multiple-testing",
    "href": "infe_04multiple.html#multiple-testing",
    "title": "4  Multiple testing",
    "section": "4.2 Multiple testing",
    "text": "4.2 Multiple testing\nIf a single test is perform we know that\n\nP(One type I error) = \\(\\alpha\\)\nP(No type I error) = \\(1 - \\alpha\\)\n\nIf \\(m\\) independent tests are performed (e.g. investigate many genes or proteins) the risk of false alarm (type I error) increases;\n\nP(No type I errors in \\(m\\) tests) = \\((1 - \\alpha)^m\\)\nP(At least one type I error in \\(m\\) tests) = \\(1 - (1 - \\alpha)^m\\)\n\n\n\n\n\n\n\n\n\n\nTwo common principles for dealing with multiple testing are control of family-wise error rate or false discovery rate.\n\nFWER: family-wise error rate, control the probability of one or more false positive \\(P(N_{FP}>0)\\), e.g. Bonferroni, Holm\nFDR: false discovery rate, control the expected value of the proportion of false positives among hits, \\(E[N_{FP}/(N_{FP}+N_{TP})]\\), e.g. Benjamini-Hochberg, Storey"
  },
  {
    "objectID": "infe_04multiple.html#bonferroni-correction",
    "href": "infe_04multiple.html#bonferroni-correction",
    "title": "4  Multiple testing",
    "section": "4.3 Bonferroni correction",
    "text": "4.3 Bonferroni correction\nTo achieve a family-wise error rate of \\(FWER \\leq \\gamma\\) when performing \\(m\\) tests, declare significance and reject the null hypothesis for any test with \\(p \\leq \\gamma/m\\).\nObjections: too conservative"
  },
  {
    "objectID": "infe_04multiple.html#benjamini-hochbergs-fdr",
    "href": "infe_04multiple.html#benjamini-hochbergs-fdr",
    "title": "4  Multiple testing",
    "section": "4.4 Benjamini-Hochbergs FDR",
    "text": "4.4 Benjamini-Hochbergs FDR\n\n\n\n\n\n  \n     \n    H0 is true \n    H0 is false \n  \n  \n    Accept H0 \n    TN \n    FN \n  \n  \n    Reject H0 \n    FP \n    TP \n  \n\n\n\n\n\nThe false discovery rate is the proportion of false positives among ‘hits’, i.e. \\(\\frac{FP}{TP+FP}\\).\nBenjamini-Hochberg’s method control the FDR level, \\(\\gamma\\), when performing \\(m\\) independent tests, as follows:\n\nSort the p-values \\(p_1 \\leq p_2 \\leq \\dots \\leq p_m\\).\nFind the maximum \\(j\\) such that \\(p_j \\leq \\gamma \\frac{j}{m}\\).\nDeclare significance for all tests \\(1, 2, \\dots, j\\)."
  },
  {
    "objectID": "infe_04multiple.html#adjusted-p-values",
    "href": "infe_04multiple.html#adjusted-p-values",
    "title": "4  Multiple testing",
    "section": "4.5 ‘Adjusted’ p-values",
    "text": "4.5 ‘Adjusted’ p-values\nSometimes an adjusted significance threshold is not reported, but instead ‘adjusted’ p-values are reported.\n\nUsing Bonferroni’s method the ‘adjusted’ p-values are:\n\n\\(\\tilde p_i = \\min(m p_i, 1)\\).\nA feature’s adjusted p-value represents the smallest FWER at which the null hypothesis will be rejected, i.e. the feature will be deemed significant.\n\nBenjamini-Hochberg’s ‘adjusted’ p-values are called \\(q\\)-values:\n\n\\(q_i = \\min(\\frac{m}{i} p_i, 1)\\)\nA feature’s \\(q\\)-value can be interpreted as the lowest FDR at which the corresponding null hypothesis will be rejected, i.e. the feature will be deemed significant.\n\nExample 4.1 (10000 independent tests (e.g. genes)**)  \n\n\n\n \n  \n    p-value \n    adj p (Bonferroni) \n    q-value (B-H) \n  \n \n\n  \n    1.7e-08 \n    0.0002 \n    0.0002 \n  \n  \n    5.8e-08 \n    0.0006 \n    0.0003 \n  \n  \n    3.4e-07 \n    0.0034 \n    0.0011 \n  \n  \n    9.1e-07 \n    0.0091 \n    0.0020 \n  \n  \n    1e-06 \n    0.0100 \n    0.0020 \n  \n  \n    2.4e-06 \n    0.0240 \n    0.0040 \n  \n  \n    2.3e-05 \n    0.2300 \n    0.0329 \n  \n  \n    3.6e-05 \n    0.3600 \n    0.0450 \n  \n  \n    0.00022 \n    1.0000 \n    0.2300 \n  \n  \n    0.00023 \n    1.0000 \n    0.2300 \n  \n  \n    0.00073 \n    1.0000 \n    0.6636 \n  \n  \n    0.0032 \n    1.0000 \n    1.0000 \n  \n  \n    0.0045 \n    1.0000 \n    1.0000 \n  \n  \n    0.0087 \n    1.0000 \n    1.0000 \n  \n  \n    0.0089 \n    1.0000 \n    1.0000 \n  \n  \n    0.012 \n    1.0000 \n    1.0000 \n  \n  \n    0.014 \n    1.0000 \n    1.0000 \n  \n  \n    0.045 \n    1.0000 \n    1.0000 \n  \n  \n    0.08 \n    1.0000 \n    1.0000 \n  \n  \n    0.23 \n    1.0000 \n    1.0000"
  },
  {
    "objectID": "infe_exr2_hypoparam_solutions.html",
    "href": "infe_exr2_hypoparam_solutions.html",
    "title": "Exercises: Hypothesis tests, parametric",
    "section": "",
    "text": "Exercise 1 The hemoglobin value (Hb) in women is on average 140 g/L. You observe the following Hb values in a set of five male blood donors: 154, 140, 147, 162, 172. Assume that Hb is normally distributed. Is there a reason to believe that the mean Hb value in men differ from that in women?\n\n​Solution\n\n\n\n\n\nLet \\(X\\) denote the Hb value in g/L for male blood donors.\n\\[H_0: \\bar X = 140\\] \\[H_1: \\bar X \\neq 140\\]\nUse the significance level \\(\\alpha=0.05\\).\nUse the test statistic \\(T = \\frac{\\bar X - \\mu_0}{SE},\\) where \\(\\mu_0\\) is the population mean if \\(H_0\\) is true, i.e. 140 and \\(SE\\) is the standard error of mean, \\(SE=\\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}}\\).\nCompute \\(t_{obs} = \\frac{m-140}{s/\\sqrt{n}}\\)\n\nx <- c(154, 140, 147, 162, 172)\n## sample mean\nm <- mean(x)\n## standard error of mean\nSE <- sd(x)/sqrt(5)\n## Observed value of test statistic\n(tobs <- (m-140)/SE)\n\n[1] 2.676865\n\n\nCompute the p-value, \\(P(|T|>|t_{obs}|) = P(T>|t_{obs}|) + P(T<-|t_{obs}|)\\)\n\n## P(T>tobs)\npt(tobs, df=4, lower.tail=FALSE)\n\n[1] 0.02770443\n\n## p = P(t>tobs) + P(t<-tobs) = 2 * P(t>tobs)\n(p <- 2*pt(tobs, df=4, lower.tail=FALSE))\n\n[1] 0.05540887\n\n\nAs \\(p > \\alpha\\) the null hypothesis is accepted and we conclude that there is not reason to believe that the Hb values for men differ from that of women.\nNote that the same can be achieved uing the function t.test;\n\n\n\n    One Sample t-test\n\ndata:  x\nt = 2.6769, df = 4, p-value = 0.05541\nalternative hypothesis: true mean is not equal to 140\n95 percent confidence interval:\n 139.442 170.558\nsample estimates:\nmean of x \n      155 \n\n\n\n\n\n\n\nExercise 2 The hemoglobin value (Hb) in men is on average 188 g/L. The Hb values in Exercise 1 were actually measured after the men had donated blood. Is there a reason to believe that the mean Hb level for men after blood donation is less than 188 g/L?\n\n​Solution\n\n\n\n\n\n\\(H_0: \\bar X = 188\\) \\(H_1: \\bar X < 188\\)\nUse the significance level \\(\\alpha=0.5\\).\nUse the test statistic \\(T = \\frac{\\bar X - \\mu_0}{SE},\\) where \\(\\mu_0=188\\).\nCompute \\(t_{obs} = \\frac{m-188}{s/\\sqrt{n}}\\)\n\n## Observed value of test statistic\n(tobs <- (m-188)/SE)\n\n[1] -5.889103\n\n\nCompute the p-value, \\(P(T<t_{obs})\\)\n\n## p=P(T<tobs)\n(p <- pt(tobs, df=4, lower.tail=TRUE))\n\n[1] 0.002078429\n\n\nAs \\(p < \\alpha\\) the null hypothesis is rejected and we can conclude that there is reason to believe that the Hb values after blood donation is less than 188 g/L.\nNote that the same can be achieved uing the function t.test;\n\nt.test(x, mu=188, alternative=\"less\")\n\n\n    One Sample t-test\n\ndata:  x\nt = -5.8891, df = 4, p-value = 0.002078\nalternative hypothesis: true mean is less than 188\n95 percent confidence interval:\n    -Inf 166.946\nsample estimates:\nmean of x \n      155 \n\n\n\n\n\n\n\nExercise 3 By observing the Hb values in 5 male blood donors; 154, 140, 147, 162, 172 g/L, and 5 female blood donors: 123, 140, 137, 132, 127 g/L, is there a reason to believe that the Hb level is higher in men than in women?\n\n​Solution\n\n\n\n\n\nLet \\(X_m\\) denote the Hb value in men and \\(X_w\\) the Hb value in women.\n\\(H_0: \\bar X_m = \\bar X_w\\) \\(H_1: \\bar X_m > \\bar X_w\\)\nUse the significance level \\(\\alpha=0.5\\).\nUse the test statistic \\(T = \\frac{\\bar X_m - \\bar X_w}{SE}\\). If we assume equal variances we can use Student’s t-test and compute SE as the pooled standard deviation. An alternative is to use Welch t-test (unequal variances t-test) and compute \\(SE=\\sqrt{\\frac{s_m^2}{n_m} + \\frac{s_w^2}{n_w}}\\), the test statistic is t-distributed and the degrees of freedom can be approximated using Welch-Satterthwaite’s equation, as implemented in t.test.\n\nx <- c(154, 140, 147, 162, 172)\ny <- c(123, 140, 137, 132, 127)\n## Perform t-test with unequal variances\nt.test(x, y, alternative=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 3.6171, df = 6.2637, p-value = 0.00517\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 10.8297     Inf\nsample estimates:\nmean of x mean of y \n    155.0     131.8 \n\n\nAs the p-value \\(p=0.0051699\\) is \\(<\\alpha\\), we reject the null hypothesis and conclude that there is reason to belive that Hb on average is higher in male blood donors than in female.\n\n\n\n\n\nExercise 4 Based on statistics from blodcentralen we learn that male Hb values (before donation) is normally distributed with mean 188 g/L and standard deviation 16 g/L. Using this new information and the following observed Hb values in five male donors after donation; 154, 140, 147, 162, 172 g/L, is there reason to believe that the mean Hb value for men after blood donation is lower than 188 g/L?\n\n​Solution\n\n\n\n\n\n\\(H_0: \\bar X = 188\\) \\(H_1: \\bar X < 188\\)\nUse \\(\\alpha = 0.05\\)\nUnder \\(H_0\\);\n\\(X \\sim N(188, 16)\\)\n\\(\\bar X \\sim N(188, 16/\\sqrt{5})\\)\nUse the test statistic\n\\(Z = \\frac{\\bar X - 188}{16/\\sqrt{5}} \\sim N(0,1)\\)\nCompute \\(z_{obs}\\)\n\n\n\nCompute the p-value $P(X ) = P(Z zobs) = $\n\n\n[1] 1.995119e-06\n\n\nAs \\(p<<\\alpha\\), reject the \\(H_0\\), there is reason to believe that the mean Hb value for men after blood donation is lower than 188 g/L.\n\n\n\n\n\nExercise 5 In order to study the effect of high-fat diet 12 mice are fed normal diet (control group) and 12 mice are fed high-fat diet. After a couple of weeks the mouse weights in gram are recorded;\n\n\n\nHigh-fat mice (g): 25, 30, 23, 18, 31, 24, 39, 26, 36, 29, 23, 32 Normal diet mice (g): 27, 25, 22, 23, 25, 37, 24, 26, 21, 26, 30, 24\nDoes high fat diet increase body weight in mice?\n\nAssume equal variances.\nDon’t assume equal variances.\n\n\n​Solution\n\n\n\n\n\n\n\n\n\n## Student's t-test with pooled variances\nt.test(xHF, xN, var.equal=TRUE, alternative=\"greater\")\n\n\n    Two Sample t-test\n\ndata:  xHF and xN\nt = 1.0234, df = 22, p-value = 0.1586\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -1.468785       Inf\nsample estimates:\nmean of x mean of y \n 28.00000  25.83333 \n\n\n\n\n\n\n## Unequal variances with Welch approximation to the degrees of freedom (the default)\nt.test(xHF, xN, var.equal=FALSE, alternative=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  xHF and xN\nt = 1.0234, df = 19.818, p-value = 0.1592\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -1.486449       Inf\nsample estimates:\nmean of x mean of y \n 28.00000  25.83333"
  },
  {
    "objectID": "infe_05interval.html#point-estimate",
    "href": "infe_05interval.html#point-estimate",
    "title": "5  Point and interval estimates",
    "section": "5.1 Point estimate",
    "text": "5.1 Point estimate\nUnknown population parameters can be inferred from estimates from random samples from the population of interest. The sample estimate will be our best guess, a point estimate, of the population parameter.\n\nExample 5.1 (Pollen) If we are interested in how large proportion of the Uppsala population is allergic to pollen, we can investigate this by studying a random sample, e.g. randomly select 100 persons in Uppsala. It is important to actually sample randomly, ideally every individual in the population should have the same probability of being sampled.\nIn our sample, we observed that 42 of the 100 has a pollen allergy. Hence, the observed sample proportion is \\(p=0.42\\).\n\nBased on this random sample our point estimate of the Uppsala popultation proportion \\(\\pi\\) is \\(\\pi \\approx p = 0.42\\). There is an uncertainty in this measurement, if the experiment is repeated 100 other persons would be selected and our point estimate would be slightly different.\n\n\nExample 5.2 (Weight) The mean weight of a mouse population, \\(\\mu\\) is unknown. By taking a sample of size \\(n\\) and measure the mean weight of the mice in the sample, \\(m\\), we measure a point estimate of the population mean."
  },
  {
    "objectID": "infe_05interval.html#bias-and-precision",
    "href": "infe_05interval.html#bias-and-precision",
    "title": "5  Point and interval estimates",
    "section": "5.2 Bias and precision",
    "text": "5.2 Bias and precision\nThe sample proportion and sample mean are unbiased point estimates of the corresponding population parameters. It will not be without error, but the larger the sample, the smaller the error.\nThe expected value of an unbiased point estimate is the the population parameter that it estimates, This means that if you would repeat the sampling many times and compute the point estimate of interest, e.g. sample mean, the average of the sample means would be the population mean.\n\n\n\n\n\nFigure 5.1: Bias and precision.\n\n\n\n\nEven an unbiased point estimate is not perfect, it will have a certain amount of uncertainty."
  },
  {
    "objectID": "infe_05interval.html#interval-estimates",
    "href": "infe_05interval.html#interval-estimates",
    "title": "5  Point and interval estimates",
    "section": "5.3 Interval estimates",
    "text": "5.3 Interval estimates\nTo show the uncertainty an interval estimate for a population parameter can be computed instead of just a point estimate. An interval estimate is an interval of possible values that with high probability contains the true population parameter. An interval estimate is estimated based on sample data. Interval estimation, just like point estimation, is a type of statistical inference.\nThe width of the interval estimate can be determined from the sampling distribution, i.e. the distribution of the sample property of interest. The dispersion of the sampling distribution is related to the uncertainty of the point estimate and can be used to define the interval estimate. As seen before there are different ways to estimate the sampling distribution.\n\n5.3.1 Bootstrap interval\nIf the distribution of the sample statistic of interest is unknown, a bootstrap confidence interval can be computed instead.\nBootstraping is performed by resampling with replacement from the available random sample. The resampling distribution can then be used to calculate e.g. a 95% bootstrap interval.\nThis can be done as follows;\n\nPut the entire sample in an urn!\n\n\n\n\n\n\nAn urn model with 42 allergy (black) and 58 non-allergy (white). The black balls represent allergic and the white balls non-allergic.\n\n\n\n\nIn R;\n\n## the sample consists of 50 non-allergic (0) and 42 allergic (1).\nx <- rep(0:1, c(58, 42))\n\n\nSample from the urn with replacement to compute the bootstrap distribution.\n\n\npboot <- replicate(1000, mean(sample(x, replace=TRUE)))\n\n\nggplot(data.frame(x=pboot), aes(x=x)) + geom_histogram(color=\"white\", binwidth=0.02) + theme_bw() + xlab(\"p\")\n\n\n\n\nFigure 5.2: Bootstrap resampling distribution.\n\n\n\n\n\nCompute the 95% bootstrap interval. This can be done using the percentile method by calculating the 2.5 and 97.5 percentiles, i.e. the values in the resampling distribution for which 2.5% and 97.5%, respectively, of all the values lie to the left of.\n\n\nciboot <- quantile(pboot, c(0.025, 0.975))\n\n\n\n\n\n\nFigure 5.3: Bootstrap resampling distribution and the 95% bootstrap confidence interval.\n\n\n\n\nThe 95% bootstrap confidence interval of \\(\\pi\\); [0.32, 0.51].\n\n\n5.3.2 Confidence interval\nA confidence interval is a type of interval estimate associated with a confidence level.\n\nDefinition 5.1 A confidence interval for \\(\\theta\\) with confidence level \\(1 - \\alpha\\) is an interval that with probability \\(1 - \\alpha\\) cover the population parameter \\(\\theta\\).\n\n\n\n\n\n\nFigure 5.4: A 95% confidence interval will have 95% chance to cover the true value.\n\n\n\n\n\nIf the sampling distribution of studied statistic is known the confidence interval can be computed.\n\n\n\n\n\nFigure 5.5: The sampling distribution of theta. The probability that the observed value of a random sample falls in the\n\n\n\n\n\n\n5.3.3 Confidence interval of proportions\nThe bootstrap is very useful if you do not know the distribution of our sampled property, but in our proportions example we actually do.\nRemember that we can use the central limit theorem to show that\n\\[P \\sim N\\left(\\pi, SE\\right) \\iff P \\sim N\\left(\\pi, \\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\right)\\]\nIt follows that\n\\[Z = \\frac{P - \\pi}{SE} \\sim N(0,1)\\] Based on what we know of the standard normal distribution, we can compute an interval around the population property \\(\\pi\\) such that the probability that a sample property \\(p\\) fall within this interval is \\(1-\\alpha\\).\n\\[P(-z_{\\alpha/2} < \\frac{P - \\pi}{SE} < z_{\\alpha/2}) = 1 - \\alpha\\]\nFor a 95% confidence interval \\(z_{0.025}=1.96\\) (from a table of the standard normal distribution). Other confidence levels of interest include 90% (\\(z_{0.05}=1.64\\)) and 99% (\\(z_{0.005}=2.58\\)). In R, the function ´qnorm` is used to compute \\(z_{\\alpha/2}\\),\n\n## 95% confidence interval, alpha=0.05\nqnorm(1-0.05/2)\n\n[1] 1.959964\n\n## 90% confidence interval, alpha=0.10\nqnorm(1-0.10/2)\n\n[1] 1.644854\n\n## 99% confidence interval, alpha=0.01\nqnorm(1-0.01/2)\n\n[1] 2.575829\n\n\nThe unequality can be rewritten;\n\\[-z_{\\alpha/2} < \\frac{P-\\pi}{SE}<z_{\\alpha/2} \\iff P-z_{\\alpha/2}SE \\leq \\pi \\leq P + z_{\\alpha/2}SE\\] Hence,\n\\[P\\left(P-z SE < \\pi < P + z SE\\right) = 1 - \\alpha\\] In words, the population proportion \\(\\pi\\) will lie within \\(\\pm z_{\\alpha/2}SE\\) from the the sample proportion \\(P\\).\nTo get the observed confidence interval, replace the random variable \\(P\\) with the observed value \\(p\\) (in our example 0.42).\nThe confidence interval can be expressed in different was;\n\\[p-z_{\\alpha/2} SE < \\pi < p + z_{\\alpha/2} SE\\] \\[\\pi = p \\pm z_{\\alpha/2} SE\\] \\[(p - z_{\\alpha/2} SE, p + z_{\\alpha/2} SE)\\]\nThe 95% confidence interval \\[\\pi = p \\pm 1.96 \\sqrt{\\frac{p(1-p)}{n}}\\] will have 95% chance to cover the true value.\n\n\n\n\n\nBack to our example of proportion pollen allergic in Uppsala. \\(p=0.42\\) and \\(SE=\\sqrt{\\frac{p(1-p)}{n}} = 0.0493559\\).\nHence, the 95% confidence interval is \\[\\pi = 0.42 \\pm 1.96 * 0.05 = 0.42 \\pm 0.092\\] or \\[(0.42-0.092, 0.42+0.092) = (0.32, 0.52)\\]\n\n\n5.3.4 Confidence interval of mean\nThe confidence interval of mean can be derived similarly.\nThe mean of a sample of \\(n\\) independent and identically normal distributed observations \\(X_i\\) is normally distributed;\n\\[\\bar X \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\]\nIf \\(\\sigma\\) is unknown the statistic\n\\[T = \\frac{\\bar X - \\mu}{\\frac{s}{\\sqrt{n}}}\\]\nis t-distributed with \\(n-1\\) degrees of freedom, in short \\(T \\sim t(n-1)\\).\nIt follows that\n\\[\n  \\begin{aligned}\nP\\left(-t < \\frac{\\bar X - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} < t\\right) = 1 - \\alpha \\iff \\\\\nP\\left(\\bar X - t \\frac{\\sigma}{\\sqrt{n}} < \\mu < \\bar X + t \\frac{\\sigma}{\\sqrt{n}}\\right) = 1 - \\alpha\n\\end{aligned}\n\\]\nThe confidence interval with confidence level \\(1-\\alpha\\) is thus;\n\\[\\mu = \\bar x \\pm t \\frac{s}{\\sqrt{n}}\\]\nThe \\(t\\) values for different values of \\(\\alpha\\) and degrees of freedom are tabulated and can be computed in R using the function qt.\nFor a 95% confidence interval, i.e. \\(\\alpha=0.05\\), and \\(n=5\\), \\(t\\) is 2.7764451 and can be computed using\n\nn=5\nalpha = 0.05\n## t value\nqt(1-alpha/2, df=n-1)\n\n[1] 2.776445"
  },
  {
    "objectID": "infe_exr3_interval_solutions.html",
    "href": "infe_exr3_interval_solutions.html",
    "title": "Exercises: Point and interval estimates",
    "section": "",
    "text": "Exercise 1 You measure the Hb value in 10 50-year old men and get the following observations; 145, 165, 134, 167, 158, 176, 156, 189, 143, 123 g/L.\n\nCompute a 95% bootstrap interval for the mean Hb value.\nCompute the sample mean Hb value\nCompute the sample variance\nCompute the sample standard deviation\nAssume that Hb is normally distributed and compute a point estimate and a 95% confidence interval for the mean Hb value.\n\n\n​Solution\n\n\n\n\n\n\n\n\n\nobs <- c(145, 165, 134, 167, 158, 176, 156, 189, 143, 123)\nmboot <- replicate(1000, {\n  x <- sample(obs, size=10, replace=TRUE)\n  mean(x)\n})\nhist(mboot)\n\n\n\n## 95% confidence interval\nquantile(mboot, c(0.025, 0.975))\n\n   2.5%   97.5% \n143.495 167.300 \n\n\n\nSample mean\n\n\n(m <- mean(obs))\n\n[1] 155.6\n\n\n\nSample variance\n\n\n(v <- var(obs))\n\n[1] 395.1556\n\n\n\nSample standard deviation\n\n\n(s <- sd(obs))\n\n[1] 19.87852\n\n\n\nThe point estimate is the sample mean, \\(m=155.6\\).\n\nThe sample size is small (\\(n=10\\)) and the population standard deviation unknown, hence we use the t-statistic;\n\\[T = \\frac{\\bar X - \\mu}{\\frac{s}{\\sqrt{n}}}\\] and compute the 95% confidence interval as\n\\[\\mu = m \\pm t_{\\alpha/2} \\frac{s}{\\sqrt{n}}\\]\n\nn <- length(obs)\nt <- qt(0.975, df=9)\n##95% confidence interval \nc(m - t*s/sqrt(n), m + t*s/sqrt(n))\n\n[1] 141.3798 169.8202\n\n\n\n\n\n\n\nExercise 2 The 95% confidence interval for a proportion can be computed using the formula \\(\\pi = p \\pm z SE,\\) where \\(\\pi\\) is the population prportion, \\(p\\) the sample proportion and the standard error \\(SE = \\sqrt{\\frac{p(1-p)}{n}}\\). \\(z=1.96\\) for a 95% confidence interval.\nWe study the proportion of pollen allergic people in Uppsala and in a random sample of size 100 observe 42 pollen allergic people.\n\nCalculate a 95% confidence interval for \\(\\pi\\)\nHow can we get a narrower confidence interval?\nWe computed a 95% interval, what if we want a 90% confidence interval?\nor a 99% confidence interval?\n\n\n​Solution\n\n\n\n\n\n\nSee lecture notes\nCalculate a 90% confidence interval instead. Or sample more people than 100.\nChange the z number,\n\n\\[\\pi = p \\pm z SE\\]\nFor a 90% confidence interval use z=1.64\n\np <- 0.42\nn <- 100\nSE <- sqrt(p*(1-p)/n)\nz <- qnorm(0.95)\nc(p - z*SE, p + z*SE)\n\n[1] 0.3388168 0.5011832\n\n\n\nor a 99% confidence interval?\n\n\nz <- qnorm(0.995)\nc(p - z*SE, p + z*SE)\n\n[1] 0.2928678 0.5471322\n\n\n\n\n\n\n\nExercise 3 A scale has a normally distributed error with mean 0 and standard deviation 2.3 g. You measure an object 10 times and observe the mean weight 43 g.\n\nCompute a 95% confidence interval of the objects weight\nCompute a 90% confidence interval of the objects weight\n\n\n​Solution\n\n\n\n\n\nThe measured weight is a random variable \\(X \\sim N(\\mu, \\sigma)\\). You know that \\(\\sigma = 2.3\\), \\(\\mu\\) is the weight of the object.\n\nCompute a 95% confidence interval of the sample weight\n\n\n## 95% confidence interval\nm <- 42\nsigma <- 2.3\nn <- 10\nz <- qnorm(0.975)\nc(m - z*sigma/sqrt(10), m + z*sigma/sqrt(10))\n\n[1] 40.57447 43.42553\n\n\n\nCompute a 90% confidence interval of the sample weight\n\n\nz <- qnorm(0.95)\nc(m - z*sigma/sqrt(10), m + z*sigma/sqrt(10))\n\n[1] 40.80366 43.19634\n\n\n\n\n\n\n\nExercise 4 You observe 150 students at BMC of which 25 are smokers. Compute a 95% confidence interval for the proportion of smokers among BMC students.\n\n​Solution\n\n\n\n\n\nPoint estimate of proportion smokers; \\(p=25/150=1/6\\).\n\\(\\pi = p \\pm z SE\\)\n\np <- 25/150\nn <- 150\nz <-qnorm(0.975)\nSE <- sqrt(p*(1-p)/n)\n## 95% CI\nc(p - z*SE, p + z*SE)\n\n[1] 0.1070269 0.2263065"
  }
]