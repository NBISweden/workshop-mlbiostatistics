<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Parametric tests | Statistical inference" />
<meta property="og:type" content="book" />




<meta name="author" content="NBIS" />

<meta name="date" content="2022-09-13" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Parametric tests | Statistical inference">

<title>Parametric tests | Statistical inference</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<!--bookdown:toc:end-->
<!--bookdown:toc:start-->
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="infe-hypparam" class="section level1">
<h1>Parametric tests</h1>
<p>In the previous chapter we computed the sampling distribution using resampling techniques to be able to perform hypothesis tests. If the null distribution was already known, or could be computed based on a few assumptions, resampling would not be necessary.</p>
<p>We can follow the same steps as before to perform a hypothesis test:</p>
<ol style="list-style-type: decimal">
<li>Define <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span></li>
<li>Select an appropriate significance level, <span class="math inline">\(\alpha\)</span></li>
<li>Select appropriate test statistic, <span class="math inline">\(T\)</span>, and compute the observed value, <span class="math inline">\(t_{obs}\)</span></li>
<li>Assume that the <span class="math inline">\(H_0\)</span> and derive the null distribution of the test statistic based on appropriate assumptions.</li>
<li>Compare the observed value, <span class="math inline">\(t_{obs}\)</span>, with the null distribution and compute a p-value. The p-value is the probability of observing a value at least as extreme as the observed value, if <span class="math inline">\(H_0\)</span> is true.</li>
<li>Based on the p-value either accept or reject <span class="math inline">\(H_0\)</span>.</li>
</ol>
<p>In this section we will present a few common situation in which the null distribution can be described parametrically.</p>
<div id="one-sample-proportions" class="section level2">
<h2>One sample, proportions</h2>
<div class="example">
<p><span id="exm:parampollen" class="example"><strong>Example 1  </strong></span><strong>Letâ€™s get back to the pollen example!</strong></p>
<p>Assume that the proportion of pollen allergy in Sweden is known to be <span class="math inline">\(0.3\)</span>. Observe 100 people from Uppsala, 42 of these were allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala <span class="math inline">\(\pi &gt; 0.3\)</span>?</p>
</div>
<div id="null-and-alternative-hypotheses-2" class="section level5 unnumbered">
<h5>Null and alternative hypotheses</h5>
<p><span class="math display">\[H_0:\, \pi=\pi_0\]</span></p>
<p><span class="math display">\[H_1:\, \pi&gt;\pi_0,\]</span></p>
<p>where <span class="math inline">\(\pi_0=0.30\)</span> in this example. Other potential alternative hypothesis are <span class="math inline">\(H_1: \pi&lt;\pi_0\)</span> or <span class="math inline">\(H_1:\pi \neq \pi_0\)</span>, but in this particular example we are only interested in the alternative that <span class="math inline">\(\pi &gt; \pi_0\)</span>.</p>
</div>
<div id="significance-level" class="section level5">
<h5>Significance level</h5>
<p>Set the signifgicance level to <span class="math inline">\(\alpha=0.05\)</span>.</p>
</div>
<div id="test-statistic-2" class="section level5 unnumbered">
<h5>Test statistic</h5>
<p>Here, we will use <span class="math inline">\(X\)</span>, the number of allergic persons in a random sample of size <span class="math inline">\(n=100\)</span>. The observed value is <span class="math inline">\(x_{obs} = 42\)</span>.</p>
</div>
<div id="null-distribution-2" class="section level5 unnumbered">
<h5>Null distribution</h5>
<p><span class="math inline">\(X\)</span> is binomially distributed, so there is no need to use resampling here, we can use the binomial distribution to answer the question.</p>
</div>
<div id="p-value" class="section level5 unnumbered">
<h5>p-value</h5>
<p>The probaility of <span class="math inline">\(x_{obs}\)</span> or something higher,</p>
<p><span class="math inline">\(P(X \geq 42) = 1 - P(X \leq 41)\)</span> [<code>1-pbinom(41,100,0.3)</code>] = 0.007174</p>
</div>
<div id="accept-or-recject-h_0" class="section level5">
<h5>Accept or recject <span class="math inline">\(H_0\)</span>?</h5>
<p>As <span class="math inline">\(p&lt;0.05\)</span> <span class="math inline">\(H_0\)</span> is rejected and we conclude that there is reason to believe that the proportion of pollen allergic in Uppsala is higher than 0.3.</p>
<p>This p-value can also be computed using the exact binomial test;</p>
<pre><code>## 
##  Exact binomial test
## 
## data:  42 and 100
## number of successes = 42, number of trials = 100, p-value = 0.007
## alternative hypothesis: true probability of success is greater than 0.3
## 95 percent confidence interval:
##  0.3365 1.0000
## sample estimates:
## probability of success 
##                   0.42</code></pre>
<p>An alternative approach is to use the Central limit theorem, see <a href="infe-hypparam.html#thm:CLTrep">1</a>, and use the normal approximation.</p>
<div class="theorem">
<p><span id="thm:CLTrep" class="theorem"><strong>Theorem 1  </strong></span>The sum of <span class="math inline">\(n\)</span> independent and equally distributed random variables
is normally distributed, if <span class="math inline">\(n\)</span> is large enough.</p>
</div>
<p>As a result of the central limit theorem, the distribution of number or proportion of allergic individuals in a sample of size <span class="math inline">\(n\)</span> is approximately normal. At least if the sample is large enough. A rule of thumb is that the sample size should be <span class="math inline">\(n&gt;30\)</span>.</p>
<p>Here, the sample size is 100!</p>
<p>This test of proportions using the normal approximation is implemented in in the r-function <code>prop.test</code>.</p>
<pre><code>## 
##  1-sample proportions test with continuity correction
## 
## data:  42 out of 100, null probability 0.3
## X-squared = 6.3, df = 1, p-value = 0.006
## alternative hypothesis: true p is greater than 0.3
## 95 percent confidence interval:
##  0.3372 1.0000
## sample estimates:
##    p 
## 0.42</code></pre>
<p>But can of course also be calculated using only the normal distribution table.</p>
<p>The normal distribution has two parameters, mean and standard deviation.</p>
<p>From the binomial distribution we know that <span class="math inline">\(E[X] = n\pi\)</span> and <span class="math inline">\(var(X) = n\pi(1-\pi)\)</span>.</p>
<p>The standard error of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[SE=\sqrt{n\pi(1-\pi)}\]</span></p>
<p>If <span class="math inline">\(H_0\)</span> is true <span class="math inline">\(\pi=\pi_0\)</span> and</p>
<p><span class="math display">\[X \sim N\left(n\pi_0, \sqrt{n\pi_0(1-\pi_0)}\right)\]</span>
<!-- An appropriate test statistic is -->
<!-- $$Z = \frac{X-n\pi_0}{\sqrt{n\pi_0(1-\pi_0)}}$$ -->
<!-- $Z \in N(0,1)$ which makes probabilities easy to compute. --></p>
<p>With this null distribution and ur observed value <span class="math inline">\(x_{obs} = 42\)</span>, the p-value can be computed.</p>
<p><span class="math inline">\(p = P(X \geq 42)\)</span></p>
<p>As we are now approximating a discrete distribution using a continuous distribution we need to use a trick called <em>continuiuty correction</em>, which simply means that we let each integer be represented by y a region <span class="math inline">\(\pm 0.5\)</span> from its value. Hence,</p>
<p><span class="math display">\[p = P(X \geq 42) = [continuity \; correction] = \]</span>
<span class="math display">\[P(X \geq 41.5) = P\left(Z \geq \frac{41.5 - n\pi_0}{\sqrt{n\pi_0(1-\pi_0)}}\right) = \]</span>
<span class="math display">\[P\left(Z \geq \frac{41.5-30}{\sqrt{100*0.3*(1-0.3)}}\right) = P(Z \geq 2.51) = 1 - P(Z \leq 2.51) = [Z-table] = 0.0060\]</span></p>
<!-- The p-value is the probability of the observed value, or something more extreme, if the null hypothesis is true. If the computed probability is below $\alpha=0.05$ our significance threshold, $H_0$ will be rejected. -->
<!-- $$p = P(P \geq \pi_0) = P(Z \geq Z_{obs}) = P(Z \geq` 2.51) = 1 - P(Z \leq 2.51) = [table] = 1 - 0.996 = 0.0044$$ -->
<p>As 0.00605&lt;0.05 we reject <span class="math inline">\(H_0\)</span> and conclude that there is reason to believe that the proportion of allergic in Uppsala is greater than 0.3.</p>
</div>
</div>
<div id="one-sample-mean" class="section level2">
<h2>One sample, mean</h2>
<p>A one sample test of means compares the mean of a sample to a prespecified value.</p>
<p>For example, we might know that the weight of a mouse on normal diet is normally distributed with mean 24.0 g and standard deviation 3 g and want to compare the weight of a sample of 10 mice on high-fat diet to the known mean value for mice on normal diet.</p>
<p>The hypotheses:</p>
<p><span class="math display">\[H_0: \mu = \mu_0 \\
H_1: \mu \neq \mu_0\]</span></p>
<p>The alternative hypothesis, <span class="math inline">\(H_1,\)</span> above is for the two sided hypothesis test. Other options are the one sided <span class="math inline">\(H_1\)</span>; <span class="math inline">\(H_1: \mu &gt; \mu_0\)</span> or <span class="math inline">\(H_1: \mu &lt; \mu_0\)</span>.</p>
<p>If <span class="math display">\[X \sim N(\mu, \sigma)\]</span> (this could for example be the weight of a mouse on high-fat diet) then the sample mean <span class="math display">\[\bar X \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right).\]</span></p>
<p>If <span class="math inline">\(\sigma\)</span> is known under the null hypothesis, then the test statistic</p>
<p><span class="math display">\[Z = \frac{\bar X - \mu_0}{\frac{\sigma}{\sqrt{n}}}\]</span>
is normally distributed, <span class="math inline">\(\sim N(0,1)\)</span>.</p>
<p>In many situations the the population standard deviation is not known, but can instead be estimated from the sample. For small <span class="math inline">\(n\)</span> and unknown <span class="math inline">\(\sigma\)</span>, we can use the <strong>one-sample t-test</strong>, that uses the test statistic</p>
<p><span class="math display">\[T = \frac{\bar X - \mu_0}{\frac{s}{\sqrt{n}}},\]</span></p>
<p>which is t-distributed with <span class="math inline">\(df=n-1\)</span> degrees of freedom.</p>
<p>Once we have an appropriate test statistic, <span class="math inline">\(T\)</span> with known distribution, we can compute the observed value, <span class="math inline">\(t_{obs}\)</span> and use the null distribution to compute the p-value, <span class="math inline">\(P(|T| \geq |t_{obs}|)\)</span>.</p>
<p>In R, the functions <code>pnorm</code> and <code>pt</code> are useful for computing these probabilities. The <strong>one-sample t-test</strong> can also be performed using the function <code>t.test</code>like this;</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="infe-hypparam.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="do">##The observed 10 mouse weights;</span></span>
<span id="cb15-2"><a href="infe-hypparam.html#cb15-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">25</span>, <span class="dv">30</span>, <span class="dv">23</span>, <span class="dv">18</span>, <span class="dv">31</span>, <span class="dv">24</span>, <span class="dv">39</span>, <span class="dv">26</span>, <span class="dv">36</span>, <span class="dv">29</span>, <span class="dv">23</span>, <span class="dv">32</span>)</span>
<span id="cb15-3"><a href="infe-hypparam.html#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Under the null hypothesis mu=24</span></span>
<span id="cb15-4"><a href="infe-hypparam.html#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="do">##Perform the t-test to investigate if the null hypotheis can be accepted, i.e. if the sample comes from a normal distribution with expected value 24.</span></span>
<span id="cb15-5"><a href="infe-hypparam.html#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(x, <span class="at">mu=</span><span class="dv">24</span>)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 2.3, df = 11, p-value = 0.04
## alternative hypothesis: true mean is not equal to 24
## 95 percent confidence interval:
##  24.2 31.8
## sample estimates:
## mean of x 
##        28</code></pre>
</div>
<div id="two-samples-proportions" class="section level2">
<h2>Two samples, proportions</h2>
<p><span class="math display">\[H_0: \pi_1 - \pi_2 = 0\\
H_1: \pi_1 - \pi_2 \neq 0\]</span></p>
<p>Alternatively, a one sided alternative hypothesis can be used; <span class="math inline">\(H_1: \pi_1 - \pi_2 &gt;0\)</span> or <span class="math inline">\(H_1: \pi_1 - \pi_2 &lt; 0\)</span>.
<!-- If $H_0$ is true --></p>
<!-- $$P_1 - P_2 \sim N\left(0, \sqrt{\pi(1-\pi)\left (\frac{1}{n_1} + \frac{1}{n_2}\right)} \right)$$ -->
<!-- where $\pi$ is the  -->
<p>If n is large the normal approximation can be used and an appropriate test statistic is</p>
<p><span class="math display">\[Z = \frac{P_1 - P_2}{\sqrt{P(1-P)\left (\frac{1}{n_1} + \frac{1}{n_2}\right)}},\]</span></p>
<p>where <span class="math inline">\(P\)</span> is the proportion in the merged sample of size <span class="math inline">\(n_1 + n_2\)</span>. <span class="math inline">\(Z \in N(0,1)\)</span> and p-value can be computed using the standard normal distribution.</p>
<p>Also the two sample proportions test is implemented in the function <code>prop.test</code>.</p>
</div>
<div id="two-samples-mean" class="section level2">
<h2>Two samples, mean</h2>
<p>A two sample test of means is used to determine if two population means are equal.</p>
<p>Two independent samples are collected (one from each population) and the means are compared. Can for example be used to determine if a treatment group is different compared to a control group, in terms of the mean of a property of interest.</p>
<p>The null hypothesis;</p>
<p><span class="math display">\[H_0: \mu_2 = \mu_1\]</span>
The alternative hypothesis can either be two sided</p>
<p><span class="math display">\[H_1: \mu_2 \neq \mu_1\]</span>
or one sided</p>
<p><span class="math inline">\(H_1: \mu_2 &gt; \mu_1\)</span> or <span class="math inline">\(H_1: \mu_2 &lt; \mu_1\)</span></p>
<p>Assume that observations from both populations are normally distributed;</p>
<p><span class="math display">\[
\begin{aligned}
X_1 \sim N(\mu_1, \sigma_1) \\
X_2 \sim N(\mu_2, \sigma_2)
\end{aligned}
\]</span>
Then it follows that the sample means will also be normally distributed;</p>
<p><span class="math display">\[
\begin{aligned}
\bar X_1 \sim N(\mu_1, \sigma_1/\sqrt{n_1}) \\
\bar X_2 \sim N(\mu_2, \sigma_2/\sqrt{n_2})
\end{aligned}
\]</span></p>
<p>The mean difference <span class="math inline">\(D = \bar X_2 - \bar X_1\)</span> is thus also normally distributed:</p>
<p><span class="math display">\[D = \bar X_2 - \bar X_1 = N\left(\mu_2-\mu_1, \sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}\right)\]</span></p>
<p>If <span class="math inline">\(H_0\)</span> is true: <span class="math display">\[D = \bar X_2 - \bar X_1 = N\left(0, \sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}\right)\]</span></p>
<p>The test statistic: <span class="math display">\[Z = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}}\]</span> is standard normal, i.e.Â <span class="math inline">\(Z \sim N(0,1)\)</span>.</p>
<p>However, note that the test statistic require the standard deviations <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span> to be known.</p>
<p>What if the population standard deviations are not known?</p>
<p>If the sample sizes are large, we can replace the known standard deviations with our sample standard deviations and according to the central limit theorem assume that</p>
<p><span class="math display">\[Z = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{s_2^2}{n_2} + \frac{s_1^2}{n_1}}} \sim N(0,1)\]</span></p>
<p>and proceed as before.</p>
<!-- Here $n_1=n_2=12$ which is not very large. -->
<p>For small sample sizes the test statistic will be t-distributed.</p>
<p><span class="math display">\[t = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{s_2^2}{n_2} + \frac{s_1^2}{n_1}}}\]</span></p>
<p>For small sample sizes we can use Studentâ€™s t-test, which requires us to assume that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> both are normally distributed and have equal variances. With these assumptions we can compute the pooled variance</p>
<p><span class="math display">\[
s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}
\]</span></p>
<p>and the test statistic</p>
<p><span class="math display">\[t = \frac{\bar X_1 - \bar X_2}{\sqrt{s_p^2(\frac{1}{n_1} + \frac{1}{n_2})}}\]</span></p>
<p><span class="math inline">\(t\)</span> is t-distributed with <span class="math inline">\(n_1+n_2-2\)</span> degrees of freedom.</p>
<p>The t-test is implemented in R, e.g.Â in the function <code>t.test</code> in the R-package <code>stats</code>, both Studentâ€™s t-test with equal variances and Welchâ€™s t-test with unequal variances.</p>
</div>
<div id="variance" class="section level2">
<h2>Variance</h2>
<p>The test of equal variance in two groups is based on the null hypothesis</p>
<p><span class="math display">\[H_0: \sigma_1^2 = \sigma_2^2\]</span></p>
<p>If the two samples both come from populations with normal distributions, the sample variances (also random variables) are</p>
<p><span class="math display">\[S_1^2 = \frac{1}{n_1-1} \sum_{i=1}^{n_1} (X_{1i}-\bar X_1)^2\\
S_2^2 = \frac{1}{n_2-1} \sum_{i=1}^{n_2} (X_{2i}-\bar X_2)^2\]</span></p>
<p>It can be shown that <span class="math inline">\(\frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2(n_1-1)\)</span> and <span class="math inline">\(\frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)\)</span>.</p>
<p>Hence, the test statistic for comparing the variances of two groups</p>
<p><span class="math display">\[F = \frac{S_1^2}{S_2^2}\]</span>
is <span class="math inline">\(F\)</span>-distributed with <span class="math inline">\(n_1-1\)</span> and <span class="math inline">\(n_2-1\)</span> degrees of freedom.</p>
<p>In R a test of equal variances can be performed using the function <code>var.test</code>.</p>
</div>
</div>
<p style="text-align: center;">
<a href="infe-exercise1-hypresamp.html"><button class="btn btn-default">Previous</button></a>
<a href="infe-exercise2-hypparam.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
