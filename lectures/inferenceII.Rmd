---
title: "Statistical inference, part II"
subtitle: Parametric tests
author: "Eva Freyhult"
date: "2019-10-05"
institute: NBIS, SciLifeLab
output:
  xaringan::moon_reader:
    css: ["default", "metropolis"]
    encoding: 'UTF-8'
    self_contained: false
    # lib_dir: libs
    nature:
      ratio: '4:3'
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
# slideNumberFormat: "Biostatistics. %current%/%total%"
---
class: spaced


```{r, include=FALSE}
require(tidyverse)
require(ggplot2)
require(reshape2)
require(knitr)
require(kableExtra)
require(latex2exp)
knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, cache=TRUE, error=FALSE, warnings=FALSE, dpi=600)
options(digits=4)
```


# Parametric tests

If the null distribution was already known (or could be computed based on a few assumptions) resampling would not be necessary.

We can follow the same steps as before to perform a hypothesis test:
  
  1. Define $H_0$ and $H_1$
  2. Select appropriate test statistic, $T$, and compute the observed value, $t_{obs}$
  3. Assume that the $H_0$ and derive the null distribution of the test statistic based on appropriate assumptions.
  4. Select an appropriate significance level, $\alpha$
  5. Compare the observed value, $t_{obs}$, with the null distribution and compute a p-value. The p-value is the probability of observing a value at least as extreme as the observed value, if $H_0$ is true.
  6. Based on the p-value either accept or reject $H_0$.

---
layout: true
# One sample, proportions

---

**Pollen example**

Assume that the proportion of pollen allergy in Sweden is known to be $0.3$. Observe 100 people from Uppsala, 42 of these are allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala $\pi > 0.3$?

Denote the unknown (Uppsala) populations proportion of pollen allergy $\pi$ and define $H_0$ and $H_1$.

$$H_0: \pi=\pi_0 \\
H_1: \pi>\pi_0,$$

where $\pi_0$ is the known proportion under $H_0$ (here 0.3, the proportion in Sweden).

Ather potential alternative hypothesis are $H_1: \pi<\pi_0$ or $H_1:\pi \neq \pi_0$, but in this particular example we are only interested in the alternative that $\pi > \pi_0$.

---

The number of allergic individuals in a sample of size $n$ is $X$ and the proportion of allergic persons is $P = X/n$. $X \sim Bin(n, \pi)$, but here we can use the Central limit theorem;

>The sum of $n$ independent and equally distributed random variables is normally distributed, if $n$ is large enough.

<!-- As a result of the central limit theorem, the distribution of number or proportion of allergic individuals in a sample of size $n$ is approximately normal, if the sample is large enough. A rule of thumb is that the sample size should be $n>30$ (in out example $n=100$). -->

From the binomial distribution we know that $E[X] = \pi$ and $var(X) = n\pi(1-\pi).$ Hence, $E[P] = \pi$ and $var(P) = \frac{\pi(1-\pi)}{n}$. The standard error is thus $SE=\sqrt{\frac{\pi(1-\pi)}{n}}$

Following the central limit theorem, $P$ is normally distributed

$$P \sim N\left(\pi, \sqrt{\frac{\pi(1-\pi)}{n}}\right)$$

---

If $H_0$ is true $\pi=\pi_0$ and

$$P \sim N\left(\pi_0, \sqrt{\frac{\pi_0(1-\pi_0)}{n}}\right)$$
An appropriate test statistic is

$$Z = \frac{P-\pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}$$

$Z \in N(0,1)$ which makes probabilities easy to compute.

---

$$Z = \frac{P-\pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}$$

Observed value in our pollen example

$p=0.42$ and $\pi_0=0.3$, hence

$$Z_{obs} = \frac{0.42-0.3}{\sqrt{\frac{0.3(1-0.3)}{100}}} = `r zobs <- (.42-.3)/sqrt(.3*.7/100);format(zobs, digits=3)`$$

Set the significance levels to $\alpha=0.05$ and compute the p-value;

$$p = P(P \geq \pi_0) = P(Z \geq Z_{obs}) = P(Z \geq `r format(zobs, digits=3)`) = \\
1 - P(Z \leq `r format(zobs, digits=3)`) = [table] = 1 - `r format(pnorm(2.62), digits=4)` = `r format(pnorm(2.62, lower.tail=FALSE), digits=3)`$$

As `r format(pnorm(2.62, lower.tail=FALSE), digits=3)`<0.05 we reject $H_0$ and conclude that there is reason to believe that the proportion of allergic in Uppsala is greater than 0.3.

---
layout: false
name: onemean
layout: true

# One sample, mean

---

A one sample test of means compares the mean of a sample to a prespecified value.

The hypotheses:

$$H_0: \mu = \mu_0 \\
H_1: \mu \neq \mu_0$$

The alternative hypothesis, $H_1,$ above is for the two sided hypothesis test. Other options are the one sided $H_1$; $H_1: \mu > \mu_0$ or $H_1: \mu < \mu_0$.


Example:  We know that the weight of a mouse on normal diet is normally distributed with mean 24.0 g and standard deviation 3 g. To investigate if body weight of mice is changed by high-fat diet, 10 mice are fed on high-fat diet for three weeks. The mean weight of the high-fat mice is 26.0 g, is there reason to believe that high fat diet affect mice body weight?

---

The body weight of a mouse is $X \sim N(\mu, \sigma)$ and the mean weight of $n$ independent mice from the same population is  
$$\bar X \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right).$$

An appropriate test statistic is 

$$Z = \frac{\bar X - \mu}{\frac{\sigma}{\sqrt{n}}}$$
If $\sigma$ is known, $Z\sim N(0,1)$.

---
layout:false

# One sample t-test

For small $n$ and unknown $\sigma$, the test statistic

$$t = \frac{\bar X - \mu}{\frac{s}{\sqrt{n}}}$$

is t-distributed with $df=n-1$ degrees of freedom.

---

# Two samples, proportions

$$H_0: \pi_1 - \pi_2 = 0\\
H_1: \pi_1 - \pi_2 \neq 0$$

Alternatively, a one sided alternative hypothesis can be used; $H_1: \pi_1 - \pi_2 >0$ or $H_1: \pi_1 - \pi_2 < 0$.
<!-- If $H_0$ is true -->

<!-- $$P_1 - P_2 \sim N\left(0, \sqrt{\pi(1-\pi)\left (\frac{1}{n_1} + \frac{1}{n_2}\right)} \right)$$ -->
<!-- where $\pi$ is the  -->

Test statistic

$$Z = \frac{P_1 - P_2}{\sqrt{P(1-P)\left (\frac{1}{n_1} + \frac{1}{n_2}\right)}}$$

where $P$ is the proportion in the merged sample of size $n_1 + n_2$. $Z \in N(0,1)$ and p-value can be computed using the standard normal distribution.

---
layout: false
layout: true

# Two samples, mean

---

A two sample test of means is used to determine if two population means are equal.

Two independent samples are collected (one from each population) and the means are compared. Can for example be used to determine if a treatment group is different compared to a control group, in terms of the mean of a property of interest.

The hypotheses;

$$H_0: \mu_2 = \mu_1\\
H_1: \mu_2 \neq \mu_1$$

The above $H_1$ is two-sided. One-sided alternatives could be
$$H_1: \mu_2 > \mu_1\\
\mathrm{or}\\
H_1: \mu_2 < \mu_1$$

---

Assume that observations from both populations are normally distributed;


$$
\begin{aligned}
X_1 \sim N(\mu_1, \sigma_1) \\
X_2 \sim N(\mu_2, \sigma_2)
\end{aligned}
$$

Then it follows that the sample means will also be normally distributed;

$$
\begin{aligned}
\bar X_1 \sim N(\mu_1, \sigma_1/\sqrt{n_1}) \\
\bar X_2 \sim N(\mu_2, \sigma_2/\sqrt{n_2})
\end{aligned}
$$

The mean difference $D = \bar X_2 - \bar X_1$ is thus also normally distributed:

---

$$D = \bar X_2 - \bar X_1 = N\left(\mu_2-\mu_1, \sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}\right)$$


If $H_0$ is true: $$D = \bar X_2 - \bar X_1 = N\left(0, \sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}\right)$$

The test statistic: $$Z = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}}$$ is standard normal, i.e. $Z \sim N(0,1)$.

However, note that the test statistic require the standard deviations $\sigma_1$ and $\sigma_2$ to be known.

---

## Unknown variances, large sample sizes

What if the population standard deviations are not known?

If the sample sizes are large, we can replace the known standard deviations with our sample standard deviations and according to the central limit theorem assume that 

$$Z = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{s_2^2}{n_2} + \frac{s_1^2}{n_1}}} \sim N(0,1)$$

and proceed as before.

---

## Unknown but equal variances, small sample sizes

<!-- Here $n_1=n_2=12$ which is not very large. -->

For small sample sizes we can use Student's t-test, which requires us to assume that $X_1$ and $X_2$ both are normally distributed and have equal variances. With these assumptions we can compute the pooled variance

$$
  s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}
$$
  
  and the test statistic

$$t = \frac{\bar X_1 - \bar X_2}{\sqrt{s_p^2(\frac{1}{n_1} + \frac{1}{n_2})}},$$
  
which is t-distributed with $n_1+n_2-2$ degrees of freedom.

---

## Unknown unequal variances, small sample sizes

For unequal variances the following test statistic can be used;

$$t = \frac{\bar X_2 - \bar X_1}{\sqrt{\frac{s_2^2}{n_2} + \frac{s_1^2}{n_1}}}$$
$t$ is $t$-distributed and the degrees of freedom can be computed using Welch approximation.

Fortunately, the t-test is implemented in R, e.g. in the function `t.test` in the R-package `stats`, both Student's t-test with equal variances and Welch's t-test with unequal variances.

---
layout: false

# Variance test

.pull-left[
The test of equal variance in two groups is based on the null hypothesis

$$H_0: \sigma_1^2 = \sigma_2^2$$
  
If the two samples both come from two populations with normal distributions, the sample variances

$$S_1^2 = \frac{1}{n_1-1} \sum_{i=1}^{n_1} (X_{1i}-\bar X_1)^2\\
S_2^2 = \frac{1}{n_2-1} \sum_{i=1}^{n_2} (X_{2i}-\bar X_2)^2$$
]
.pull-right[
It can be shown that $\frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2(n_1-1)$ and $\frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)$.

Hence, the test statistic for comparing the variances of two groups

$$F = \frac{S_1^2}{S_2^2}$$
is $F$-distributed with $n_1-1$ and $n_2-1$ degrees of freedom.
]

In R a test of equal variances can be performed using the function `var.test`.


---
layout: false
layout: true

# Multiple testing

---

## Error types

.pull-left[
```{r}
kable(matrix(c("", "H0 is true", "H0 is false", "Accept H0", "", "Type II error, miss", "Reject H0", "Type I error, false alarm", ""), byrow=F, ncol=3)) %>% kable_styling(font_size=14)
```
]
--
.pull-right[
```{r}
kable(matrix(c("", "H0 is true", "H0 is false", "Accept H0", "TN", "FN", "Reject H0", "FP", "TP"), byrow=F, ncol=3)) %>% kable_styling(font_size=14)
```
]

--

**Significance level**

$$P(\mbox{reject }\,H_0 | H_0 \,\mbox{is true}) = P(\mbox{type I error}) = \alpha$$
  
**Statistical power**

$$P(\mbox{reject } H_0 | H_1 \mbox{ is true}) = P(\mbox{reject } H_0 | H_0 \mbox{ is false}) = 1 - P(\mbox{type II error})$$
---

#### Perform one test:
  
  - P(One type I error) = $\alpha$
  - P(No type I error) = $1 - \alpha$
  
#### Perform $m$ independent tests:
  
  - P(No type I errors in $m$ tests) = $(1 - \alpha)^m$
  - P(At least one type I error in $m$ tests) = $1 - (1 - \alpha)^m$
  
---

  ```{r multiple, echo=FALSE, out.width="50%", fig.align="center", fig.width=4, fig.height=4}
a=0.05
k <- 1:100
ggplot(data.frame(k=k, p = 1-(1-a)^k), aes(x=k, y=p)) + geom_line() + xlab("Number of tests") + ylab("P(At least one type I error)") + theme_bw() + annotate("label", x=75, y=0.2, label="alpha == 0.05", parse=TRUE)
```

---
- FWER: family-wise error rate, probability of one or more false positive, e.g. Bonferroni, Holm
- FDR: false discovery rate, proportion of false positives among hits, e.g. Benjamini-Hochberg, Storey

---
layout: false

# Bonferroni correction

To achieve a family-wise error rate of $\leq \alpha$ when performing $m$ tests, declare significance and reject the null hypothesis for any test with $p \leq \alpha/m$.

Objections: too conservative

---

# Benjamini-Hochbergs FDR

```{r}
kable(matrix(c("", "H0 is true", "H0 is false", "Accept H0", "TN", "FN", "Reject H0", "FP", "TP"), byrow=3, ncol=3)) %>% kable_styling(font_size=14)
```

The false discovery rate is the proportion of false positives among 'hits', i.e. $\frac{FP}{TP+FP}$.


Benjamini-Hochberg's method control the FDR level, $\gamma$, when performing $m$ *independent* tests, as follows:
  
1. Sort the p-values $p_1 \leq p_2 \leq \dots \leq p_m$.
2. Find the maximum $j$ such that $p_j \leq \gamma \frac{j}{m}$.
3. Declare significance for all tests $1, 2, \dots, j$.

---

# 'Adjusted' p-values
  
Sometimes an adjusted significance threshold is not reported, but instead 'adjusted' p-values are reported.
  
- Using Bonferroni's method the 'adjusted' p-values are:
  
  $\tilde p_i = \min(m p_i, 1)$.

A feature's adjusted p-value represents the smallest FWER at which the null hypothesis will be rejected, i.e. the feature will be deemed significant.
  
- Benjamini-Hochberg's 'adjusted' p-values are called $q$-values:
  
  $q_i = \min(\frac{m}{i} p_i, 1)$
  
  A feature's $q$-value can be interpreted as the lowest FDR at which the corresponding null hypothesis will be rejected, i.e. the feature will be deemed significant.

---

# Example, 10000 independent tests (e.g. genes) {-}

```{r padjust, results="asis"}
p <- sort(c(1.7e-8, 5.8e-8, 3.4e-7, 9.1e-7, 1.0e-6, 2.4e-6, 3.6e-5, 2.3e-5, 2.3e-4, 2.2e-4, 8.9e-3,7.3e-4, 0.0045, 0.0032, 0.0087, 0.012, 0.014, 0.045, 0.08, 0.23))
kable(data.frame(`p-value`=sprintf("%.3g", p), `adj p (Bonferroni)` = p.adjust(p, "bonferroni", 10000), `q-value (B-H)`=p.adjust(p, "BH", 10000), check.names=FALSE))
```
