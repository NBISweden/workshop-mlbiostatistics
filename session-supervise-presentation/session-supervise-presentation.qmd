---
title: "Supervised learning"
# author: Olga Dethlefsen
format: 
  revealjs:
    slide-number: true
    theme: [default, custom.scss]
    view-distance: 10
    chalkboard: 
      buttons: true
  html:
    code-fold: false
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

# We will discuss

- What supervised learning is.
- Data splitting.
- How to evaluate classification and regression ML models.
- How to put together a minimum working example of supervised learning with KNN classifier.

## What is supervised learning
*Definition*

<br>

```{mermaid}
flowchart TD
  A(Machine n learning) --> B(unsupervised n learning)
  A --> C(supervised n learning)
```

<br>

::: incremental

-   Supervised learning can be used for **classification** and **regression**.
    - For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)
    - For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)
- In supervised learning we are using sample **labels** to **train** (build) a model. 
- We then use the trained model for **interpretation** and **prediction**.

:::

## What is supervised learning?
*Definition*
<br>

::: incremental
-   **Training** a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).
-   Common supervised machine learning algorithms include:
    - K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). 
- Many of the ML methods can be implemented to work both for classifying samples and forecasting numeric outcome.
:::

## Supervised learning
*Outline*
<br>

Across many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:

::: incremental

- deciding on the task: classification or regression  
- **splitting data** to keep part of data for training and part for testing
-   selecting supervised machine learning algorithms to be trained (or a set of these)
-   deciding on the training strategy, i.e. which **performance metrics** to use and how to search for the best model parameters
-   running **feature engineering**: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables
-   performing **feature selection**: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present
:::

## Supervised learning {.scrollable}
*Outline*
<br>

The diagram below shows a basic strategy on how to train KNN for classification, given a data set with $n$ samples, $p$ variables and $y$ categorical outcome

```{mermaid}

flowchart TD
  A([data]) -. split data \n e.g. basic, stratified, grouped -.-> B([non-test set])
  A([data]) -.-> C([test set])
  B -.-> D(choose algorithm \n e.g. KNN)
  D -.-> E(choose evaluation metric \n e.g. overall accuracy)
  E -.-> F(feature engineering & selection)
  F -.-> G(prepare parameter space, e.g. odd k-values from 3 to 30)
  G -. split non-test -.-> H([train set & validation set])
  H -.-> J(fit model on train set)
  J -.-> K(collect evaluation metric on validation set)
  K -.-> L{all values checked? \n e.g. k more than 30}
  L -. No .-> J
  L -. Yes .-> M(select best parameter values)
  M -.-> N(fit model on all non-test data)
  N -.-> O(assess model on test data)
  C -.-> O

```

## Supervised learning
*Outline*
<br>

Before we see how this training may look like in `R`, let's talk more about

-   **KNN**, K-nearest neighbor algorithm
-   **data splitting** and
-   **performance metrics** useful for evaluating models

## Classification
*Definition*
<br>

::: incremental

-   Classification methods are algorithms used to categorize (classify) objects based on their measurements.
-   They belong under **supervised learning** as we usually start off with **labeled** data, i.e. observations with measurements for which we know the label (class) of.
-   Let's for each observations $i$ collect pair of information $\{\mathbf{x_i}, g_i\}$
-   where $\{\mathbf{x_i}\}$ is a set of exploratory variables e.g. a gene expression data
-   and $g_i \in \{1, \dots, G\}$ is the class label for each observation (known), e.g. cancer stage I, II, III or IV
-   Then we want to find a **classification rule** $f(.)$ (model) such that $$f(\mathbf{x_i})=g_i$$

:::






## KNN 

*example of a classification algorithm*

```{r}
#| label: fig-knn-create-points
#| fig-cap: An example of k-nearest neighbours algorithm with k=3; in the top new observation (blue) is closest to three red triangales and thus classified as a red triangle; in the bottom, a new observation (blue) is closest to 2 black dots and 1 red triangle thus classified as a black dot (majority vote)

# Example data
n1 <- 10
n2 <- 10
set.seed(1)
x <- c(rnorm(n1, mean=0, sd=1), rnorm(n2, mean=0.5, sd=1))
y <- rnorm(n1+n2, mean=0, sd=1)

group <- rep(1, (n1+n2))
group[1:n1] <- 0
idx.1 <- which(group==0)
idx.2 <- which(group==1)

# new points
p1 <- c(1.5, 0.5)
p2 <- c(0, 0.6)

# distance 
dist.1 <- c()
dist.2 <- c()
for (i in 1:length(x))
{
  dist.1[i] <- round(sqrt((p1[1]-x[i])^2 + (p1[2]-y[i])^2),2)
  dist.2[i] <- round(sqrt((p2[1]-x[i])^2 + (p2[2]-y[i])^2),2)
}

# find nearest friends
n.idx1 <- order(dist.1)
n.idx2 <- order(dist.2)

```

```{r}
#| label: fig-knn-00
#| fig-width: 4
#| fig-height: 5
#| include: false

# a) 
par(mar=(c(3, 3, 1, 1)))
plot(x[idx.1],y[idx.1], pch=0, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
#points(p1[1], p1[2], pch=13, col="blue", cex=2)

```

```{r}
#| label: fig-knn-01
#| fig-width: 4
#| fig-height: 5
#| include: false

# b) 
par(mar=(c(3, 3, 1, 1)))
plot(x[idx.1],y[idx.1], pch=0, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p1[1], p1[2], pch=13, col="blue", cex=2)

```

```{r}
#| label: fig-knn-02
#| fig-width: 4
#| fig-height: 5
#| include: false

# c) 
par(mar=(c(3, 3, 1, 1)))
plot(x[idx.1],y[idx.1], pch=0, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p1[1], p1[2], pch=13, col="blue", cex=2)
points(x[n.idx1[1:3]], y[n.idx1[1:3]], pch=17, col="red")

```

```{r}
#| label: fig-knn-03
#| fig-width: 4
#| fig-height: 5
#| include: false

# d) 
par(mar=(c(3, 3, 1, 1)))
plot(x[idx.1],y[idx.1], pch=0, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p1[1], p1[2], pch=17, col="red", cex=3)
points(x[n.idx1[1:3]], y[n.idx1[1:3]], pch=17, col="red")

```

::: r-stack
![](session-supervise-presentation_files/figure-revealjs/fig-knn-00-1.png){.fragment width="700" height="600"}

![](session-supervise-presentation_files/figure-revealjs/fig-knn-01-1.png){.fragment width="700" height="600"}

![](session-supervise-presentation_files/figure-revealjs/fig-knn-02-1.png){.fragment width="700" height="600"}

![](session-supervise-presentation_files/figure-revealjs/fig-knn-03-1.png){.fragment width="700" height="600"}
:::

## KNN

*example of a classification algorithm*

```{r}
#| label: fig-knn-10
#| fig-width: 4
#| fig-height: 5
#| include: false

par(mar=(c(3, 3, 1, 1)))
plot(x[idx.1],y[idx.1], pch=0, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
```

```{r}
#| label: fig-knn-20
#| fig-width: 4
#| fig-height: 5
#| include: false

par(mar=(c(3, 3, 1, 1)))
plot(x[idx.1],y[idx.1], pch=0, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p2[1], p2[2], pch=13, col="blue", cex=2)

```

```{r}
#| label: fig-knn-30
#| fig-width: 4
#| fig-height: 5
#| include: false

par(mar=(c(3, 3, 1, 1)))
plot(x[idx.1],y[idx.1], pch=0, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")

points(x[idx.2], y[idx.2], pch=2, col="red")
points(p2[1], p2[2], pch=13, col="blue", cex=2)
points(x[n.idx2[1]], y[n.idx2[1]], pch=17, col="red")
points(x[n.idx2[2:3]], y[n.idx2[2:3]], pch=19, col="black")

```

```{r}
#| label: fig-knn-40
#| fig-width: 4
#| fig-height: 5
#| include: false


par(mar=(c(3, 3, 1, 1)))
plot(x[idx.1],y[idx.1], pch=0, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p2[1], p2[2], pch=15, col="black", cex=2)
points(x[n.idx2[1]], y[n.idx2[1]], pch=17, col="red")
points(x[n.idx2[2:3]], y[n.idx2[2:3]], pch=19, col="black")

```

::: r-stack
![](session-supervise-presentation_files/figure-revealjs/fig-knn-10-1.png){.fragment width="700" height="600"}

![](session-supervise-presentation_files/figure-revealjs/fig-knn-20-1.png){.fragment width="700" height="600"}

![](session-supervise-presentation_files/figure-revealjs/fig-knn-30-1.png){.fragment width="700" height="600"}

![](session-supervise-presentation_files/figure-revealjs/fig-knn-40-1.png){.fragment width="700" height="600"}
:::

## KNN
*Algorithm*
<br>

::: incremental
-   Decide on the value of $k$.
-   Calculate the distance between the query-instance (observations for new sample) and all the training samples.
-   Sort the distances and determine the nearest neighbors based on the $k$-th minimum distance.
-   Gather the categories of the nearest neighbors.
-   Use majority voting of the categories of the nearest neighbors as the prediction value for the new sample.
- Note: *Euclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.*
:::


## Data splitting
*Why*
<br>

-   Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.
-   As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to **overfitting**.
-   To deal with overconfident estimation of future performance we can implement various data splitting strategies.

## Data splitting
*train, validation & test sets*

```{r}
#| fig-align: center
#| echo: false
#| out-width: 100%

knitr::include_graphics("figures/data-split-02.png")
```

-   **Training data**: this is data used to fit (train) the classification or regression model, i.e. derive the classification rule.
-   **Validation data**: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters.
-   **Test data**: this data is used to give an estimate of future prediction performance for the model and parameters chosen.
-   Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively

## Data splitting
*cross validation & repeated cross validation*

```{r}
#| fig-align: center
#| echo: false
#| out-width: 100%
#| 
knitr::include_graphics("figures/data-split-kfolds-02.png")
##Eva: k=1,2,3 look identical. Can the splits be indicated somehow, circle the validation fold or something similar? I see 20 different objects, so I would expect 6 or 7 objects in each validation set. Use 'validation set' and 'training set' instead of 'validation fold' etc.
```

-   In **k-fold cross-validation** we split data into $k$ roughly equal-sized parts.
-   We start by setting the validation data to be the first set of data and the training data to be all other sets.
-   We estimate the validation error rate / correct classification rate for the split.
-   We then repeat the process $k-1$ times, each time with a different part of the data set to be the validation data and the remainder being the training data.
-   We finish with $k$ different error or correct classification rates.
-   In this way, every data point has its class membership predicted once.
-   The final reported error rate is usually the average of $k$ error rates.

## Data splitting
*Leave-one-out cross-validation*
<br>

-   Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.

```{r}
#| label: fig-data-split-loocv
#| fig-cap: Example of LOOCV, leave-one-out cross validation
#| fig-cap-location: margin
#| fig-align: center
#| echo: false
#| out-width: 100%

knitr::include_graphics("figures/data-split-loocv-02.png")
```

## Performance metrics
*Evaluating classification*
<br>

-   To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of $k$ in KNN.
-   There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.
-   Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.

## Evaluating classification
<br>
<br>
**Correct (miss)classification rate**

-   The simplest way to evaluate in which we count for all the $n$ predictions how many times we got the classification right. $$Correct\; Classifcation \; Rate = \frac{\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}$$ where $1[]$ is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise

**Missclassification Rate**

Missclassification Rate = 1 - Correct Classification Rate

## Evaluating classification {.smaller}
<br>

**Confusion matrix**

Confusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes. 

|                     | Predicted Positive  | Predicted Negative  |
|---------------------|---------------------|---------------------|
| **Actual Positive** | True Positive (TP)  | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN)  |

- **Accuracy**: measures the proportion of correctly classified samples over the total number of samples. $$ACC = \frac{TP+TN}{TP+TN+FP+FN}$$
- **Sensitivity**: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as **true positive rate** and **recall**.
 $$TPR = \frac{TP}{TP + FN}$$

- **Specificity**: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as **true negative rate** and **selectivity**. $$TNR = \frac{TN}{TN+FP}$$

- **Precision**: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives.  It is also known as **positive predictive value**. $$PPV = \frac{TP}{TP + FP}$$ 

## Performance metrics
*Evaluating regression*
<br>

::: incremental
- The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. 
- For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. 
- In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. 
:::

## Evaluating regression {.smaller}
<br>


- **R-squared**: As seen in the linear regression session.
$$
R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
$$
- **Adjusted R-squared**: seen before
$$
R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
$$
- **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. 
$$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$
- **Root Mean Squared Error (RMSE)**: square root of the MSE 
$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$
- **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$
- **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.

## Live demo
*KNN model for classification*
